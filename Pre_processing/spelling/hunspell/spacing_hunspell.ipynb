{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_clean(text):\n",
    "    pattern = '([ㄱ-ㅎㅏ-ㅣ]+)'  # 한글 자음, 모음 제거\n",
    "    text = re.sub(pattern, '', text)\n",
    "    \n",
    "    # pattern = '[^\\w\\s]'         # 특수기호제거\n",
    "    # text = re.sub(pattern, '', text)\n",
    "    \n",
    "    pattern = '(?<!\\d)-|\\b(?!\\d+-\\d)\\d+\\b|[^\\w\\s-]' # 번지수\n",
    "    text = re.sub(pattern, '', text)\n",
    "    \n",
    "    pattern = '\\s{2, }'           # 중복 공백 제거\n",
    "    text = re.sub(pattern, '', text).strip() \n",
    "    \n",
    "    pattern = '\\n'            # 줄 바꿈 제거\n",
    "    text = re.sub(pattern, '', text) \n",
    "\n",
    "    # pattern = '\\d+'        # 숫자 제거\n",
    "    # text = re.sub(pattern, '', text) \n",
    "\n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('/home/yjtech2/Desktop/yurim/LLM/Data/text_test_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>음식물 쓰레기 냄세가 나요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>전라북도 순창군 면 마을 꼴목길에  퇴비를 가져다 놓음바람이불면 골목안쪽으로 냄새가...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>인도에 폐기물 컨테이너가 있어 통행 불편 및 악취로 불편합니다발리조치해 주시기 바랍니다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>이웃집 에서 보일러 기름으로 추정되는 물질이 다량으로 유출되어 주변에 피해를 쥽니다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>동국대 신공학관 근처 하수 처리 냄세 신고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>층간소음 문제로 12년된 아파트 하자 점검 서비스가 있나요원룸형 아파트 인데요 옆집...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>건물 옆 주차장에 길고양들이 배설물을 싸고 있습니다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>공공시설 길거리 흡연자가 녀무 많습니다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>벌래와 모기 하수구냄새가 납니다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>바람이 동서족에서 불고 불쾌한 냄새가 납니다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                                     음식물 쓰레기 냄세가 나요\n",
       "1  전라북도 순창군 면 마을 꼴목길에  퇴비를 가져다 놓음바람이불면 골목안쪽으로 냄새가...\n",
       "2   인도에 폐기물 컨테이너가 있어 통행 불편 및 악취로 불편합니다발리조치해 주시기 바랍니다\n",
       "3     이웃집 에서 보일러 기름으로 추정되는 물질이 다량으로 유출되어 주변에 피해를 쥽니다\n",
       "4                            동국대 신공학관 근처 하수 처리 냄세 신고\n",
       "5  층간소음 문제로 12년된 아파트 하자 점검 서비스가 있나요원룸형 아파트 인데요 옆집...\n",
       "6                       건물 옆 주차장에 길고양들이 배설물을 싸고 있습니다\n",
       "7                              공공시설 길거리 흡연자가 녀무 많습니다\n",
       "8                                  벌래와 모기 하수구냄새가 납니다\n",
       "9                           바람이 동서족에서 불고 불쾌한 냄새가 납니다"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(text_clean)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yjtech2/Desktop/yurim/anaconda3/envs/venv-llm/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-19 13:20:31.397466: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-19 13:20:31.421317: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-19 13:20:31.785480: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-19 13:20:33.632480] ====== Data Load Start ======\n",
      "[2024-11-19 13:20:33.632548] ====== Data Load Finished ======\n",
      "[2024-11-19 13:20:33.632548] ====== Data Preprocessing Start ======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing spacing correction: 100%|██████████| 10/10 [00:00<00:00, 108.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-19 13:20:33.727519] ====== Data Preprocessing Finished ======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 모델 정의\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"fiveflow/roberta-base-spacing\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"fiveflow/roberta-base-spacing\")\n",
    "\n",
    "# GPU 사용 가능시 GPU 사용\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# 레이블 정의\n",
    "label = [\"UNK\", \"PAD\", \"O\", \"B\", \"I\", \"E\", \"S\"]\n",
    "\n",
    "def apply_spacing_correction(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        # 최대 토큰 길이 설정\n",
    "        max_token_length = 512 - 2  # CLS와 SEP 토큰을 위한 자리\n",
    "        \n",
    "        # 공백 제거된 텍스트 준비\n",
    "        org_text = text.replace(\" \", \"\")\n",
    "        corrected_sentences = []\n",
    "        \n",
    "        # 512 토큰 단위로 나누기\n",
    "        for start_idx in range(0, len(org_text), max_token_length):\n",
    "            chunk = org_text[start_idx:start_idx + max_token_length]\n",
    "            token_list = [tokenizer.cls_token_id]\n",
    "            for char in chunk:\n",
    "                token_list.extend(tokenizer.encode(char)[1:-1])  # CLS, SEP 토큰 제외\n",
    "            token_list.append(tokenizer.sep_token_id)\n",
    "            \n",
    "            tkd = torch.tensor(token_list).unsqueeze(0).to(device)\n",
    "            \n",
    "            # 모델 예측\n",
    "            with torch.no_grad():\n",
    "                outputs = model(tkd)\n",
    "                pred_idx = torch.argmax(outputs.logits, dim=-1)\n",
    "            \n",
    "            # 예측 결과를 문자열로 변환\n",
    "            pred_sent = \"\"\n",
    "            for char_idx, spc_idx in enumerate(pred_idx.squeeze()[1:-1]):\n",
    "                if char_idx >= len(chunk):  # 인덱스 범위 체크\n",
    "                    break\n",
    "                \n",
    "                curr_label = label[spc_idx]\n",
    "                if curr_label in [\"E\", \"S\"]:  # E나 S 태그가 있으면 띄어쓰기 추가\n",
    "                    pred_sent += chunk[char_idx] + \" \"\n",
    "                else:\n",
    "                    pred_sent += chunk[char_idx]\n",
    "            \n",
    "            corrected_sentences.append(pred_sent.strip())\n",
    "        \n",
    "        # 모든 청크를 다시 합치기\n",
    "        final_text = \" \".join(corrected_sentences)\n",
    "        return final_text.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        return text  # 에러 발생시 원본 텍스트 반환\n",
    "\n",
    "def process_dataframe(df, text_column):\n",
    "    # 진행바 표시\n",
    "    tqdm.pandas(desc=\"Processing spacing correction\")\n",
    "    \n",
    "    # 띄어쓰기 교정 적용\n",
    "    df[\"res_sentence\"] = df[text_column].progress_apply(apply_spacing_correction)\n",
    "    \n",
    "    return df\n",
    "\n",
    "     \n",
    "# 수행\n",
    "if __name__ == \"__main__\":\n",
    "    _now_time = datetime.now().__str__()\n",
    "    print(f'[{_now_time}] ====== Data Load Start ======')\n",
    "    _now_time = datetime.now().__str__()\n",
    "    print(f'[{_now_time}] ====== Data Load Finished ======')\n",
    "    \n",
    "    print(f'[{_now_time}] ====== Data Preprocessing Start ======')\n",
    "    processed_df = process_dataframe(df, \"text\")\n",
    "    _now_time = datetime.now().__str__()\n",
    "    print(f'[{_now_time}] ====== Data Preprocessing Finished ======')\n",
    "    \n",
    "    def text_clean(text):\n",
    "        # pattern = '([ㄱ-ㅎㅏ-ㅣ]+)'  # 한글 자음, 모음 제거\n",
    "        # text = re.sub(pattern, '', text)\n",
    "        \n",
    "        pattern = '[^\\w\\s]'         # 특수기호제거\n",
    "        text = re.sub(pattern, '', text)\n",
    "\n",
    "        return text \n",
    "    \n",
    "    processed_df['res_sentence'] = processed_df['res_sentence'].apply(text_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>res_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>음식물 쓰레기 냄세가 나요</td>\n",
       "      <td>음식물 쓰레기 냄세가 나요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>전라북도 순창군 면 마을 꼴목길에  퇴비를 가져다 놓음바람이불면 골목안쪽으로 냄새가...</td>\n",
       "      <td>전라북도 순창군 면마을 꼴목길에 퇴비를 가져다 놓음 바람이 불면 골목 안쪽으로 냄새...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>인도에 폐기물 컨테이너가 있어 통행 불편 및 악취로 불편합니다발리조치해 주시기 바랍니다</td>\n",
       "      <td>인도에 폐기물 컨테이너가 있어 통행 불편 및 악취로 불편합니다 발리 조치해 주시기 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>이웃집 에서 보일러 기름으로 추정되는 물질이 다량으로 유출되어 주변에 피해를 쥽니다</td>\n",
       "      <td>이웃집에서 보일러 기름으로 추정되는 물질이 다량으로 유출되어 주변에 피해를 쥽니다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>동국대 신공학관 근처 하수 처리 냄세 신고</td>\n",
       "      <td>동국대 신공학관 근처 하수 처리 냄세 신고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>층간소음 문제로 12년된 아파트 하자 점검 서비스가 있나요원룸형 아파트 인데요 옆집...</td>\n",
       "      <td>층간 소음 문제로 12년 된 아파트 하자 점검 서비스가 있나요 원룸형 아파트인데요 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>건물 옆 주차장에 길고양들이 배설물을 싸고 있습니다</td>\n",
       "      <td>건물 옆 주차장에 길 고양들이 배설물을 싸고 있습니다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>공공시설 길거리 흡연자가 녀무 많습니다</td>\n",
       "      <td>공공시설 길거리 흡연자가 녀무 많습니다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>벌래와 모기 하수구냄새가 납니다</td>\n",
       "      <td>벌래와 모기 하수구 냄새가 납니다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>바람이 동서족에서 불고 불쾌한 냄새가 납니다</td>\n",
       "      <td>바람이 동서족에서 불고 불쾌한 냄새가 납니다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                                     음식물 쓰레기 냄세가 나요   \n",
       "1  전라북도 순창군 면 마을 꼴목길에  퇴비를 가져다 놓음바람이불면 골목안쪽으로 냄새가...   \n",
       "2   인도에 폐기물 컨테이너가 있어 통행 불편 및 악취로 불편합니다발리조치해 주시기 바랍니다   \n",
       "3     이웃집 에서 보일러 기름으로 추정되는 물질이 다량으로 유출되어 주변에 피해를 쥽니다   \n",
       "4                            동국대 신공학관 근처 하수 처리 냄세 신고   \n",
       "5  층간소음 문제로 12년된 아파트 하자 점검 서비스가 있나요원룸형 아파트 인데요 옆집...   \n",
       "6                       건물 옆 주차장에 길고양들이 배설물을 싸고 있습니다   \n",
       "7                              공공시설 길거리 흡연자가 녀무 많습니다   \n",
       "8                                  벌래와 모기 하수구냄새가 납니다   \n",
       "9                           바람이 동서족에서 불고 불쾌한 냄새가 납니다   \n",
       "\n",
       "                                        res_sentence  \n",
       "0                                     음식물 쓰레기 냄세가 나요  \n",
       "1  전라북도 순창군 면마을 꼴목길에 퇴비를 가져다 놓음 바람이 불면 골목 안쪽으로 냄새...  \n",
       "2  인도에 폐기물 컨테이너가 있어 통행 불편 및 악취로 불편합니다 발리 조치해 주시기 ...  \n",
       "3      이웃집에서 보일러 기름으로 추정되는 물질이 다량으로 유출되어 주변에 피해를 쥽니다  \n",
       "4                            동국대 신공학관 근처 하수 처리 냄세 신고  \n",
       "5  층간 소음 문제로 12년 된 아파트 하자 점검 서비스가 있나요 원룸형 아파트인데요 ...  \n",
       "6                      건물 옆 주차장에 길 고양들이 배설물을 싸고 있습니다  \n",
       "7                              공공시설 길거리 흡연자가 녀무 많습니다  \n",
       "8                                 벌래와 모기 하수구 냄새가 납니다  \n",
       "9                           바람이 동서족에서 불고 불쾌한 냄새가 납니다  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본적인 hunspell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HunSpell', 'HunSpellError', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'add', 'add_dic', 'add_with_affix', 'analyze', 'generate', 'generate2', 'get_dic_encoding', 'remove', 'spell', 'stem', 'suggest']\n"
     ]
    }
   ],
   "source": [
    "import hunspell\n",
    "\n",
    "print(dir(hunspell))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class HunSpell in module builtins:\n",
      "\n",
      "class HunSpell(object)\n",
      " |  HunSpell binding. \n",
      " |  \n",
      " |  Instantiation goes like this:\n",
      " |  >>> hobj = HunSpell('/path/to/dict.dic', '/path/to/dict.aff')\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, /, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  add(...)\n",
      " |      Adds the given word into the runtime dictionary.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : string\n",
      " |          The word to add in the dictionary\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int : 0 if success, hunspell program error code else.\n",
      " |  \n",
      " |  add_dic(...)\n",
      " |      Load an extra dictionary to the current instance.\n",
      " |      The  extra dictionaries use the affix file of the allocated Hunspell object.\n",
      " |      Maximal number of the extra dictionaries is limited in the Hunspell source code to 20.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dpath : string\n",
      " |          Path to the .dic to add.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int : hunspell program error code.\n",
      " |  \n",
      " |  add_with_affix(...)\n",
      " |      Adds the given word with affix flags of the example (a dictionary word) into the runtime dictionary.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : string\n",
      " |          The word to transform.\n",
      " |      word : string\n",
      " |          The example to use to find flags\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int : 0 if success, hunspell program error code else.\n",
      " |  \n",
      " |  analyze(...)\n",
      " |      Provide morphological analysis for the given word.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : string\n",
      " |          Input word to analyze.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of strings : Each string is a possible analysis of the input word. It contains the stem of the word (st:XXX) and some information about modifications done to get to the input word.\n",
      " |      For more information see: man 4 hunspell (or https://sourceforge.net/projects/hunspell/files/Hunspell/Documentation/) in the 'Optional data fields\" section.\n",
      " |  \n",
      " |  generate(...)\n",
      " |      Provide morphological generation for the given word using the second one as example.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : string\n",
      " |          The word to transform.\n",
      " |      word : string\n",
      " |          The example to use as a generator\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of string : A list of possible transformations or an empty list if nothing were found\n",
      " |  \n",
      " |  generate2(...)\n",
      " |      Provide morphological generation for the given word the second one as example.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : string\n",
      " |          The word to transform.\n",
      " |      tags : string\n",
      " |          String of an analyzed word\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of string : A list of possible transformations or an empty list if nothing were found\n",
      " |  \n",
      " |  get_dic_encoding(...)\n",
      " |      Gets encoding of loaded dictionary.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      string : The encoding of currently used dic file (UTF-8, ISO8859-1, ...)\n",
      " |  \n",
      " |  remove(...)\n",
      " |      Removes the given word from the runtime dictionary\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : string\n",
      " |          The word to remove from the dictionary\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int : 0 if success, hunspell program error code else.\n",
      " |  \n",
      " |  spell(...)\n",
      " |      Checks the spelling of the given word.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : string\n",
      " |          Word to check.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool : True if the word is correctly spelled else False\n",
      " |  \n",
      " |  stem(...)\n",
      " |      Stemmer method. It is a simplified version of analyze method.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : string\n",
      " |          The word to stem.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of string : The possible stems of the input word.\n",
      " |  \n",
      " |  suggest(...)\n",
      " |      Provide suggestions for the given word.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : string\n",
      " |          Word for which we want suggestions\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of strings : The list of suggestions for input word. (No suggestion returns an empty list).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from hunspell import HunSpell\n",
    "\n",
    "print(help(HunSpell))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-19 13:28:58] ====== hunspell Start ======\n",
      "[2024-11-19 13:28:59] ====== hunspell End ======\n"
     ]
    }
   ],
   "source": [
    "from hunspell import HunSpell\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def hunspell_basic(df, column_name):\n",
    "    try:\n",
    "        # Hunspell 객체 생성\n",
    "        hunspell = HunSpell(\n",
    "            '/home/yjtech2/Desktop/yurim/LLM/Pre_processing/spelling/hunspell/ko_KR.dic',\n",
    "            '/home/yjtech2/Desktop/yurim/LLM/Pre_processing/spelling/hunspell/ko_KR.aff'\n",
    "        )\n",
    "\n",
    "        # 결과 저장용 리스트\n",
    "        corrected_texts = []\n",
    "\n",
    "        for text in df[column_name]:\n",
    "            # 문장에서 단어 추출\n",
    "            words = re.findall(r'\\b\\w+\\b', text)\n",
    "            \n",
    "            # 수정된 문장을 저장할 변수\n",
    "            corrected_sentence = text\n",
    "            \n",
    "            for word in words:\n",
    "                if not hunspell.spell(word):\n",
    "                    suggestions = hunspell.suggest(word)\n",
    "                    \n",
    "                    if suggestions:\n",
    "                        corrected_word = suggestions[0]\n",
    "                        corrected_sentence = re.sub(r'\\b' + re.escape(word) + r'\\b', corrected_word, corrected_sentence, count=1)\n",
    "                        # print(f\"'{word}'를 '{corrected_word}'로 수정했습니다.\")\n",
    "                    else:\n",
    "                        print(f\"'{word}'에 대한 제안이 없습니다.\")\n",
    "            \n",
    "            corrected_texts.append(corrected_sentence)\n",
    "            # print(f'\\n수정 전 문장: {text}')\n",
    "            # print(f'\\n수정 후 문장: {corrected_sentence}')\n",
    "        \n",
    "        return corrected_texts\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Hunspell 초기화 중 오류 발생: {e}\")\n",
    "        return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    now_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(f'[{now_time}] ====== hunspell Start ======')\n",
    "    \n",
    "    # 새로운 열에 수정된 결과 저장\n",
    "    processed_df['hunspell_result'] = hunspell_basic(processed_df, 'res_sentence')\n",
    "    \n",
    "    now_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(f'[{now_time}] ====== hunspell End ======')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>res_sentence</th>\n",
       "      <th>hunspell_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>음식물 쓰레기 냄세가 나요</td>\n",
       "      <td>음식물 쓰레기 냄세가 나요</td>\n",
       "      <td>음식물 쓰레기 냄새가 나요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>전라북도 순창군 면 마을 꼴목길에  퇴비를 가져다 놓음바람이불면 골목안쪽으로 냄새가...</td>\n",
       "      <td>전라북도 순창군 면마을 꼴목길에 퇴비를 가져다 놓음 바람이 불면 골목 안쪽으로 냄새...</td>\n",
       "      <td>전라북도 순차군 면 마을 골목길에 퇴비를 가져다 놓음 바람이 불면 골목 안쪽으로 냄...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>인도에 폐기물 컨테이너가 있어 통행 불편 및 악취로 불편합니다발리조치해 주시기 바랍니다</td>\n",
       "      <td>인도에 폐기물 컨테이너가 있어 통행 불편 및 악취로 불편합니다 발리 조치해 주시기 ...</td>\n",
       "      <td>인도에 폐기물 컨테이너가 있어 통행 불편 및 악취로 불편합니다 발이 조치해 주시기 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>이웃집 에서 보일러 기름으로 추정되는 물질이 다량으로 유출되어 주변에 피해를 쥽니다</td>\n",
       "      <td>이웃집에서 보일러 기름으로 추정되는 물질이 다량으로 유출되어 주변에 피해를 쥽니다</td>\n",
       "      <td>이웃집에서 보일러 기름으로 추정되는 물질이 다량으로 유출되어 주변에 피해를 잡니다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>동국대 신공학관 근처 하수 처리 냄세 신고</td>\n",
       "      <td>동국대 신공학관 근처 하수 처리 냄세 신고</td>\n",
       "      <td>동국도 신 공학관 근처 하수 처리 냄새 신고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>층간소음 문제로 12년된 아파트 하자 점검 서비스가 있나요원룸형 아파트 인데요 옆집...</td>\n",
       "      <td>층간 소음 문제로 12년 된 아파트 하자 점검 서비스가 있나요 원룸형 아파트인데요 ...</td>\n",
       "      <td>증간 소음 문제로 12년 된 아파트 하자 점검 서비스가 있나요 원룸 형 아파트인데요...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>건물 옆 주차장에 길고양들이 배설물을 싸고 있습니다</td>\n",
       "      <td>건물 옆 주차장에 길 고양들이 배설물을 싸고 있습니다</td>\n",
       "      <td>건물 옆 주차장에 길 고양들이 배설물을 싸고 있습니다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>공공시설 길거리 흡연자가 녀무 많습니다</td>\n",
       "      <td>공공시설 길거리 흡연자가 녀무 많습니다</td>\n",
       "      <td>공공시설 길거리 흡연자가 누며 많습니다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>벌래와 모기 하수구냄새가 납니다</td>\n",
       "      <td>벌래와 모기 하수구 냄새가 납니다</td>\n",
       "      <td>벌레와 모기 하수구 냄새가 납니다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>바람이 동서족에서 불고 불쾌한 냄새가 납니다</td>\n",
       "      <td>바람이 동서족에서 불고 불쾌한 냄새가 납니다</td>\n",
       "      <td>바람이 동서 족에서 불고 불쾌한 냄새가 납니다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                                     음식물 쓰레기 냄세가 나요   \n",
       "1  전라북도 순창군 면 마을 꼴목길에  퇴비를 가져다 놓음바람이불면 골목안쪽으로 냄새가...   \n",
       "2   인도에 폐기물 컨테이너가 있어 통행 불편 및 악취로 불편합니다발리조치해 주시기 바랍니다   \n",
       "3     이웃집 에서 보일러 기름으로 추정되는 물질이 다량으로 유출되어 주변에 피해를 쥽니다   \n",
       "4                            동국대 신공학관 근처 하수 처리 냄세 신고   \n",
       "5  층간소음 문제로 12년된 아파트 하자 점검 서비스가 있나요원룸형 아파트 인데요 옆집...   \n",
       "6                       건물 옆 주차장에 길고양들이 배설물을 싸고 있습니다   \n",
       "7                              공공시설 길거리 흡연자가 녀무 많습니다   \n",
       "8                                  벌래와 모기 하수구냄새가 납니다   \n",
       "9                           바람이 동서족에서 불고 불쾌한 냄새가 납니다   \n",
       "\n",
       "                                        res_sentence  \\\n",
       "0                                     음식물 쓰레기 냄세가 나요   \n",
       "1  전라북도 순창군 면마을 꼴목길에 퇴비를 가져다 놓음 바람이 불면 골목 안쪽으로 냄새...   \n",
       "2  인도에 폐기물 컨테이너가 있어 통행 불편 및 악취로 불편합니다 발리 조치해 주시기 ...   \n",
       "3      이웃집에서 보일러 기름으로 추정되는 물질이 다량으로 유출되어 주변에 피해를 쥽니다   \n",
       "4                            동국대 신공학관 근처 하수 처리 냄세 신고   \n",
       "5  층간 소음 문제로 12년 된 아파트 하자 점검 서비스가 있나요 원룸형 아파트인데요 ...   \n",
       "6                      건물 옆 주차장에 길 고양들이 배설물을 싸고 있습니다   \n",
       "7                              공공시설 길거리 흡연자가 녀무 많습니다   \n",
       "8                                 벌래와 모기 하수구 냄새가 납니다   \n",
       "9                           바람이 동서족에서 불고 불쾌한 냄새가 납니다   \n",
       "\n",
       "                                     hunspell_result  \n",
       "0                                     음식물 쓰레기 냄새가 나요  \n",
       "1  전라북도 순차군 면 마을 골목길에 퇴비를 가져다 놓음 바람이 불면 골목 안쪽으로 냄...  \n",
       "2  인도에 폐기물 컨테이너가 있어 통행 불편 및 악취로 불편합니다 발이 조치해 주시기 ...  \n",
       "3      이웃집에서 보일러 기름으로 추정되는 물질이 다량으로 유출되어 주변에 피해를 잡니다  \n",
       "4                           동국도 신 공학관 근처 하수 처리 냄새 신고  \n",
       "5  증간 소음 문제로 12년 된 아파트 하자 점검 서비스가 있나요 원룸 형 아파트인데요...  \n",
       "6                      건물 옆 주차장에 길 고양들이 배설물을 싸고 있습니다  \n",
       "7                              공공시설 길거리 흡연자가 누며 많습니다  \n",
       "8                                 벌레와 모기 하수구 냄새가 납니다  \n",
       "9                          바람이 동서 족에서 불고 불쾌한 냄새가 납니다  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.to_excel('hunspell_result.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity 계산 이용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "단어 '남서방향족에서'에 대한 상위 3개 제안 및 유사도:\n",
      "제안: 조선왕조실록에서, 유사도: 0.7168\n",
      "'남서방향족에서'를 '조선왕조실록에서'로 수정했습니다.\n",
      "previous_sentence:  바람이 조선왕조실록에서 불고 불쾌한 냄새가 납니다.\n",
      "\n",
      "수정 전 문장: 바람이 남서방향족에서 불고 불쾌한 냄새가 납니다.\n",
      "\n",
      "수정 후 문장: 바람이 조선왕조실록에서 불고 불쾌한 냄새가 납니다.\n"
     ]
    }
   ],
   "source": [
    "from hunspell import HunSpell\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Cosine Similarity 계산을 위한 함수\n",
    "def calculate_similarity(word1, word2):\n",
    "    vectorizer = TfidfVectorizer().fit_transform([word1, word2])\n",
    "    vectors = vectorizer.toarray()\n",
    "    return cosine_similarity(vectors)[0, 1]\n",
    "\n",
    "try:\n",
    "    # Hunspell 객체 생성\n",
    "    hunspell = HunSpell(\n",
    "        '/home/yjtech2/Desktop/yurim/LLM/Pre_processing/spelling/hunspell/ko_KR.dic',\n",
    "        '/home/yjtech2/Desktop/yurim/LLM/Pre_processing/spelling/hunspell/ko_KR.aff'\n",
    "\n",
    "    )\n",
    "\n",
    "    # 맞춤법 검사할 텍스트\n",
    "    text = \"바람이 남서방향족에서 불고 불쾌한 냄새가 납니다.\"\n",
    "    \n",
    "    # 문장을 단락으로 나누기\n",
    "    sentences = re.split(r'(?<=[.?!])\\s+', text.strip())\n",
    "    \n",
    "    # 수정된 문장을 저장할 변수\n",
    "    corrected_sentence = []\n",
    "    \n",
    "    # 앞 문장을 저장할 변수\n",
    "    previous_sentence = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = re.findall(r'\\b\\w+\\b', sentence)\n",
    "        \n",
    "        for word in words:\n",
    "            if not hunspell.spell(word):\n",
    "                suggestions = hunspell.suggest(word)\n",
    "                if suggestions:\n",
    "                    # 제안된 단어와 유사도 값을 계산\n",
    "                    similarity_scores = [(suggestion, calculate_similarity(previous_sentence + \" \" + sentence, previous_sentence + \" \" + sentence.replace(word, suggestion))) for suggestion in suggestions]\n",
    "                    \n",
    "                    # 유사도 값을 기준으로 상위 3개의 제안을 선택\n",
    "                    top_3_suggestions = sorted(similarity_scores, key=lambda x: x[1], reverse=True)[:3]\n",
    "                    \n",
    "                    # 선택된 제안 중에서 유사도가 가장 높은 제안을 선택\n",
    "                    best_suggestion = top_3_suggestions[0][0]\n",
    "                    sentence = sentence.replace(word, best_suggestion, 1)\n",
    "                    \n",
    "                    print(f\"\\n단어 '{word}'에 대한 상위 3개 제안 및 유사도:\")\n",
    "                    for suggestion, score in top_3_suggestions:\n",
    "                        print(f\"제안: {suggestion}, 유사도: {score:.4f}\")\n",
    "                    \n",
    "                    print(f\"'{word}'를 '{best_suggestion}'로 수정했습니다.\")\n",
    "                else:\n",
    "                    print(f\"'{word}'에 대한 제안이 없습니다.\")\n",
    "        \n",
    "        corrected_sentence.append(sentence)\n",
    "        previous_sentence = sentence\n",
    "    print('previous_sentence: ', previous_sentence)\n",
    "    \n",
    "    # 교정된 문장 출력\n",
    "    final_text = ' '.join(corrected_sentence)\n",
    "    print(f'\\n수정 전 문장: {text}') \n",
    "    print(f'\\n수정 후 문장: {final_text}')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Hunspell 초기화 중 오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석 + n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "원본 텍스트: 바람이 남서방향족에서 불고 불쾌한 냄새가 납니다.\n",
      "교정된 텍스트: 바람이 남서방향족에서 불고 불쾌한 냄새가 납니다\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Komoran  # Komoran으로 변경\n",
    "from hunspell import HunSpell\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "class EnhancedSpellChecker:\n",
    "    def __init__(self, dict_path, aff_path, ngram_corpus_path=None):\n",
    "        \"\"\"\n",
    "        Enhanced spell checker initialization\n",
    "        Args:\n",
    "            dict_path: Path to Hunspell dictionary\n",
    "            aff_path: Path to Hunspell affix file\n",
    "            ngram_corpus_path: Path to pre-trained n-gram corpus (optional)\n",
    "        \"\"\"\n",
    "        self.hunspell = HunSpell(dict_path, aff_path)\n",
    "        self.komoran = Komoran()  # Komoran 초기화\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            analyzer='char',\n",
    "            ngram_range=(2, 3),  # 2-gram과 3-gram 모두 사용\n",
    "            min_df=2\n",
    "        )\n",
    "        self.ngram_model = self._load_or_create_ngram_model(ngram_corpus_path)\n",
    "        \n",
    "    def _load_or_create_ngram_model(self, corpus_path):\n",
    "        \"\"\"N-gram 모델 로드 또는 생성\"\"\"\n",
    "        if corpus_path and os.path.exists(corpus_path):\n",
    "            with open(corpus_path, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        return defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    def _calculate_contextual_similarity(self, original_context, candidate_context):\n",
    "        \"\"\"문맥 기반 유사도 계산\"\"\"\n",
    "        # 형태소 분석 결과를 활용한 유사도 계산\n",
    "        orig_morphs = ' '.join(self.komoran.morphs(original_context))  # Komoran으로 변경\n",
    "        cand_morphs = ' '.join(self.komoran.morphs(candidate_context))  # Komoran으로 변경\n",
    "        \n",
    "        # TF-IDF 벡터화\n",
    "        vectors = self.vectorizer.fit_transform([orig_morphs, cand_morphs])\n",
    "        \n",
    "        # 코사인 유사도 계산\n",
    "        return cosine_similarity(vectors)[0, 1]\n",
    "\n",
    "    def _calculate_phonetic_similarity(self, word1, word2):\n",
    "        \"\"\"음소 기반 유사도 계산\"\"\"\n",
    "        # 초성, 중성, 종성으로 분리하여 비교\n",
    "        def decompose_hangul(text):\n",
    "            CHOSUNG = ['ㄱ','ㄲ','ㄴ','ㄷ','ㄸ','ㄹ','ㅁ','ㅂ','ㅃ','ㅅ','ㅆ','ㅇ','ㅈ','ㅉ','ㅊ','ㅋ','ㅌ','ㅍ','ㅎ']\n",
    "            JUNGSUNG = ['ㅏ','ㅐ','ㅑ','ㅒ','ㅓ','ㅔ','ㅕ','ㅖ','ㅗ','ㅘ','ㅙ','ㅚ','ㅛ','ㅜ','ㅝ','ㅞ','ㅟ','ㅠ','ㅡ','ㅢ','ㅣ']\n",
    "            JONGSUNG = [' ','ㄱ','ㄲ','ㄳ','ㄴ','ㄵ','ㄶ','ㄷ','ㄹ','ㄺ','ㄻ','ㄼ','ㄽ','ㄾ','ㄿ','ㅀ','ㅁ','ㅂ','ㅄ','ㅅ','ㅆ','ㅇ','ㅈ','ㅊ','ㅋ','ㅌ','ㅍ','ㅎ']\n",
    "            \n",
    "            result = []\n",
    "            for char in text:\n",
    "                if '가' <= char <= '힣':\n",
    "                    char_code = ord(char) - ord('가')\n",
    "                    jong = char_code % 28\n",
    "                    jung = ((char_code - jong) // 28) % 21\n",
    "                    cho = (((char_code - jong) // 28) - jung) // 21\n",
    "                    result.extend([CHOSUNG[cho], JUNGSUNG[jung], JONGSUNG[jong]])\n",
    "            return result\n",
    "\n",
    "        # 자모 단위로 분해하여 유사도 계산\n",
    "        char1 = decompose_hangul(word1)\n",
    "        char2 = decompose_hangul(word2)\n",
    "        \n",
    "        # Levenshtein distance 계산\n",
    "        def levenshtein_distance(s1, s2):\n",
    "            if len(s1) < len(s2):\n",
    "                return levenshtein_distance(s2, s1)\n",
    "            if len(s2) == 0:\n",
    "                return len(s1)\n",
    "            \n",
    "            previous_row = range(len(s2) + 1)\n",
    "            for i, c1 in enumerate(s1):\n",
    "                current_row = [i + 1]\n",
    "                for j, c2 in enumerate(s2):\n",
    "                    insertions = previous_row[j + 1] + 1\n",
    "                    deletions = current_row[j] + 1\n",
    "                    substitutions = previous_row[j] + (c1 != c2)\n",
    "                    current_row.append(min(insertions, deletions, substitutions))\n",
    "                previous_row = current_row\n",
    "            \n",
    "            return previous_row[-1]\n",
    "        \n",
    "        distance = levenshtein_distance(char1, char2)\n",
    "        max_len = max(len(char1), len(char2))\n",
    "        return 1 - (distance / max_len)\n",
    "\n",
    "    def _get_window_context(self, words, current_idx, window_size=2):\n",
    "        \"\"\"주변 단어들의 문맥 추출\"\"\"\n",
    "        start = max(0, current_idx - window_size)\n",
    "        end = min(len(words), current_idx + window_size + 1)\n",
    "        return ' '.join(words[start:end])\n",
    "\n",
    "    def correct_text(self, text, min_similarity=0.6):\n",
    "        \"\"\"텍스트 맞춤법 교정\"\"\"\n",
    "        # 문장 분리\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "        corrected_sentences = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            words = re.findall(r'\\b\\w+\\b', sentence)\n",
    "            corrected_words = words.copy()\n",
    "            \n",
    "            for i, word in enumerate(words):\n",
    "                if not self.hunspell.spell(word):\n",
    "                    suggestions = self.hunspell.suggest(word)\n",
    "                    if suggestions:\n",
    "                        # 문맥 추출\n",
    "                        context = self._get_window_context(words, i)\n",
    "                        \n",
    "                        # 각 제안에 대한 종합 점수 계산\n",
    "                        scored_suggestions = []\n",
    "                        for suggestion in suggestions:\n",
    "                            # 새로운 문맥 생성\n",
    "                            new_context = context.replace(word, suggestion)\n",
    "                            \n",
    "                            # 다양한 유사도 점수 계산\n",
    "                            contextual_score = self._calculate_contextual_similarity(context, new_context)\n",
    "                            phonetic_score = self._calculate_phonetic_similarity(word, suggestion)\n",
    "                            \n",
    "                            # N-gram 확률 계산\n",
    "                            ngram_score = self._calculate_ngram_probability(new_context)\n",
    "                            \n",
    "                            # 종합 점수 계산 (가중치 적용)\n",
    "                            total_score = (\n",
    "                                contextual_score * 0.4 +\n",
    "                                phonetic_score * 0.3 +\n",
    "                                ngram_score * 0.3\n",
    "                            )\n",
    "                            \n",
    "                            scored_suggestions.append((suggestion, total_score))\n",
    "                        \n",
    "                        # 가장 높은 점수의 제안 선택\n",
    "                        scored_suggestions.sort(key=lambda x: x[1], reverse=True)\n",
    "                        best_suggestion, best_score = scored_suggestions[0]\n",
    "                        \n",
    "                        # 최소 유사도 threshold를 넘는 경우에만 수정\n",
    "                        if best_score >= min_similarity:\n",
    "                            corrected_words[i] = best_suggestion\n",
    "                            print(f\"\\n단어 '{word}' 교정:\")\n",
    "                            print(f\"선택된 교정: {best_suggestion} (점수: {best_score:.4f})\")\n",
    "                            for sugg, score in scored_suggestions[:3]:\n",
    "                                print(f\"- 후보: {sugg}, 점수: {score:.4f}\")\n",
    "            \n",
    "            corrected_sentence = ' '.join(corrected_words)\n",
    "            corrected_sentences.append(corrected_sentence)\n",
    "        \n",
    "        return ' '.join(corrected_sentences)\n",
    "\n",
    "    def _calculate_ngram_probability(self, context):\n",
    "        \"\"\"N-gram 확률 계산\"\"\"\n",
    "        words = context.split()\n",
    "        if len(words) < 2:\n",
    "            return 0.5  # 기본값\n",
    "        \n",
    "        total_prob = 0\n",
    "        count = 0\n",
    "        \n",
    "        for i in range(len(words) - 1):\n",
    "            bigram = tuple(words[i:i+2])\n",
    "            if bigram in self.ngram_model:\n",
    "                total_prob += self.ngram_model[bigram] / sum(self.ngram_model[bigram[0]].values())\n",
    "                count += 1\n",
    "        \n",
    "        return total_prob / max(1, count)\n",
    "\n",
    "    def train_ngram_model(self, corpus_text):\n",
    "        \"\"\"N-gram 모델 학습\"\"\"\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', corpus_text.strip())\n",
    "        for sentence in sentences:\n",
    "            words = sentence.split()\n",
    "            for i in range(len(words) - 1):\n",
    "                self.ngram_model[words[i]][words[i+1]] += 1\n",
    "\n",
    "# 사용 예시\n",
    "if __name__ == \"__main__\":\n",
    "    checker = EnhancedSpellChecker(\n",
    "        '/home/yjtech2/Desktop/yurim/LLM/Pre_processing/spelling/hunspell/ko_KR.dic',\n",
    "        '/home/yjtech2/Desktop/yurim/LLM/Pre_processing/spelling/hunspell/ko_KR.aff'\n",
    "    )\n",
    "    \n",
    "    # 테스트 텍스트\n",
    "    test_text = \"바람이 남서방향족에서 불고 불쾌한 냄새가 납니다.\"\n",
    "    \n",
    "    # 맞춤법 교정 실행\n",
    "    corrected_text = checker.correct_text(test_text)\n",
    "    \n",
    "    print(f\"\\n원본 텍스트: {test_text}\")\n",
    "    print(f\"교정된 텍스트: {corrected_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석 + n-gram 확장 + 음소 기반 유사도 개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "단어 '이샹한' 교정:\n",
      "선택된 교정: 이상한 (점수: 0.6667)\n",
      "- 후보: 이상한, 점수: 0.6667\n",
      "- 후보: 이송한, 점수: 0.6667\n",
      "- 후보: 이양한, 점수: 0.6667\n",
      "\n",
      "단어 '스레기' 교정:\n",
      "선택된 교정: 쓰레기 (점수: 0.6667)\n",
      "- 후보: 쓰레기, 점수: 0.6667\n",
      "- 후보: 수레기, 점수: 0.6667\n",
      "\n",
      "원본 텍스트: 바람이 어디에서 부는진 모르겠지만 이샹한 음식물 스레기 냄새가 난다\n",
      "교정된 텍스트: 바람이 어디에서 부는진 모르겠지만 이상한 음식물 쓰레기 냄새가 난다\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt  # Okt로 변경\n",
    "from hunspell import HunSpell\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "class EnhancedSpellChecker:\n",
    "    def __init__(self, dict_path, aff_path, ngram_corpus_path=None):\n",
    "        \"\"\"\n",
    "        Enhanced spell checker initialization\n",
    "        Args:\n",
    "            dict_path: Path to Hunspell dictionary\n",
    "            aff_path: Path to Hunspell affix file\n",
    "            ngram_corpus_path: Path to pre-trained n-gram corpus (optional)\n",
    "        \"\"\"\n",
    "        self.hunspell = HunSpell(dict_path, aff_path)\n",
    "        self.okt = Okt()  # Okt 초기화\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            analyzer='char',\n",
    "            ngram_range=(2, 3),  # 2-gram과 3-gram 모두 사용\n",
    "            min_df=2\n",
    "        )\n",
    "        self.ngram_model = self._load_or_create_ngram_model(ngram_corpus_path)\n",
    "        \n",
    "    def _load_or_create_ngram_model(self, corpus_path):\n",
    "        \"\"\"N-gram 모델 로드 또는 생성\"\"\"\n",
    "        if corpus_path and os.path.exists(corpus_path):\n",
    "            with open(corpus_path, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        return defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    def _calculate_contextual_similarity(self, original_context, candidate_context):\n",
    "        \"\"\"문맥 기반 유사도 계산\"\"\"\n",
    "        # 형태소 분석 결과를 활용한 유사도 계산\n",
    "        orig_morphs = ' '.join(self.okt.morphs(original_context))  # Okt로 변경\n",
    "        cand_morphs = ' '.join(self.okt.morphs(candidate_context))  # Okt로 변경\n",
    "        \n",
    "        # TF-IDF 벡터화\n",
    "        vectors = self.vectorizer.fit_transform([orig_morphs, cand_morphs])\n",
    "        \n",
    "        # 코사인 유사도 계산\n",
    "        return cosine_similarity(vectors)[0, 1]\n",
    "\n",
    "    def _calculate_phonetic_similarity(self, word1, word2):\n",
    "        \"\"\"음소 기반 유사도 계산\"\"\"\n",
    "        # 초성, 중성, 종성으로 분리하여 비교\n",
    "        def decompose_hangul(text):\n",
    "            CHOSUNG = ['ㄱ','ㄲ','ㄴ','ㄷ','ㄸ','ㄹ','ㅁ','ㅂ','ㅃ','ㅅ','ㅆ','ㅇ','ㅈ','ㅉ','ㅊ','ㅋ','ㅌ','ㅍ','ㅎ']\n",
    "            JUNGSUNG = ['ㅏ','ㅐ','ㅑ','ㅒ','ㅓ','ㅔ','ㅕ','ㅖ','ㅗ','ㅘ','ㅙ','ㅚ','ㅛ','ㅜ','ㅝ','ㅞ','ㅟ','ㅠ','ㅡ','ㅢ','ㅣ']\n",
    "            JONGSUNG = [' ','ㄱ','ㄲ','ㄳ','ㄴ','ㄵ','ㄶ','ㄷ','ㄹ','ㄺ','ㄻ','ㄼ','ㄽ','ㄾ','ㄿ','ㅀ','ㅁ','ㅂ','ㅄ','ㅅ','ㅆ','ㅇ','ㅈ','ㅊ','ㅋ','ㅌ','ㅍ','ㅎ']\n",
    "            \n",
    "            result = []\n",
    "            for char in text:\n",
    "                if '가' <= char <= '힣':\n",
    "                    char_code = ord(char) - ord('가')\n",
    "                    jong = char_code % 28\n",
    "                    jung = ((char_code - jong) // 28) % 21\n",
    "                    cho = (((char_code - jong) // 28) - jung) // 21\n",
    "                    result.extend([CHOSUNG[cho], JUNGSUNG[jung], JONGSUNG[jong]])\n",
    "            return result\n",
    "\n",
    "        # 자모 단위로 분해하여 유사도 계산\n",
    "        char1 = decompose_hangul(word1)\n",
    "        char2 = decompose_hangul(word2)\n",
    "        \n",
    "        # Levenshtein distance 계산\n",
    "        def levenshtein_distance(s1, s2):\n",
    "            if len(s1) < len(s2):\n",
    "                return levenshtein_distance(s2, s1)\n",
    "            if len(s2) == 0:\n",
    "                return len(s1)\n",
    "            \n",
    "            previous_row = range(len(s2) + 1)\n",
    "            for i, c1 in enumerate(s1):\n",
    "                current_row = [i + 1]\n",
    "                for j, c2 in enumerate(s2):\n",
    "                    insertions = previous_row[j + 1] + 1\n",
    "                    deletions = current_row[j] + 1\n",
    "                    substitutions = previous_row[j] + (c1 != c2)\n",
    "                    current_row.append(min(insertions, deletions, substitutions))\n",
    "                previous_row = current_row\n",
    "            \n",
    "            return previous_row[-1]\n",
    "        \n",
    "        distance = levenshtein_distance(char1, char2)\n",
    "        max_len = max(len(char1), len(char2))\n",
    "        return 1 - (distance / max_len)\n",
    "\n",
    "    def _get_window_context(self, words, current_idx, window_size=2):\n",
    "        \"\"\"주변 단어들의 문맥 추출\"\"\"\n",
    "        start = max(0, current_idx - window_size)\n",
    "        end = min(len(words), current_idx + window_size + 1)\n",
    "        return ' '.join(words[start:end])\n",
    "\n",
    "    def correct_text(self, text, min_similarity=0.6):\n",
    "        \"\"\"텍스트 맞춤법 교정\"\"\"\n",
    "        # 문장 분리\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "        corrected_sentences = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            words = re.findall(r'\\b\\w+\\b', sentence)\n",
    "            corrected_words = words.copy()\n",
    "            \n",
    "            for i, word in enumerate(words):\n",
    "                if not self.hunspell.spell(word):\n",
    "                    suggestions = self.hunspell.suggest(word)\n",
    "                    if suggestions:\n",
    "                        # 문맥 추출\n",
    "                        context = self._get_window_context(words, i)\n",
    "                        \n",
    "                        # 각 제안에 대한 종합 점수 계산\n",
    "                        scored_suggestions = []\n",
    "                        for suggestion in suggestions:\n",
    "                            # 새로운 문맥 생성\n",
    "                            new_context = context.replace(word, suggestion)\n",
    "                            \n",
    "                            # 다양한 유사도 점수 계산\n",
    "                            contextual_score = self._calculate_contextual_similarity(context, new_context)\n",
    "                            phonetic_score = self._calculate_phonetic_similarity(word, suggestion)\n",
    "                            \n",
    "                            # N-gram 확률 계산\n",
    "                            ngram_score = self._calculate_ngram_probability(new_context)\n",
    "                            \n",
    "                            # 종합 점수 계산 (가중치 적용)\n",
    "                            total_score = (\n",
    "                                contextual_score * 0.4 +\n",
    "                                phonetic_score * 0.3 +\n",
    "                                ngram_score * 0.3\n",
    "                            )\n",
    "                            \n",
    "                            scored_suggestions.append((suggestion, total_score))\n",
    "                        \n",
    "                        # 가장 높은 점수의 제안 선택\n",
    "                        scored_suggestions.sort(key=lambda x: x[1], reverse=True)\n",
    "                        best_suggestion, best_score = scored_suggestions[0]\n",
    "                        \n",
    "                        # 최소 유사도 threshold를 넘는 경우에만 수정\n",
    "                        if best_score >= min_similarity:\n",
    "                            corrected_words[i] = best_suggestion\n",
    "                            print(f\"\\n단어 '{word}' 교정:\")\n",
    "                            print(f\"선택된 교정: {best_suggestion} (점수: {best_score:.4f})\")\n",
    "                            for sugg, score in scored_suggestions[:3]:\n",
    "                                print(f\"- 후보: {sugg}, 점수: {score:.4f}\")\n",
    "            \n",
    "            corrected_sentence = ' '.join(corrected_words)\n",
    "            corrected_sentences.append(corrected_sentence)\n",
    "        \n",
    "        return ' '.join(corrected_sentences)\n",
    "\n",
    "    def _calculate_ngram_probability(self, context):\n",
    "        \"\"\"N-gram 확률 계산\"\"\"\n",
    "        words = context.split()\n",
    "        if len(words) < 2:\n",
    "            return 0.5  # 기본값\n",
    "\n",
    "        total_prob = 0\n",
    "        count = 0\n",
    "\n",
    "        # Bigram 확률 계산\n",
    "        for i in range(len(words) - 1):\n",
    "            bigram = tuple(words[i:i+2])\n",
    "            if bigram in self.ngram_model:\n",
    "                total_prob += self.ngram_model[bigram[0]][bigram[1]] / sum(self.ngram_model[bigram[0]].values())\n",
    "                count += 1\n",
    "\n",
    "        # Trigram 확률 계산\n",
    "        for i in range(len(words) - 2):\n",
    "            trigram = tuple(words[i:i+3])\n",
    "            if (\n",
    "                trigram[0] in self.ngram_model\n",
    "                and trigram[1] in self.ngram_model[trigram[0]]\n",
    "                and trigram[2] in self.ngram_model[trigram[0]][trigram[1]]\n",
    "            ):\n",
    "                total_prob += self.ngram_model[trigram[0]][trigram[1]][trigram[2]] / sum(\n",
    "                    self.ngram_model[trigram[0]][trigram[1]].values()\n",
    "                )\n",
    "                count += 1\n",
    "\n",
    "        return total_prob / max(1, count)\n",
    "\n",
    "    def train_ngram_model(self, corpus_text):\n",
    "        \"\"\"N-gram 모델 학습\"\"\"\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', corpus_text.strip())\n",
    "        for sentence in sentences:\n",
    "            words = sentence.split()\n",
    "            # Bigram 학습\n",
    "            for i in range(len(words) - 1):\n",
    "                bigram = tuple(words[i:i+2])\n",
    "                self.ngram_model[bigram[0]][bigram[1]] += 1\n",
    "            # Trigram 학습\n",
    "            for i in range(len(words) - 2):\n",
    "                trigram = tuple(words[i:i+3])\n",
    "                if trigram[0] not in self.ngram_model:\n",
    "                    self.ngram_model[trigram[0]] = defaultdict(lambda: defaultdict(int))\n",
    "                self.ngram_model[trigram[0]][trigram[1]][trigram[2]] += 1\n",
    "\n",
    "# 사용 예시\n",
    "if __name__ == \"__main__\":\n",
    "    checker = EnhancedSpellChecker(\n",
    "        '/home/yjtech2/Desktop/yurim/LLM/Pre_processing/spelling/hunspell/ko_KR.dic',\n",
    "        '/home/yjtech2/Desktop/yurim/LLM/Pre_processing/spelling/hunspell/ko_KR.aff'\n",
    "    )\n",
    "    \n",
    "    # 테스트 텍스트\n",
    "    test_text = \"바람이 어디에서 부는진 모르겠지만 이샹한 음식물 스레기 냄새가 난다\"\n",
    "    \n",
    "    # 맞춤법 교정 실행\n",
    "    corrected_text = checker.correct_text(test_text)\n",
    "    \n",
    "    print(f\"\\n원본 텍스트: {test_text}\")\n",
    "    print(f\"교정된 텍스트: {corrected_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 성능 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-20 10:02:30.423909] ====== Data Load Start ======\n",
      "[2024-11-20 10:02:30.580781] ====== Data Load Finished ======\n",
      "[2024-11-20 10:02:30.580781] ====== Initializing Spell Checker ======\n",
      "[2024-11-20 10:02:30.580781] ====== Processing Text Correction ======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:14<00:00,  7.03it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "add_comparison_columns() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 302\u001b[0m\n\u001b[1;32m    299\u001b[0m processed_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcor_sentence\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m processed_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcor_sentence\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(text_clean)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# 비교 컬럼 추가\u001b[39;00m\n\u001b[0;32m--> 302\u001b[0m result_df \u001b[38;5;241m=\u001b[39m \u001b[43mspell_checker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_comparison_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# 결과 출력\u001b[39;00m\n\u001b[1;32m    305\u001b[0m _now_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__str__\u001b[39m()\n",
      "\u001b[0;31mTypeError\u001b[0m: add_comparison_columns() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt  # Okt로 변경\n",
    "import json\n",
    "from hunspell import HunSpell\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 데이터 불러오기\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        \n",
    "    data = []\n",
    "    for item in json_data['data']:\n",
    "        if 'annotation' in item:\n",
    "            annotation = item['annotation']\n",
    "            \n",
    "            if 'err_sentence' in annotation and 'cor_sentence' in annotation:\n",
    "                data.append({\n",
    "                    'err_sentence': annotation['err_sentence'],\n",
    "                    'cor_sentence': annotation['cor_sentence']\n",
    "                })\n",
    "                \n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "class EnhancedSpellChecker:\n",
    "    def __init__(self, dict_path, aff_path, ngram_corpus_path=None):\n",
    "        \"\"\"\n",
    "        Enhanced spell checker initialization\n",
    "        Args:\n",
    "            dict_path: Path to Hunspell dictionary\n",
    "            aff_path: Path to Hunspell affix file\n",
    "            ngram_corpus_path: Path to pre-trained n-gram corpus (optional)\n",
    "        \"\"\"\n",
    "        self.hunspell = HunSpell(dict_path, aff_path)\n",
    "        self.okt = Okt()  # Okt 초기화\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            analyzer='char',\n",
    "            ngram_range=(2, 3),  # 2-gram과 3-gram 모두 사용\n",
    "            min_df=2\n",
    "        )\n",
    "        self.ngram_model = self._load_or_create_ngram_model(ngram_corpus_path)\n",
    "        \n",
    "    def _load_or_create_ngram_model(self, corpus_path):\n",
    "        \"\"\"N-gram 모델 로드 또는 생성\"\"\"\n",
    "        if corpus_path and os.path.exists(corpus_path):\n",
    "            with open(corpus_path, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        return defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    def _calculate_contextual_similarity(self, original_context, candidate_context):\n",
    "        \"\"\"문맥 기반 유사도 계산\"\"\"\n",
    "        # 형태소 분석 결과를 활용한 유사도 계산\n",
    "        orig_morphs = ' '.join(self.okt.morphs(original_context))  # Okt로 변경\n",
    "        cand_morphs = ' '.join(self.okt.morphs(candidate_context))  # Okt로 변경\n",
    "        \n",
    "        # TF-IDF 벡터화\n",
    "        vectors = self.vectorizer.fit_transform([orig_morphs, cand_morphs])\n",
    "        \n",
    "        # 코사인 유사도 계산\n",
    "        return cosine_similarity(vectors)[0, 1]\n",
    "\n",
    "    def _calculate_phonetic_similarity(self, word1, word2):\n",
    "        \"\"\"음소 기반 유사도 계산\"\"\"\n",
    "        # 초성, 중성, 종성으로 분리하여 비교\n",
    "        def decompose_hangul(text):\n",
    "            CHOSUNG = ['ㄱ','ㄲ','ㄴ','ㄷ','ㄸ','ㄹ','ㅁ','ㅂ','ㅃ','ㅅ','ㅆ','ㅇ','ㅈ','ㅉ','ㅊ','ㅋ','ㅌ','ㅍ','ㅎ']\n",
    "            JUNGSUNG = ['ㅏ','ㅐ','ㅑ','ㅒ','ㅓ','ㅔ','ㅕ','ㅖ','ㅗ','ㅘ','ㅙ','ㅚ','ㅛ','ㅜ','ㅝ','ㅞ','ㅟ','ㅠ','ㅡ','ㅢ','ㅣ']\n",
    "            JONGSUNG = [' ','ㄱ','ㄲ','ㄳ','ㄴ','ㄵ','ㄶ','ㄷ','ㄹ','ㄺ','ㄻ','ㄼ','ㄽ','ㄾ','ㄿ','ㅀ','ㅁ','ㅂ','ㅄ','ㅅ','ㅆ','ㅇ','ㅈ','ㅊ','ㅋ','ㅌ','ㅍ','ㅎ']\n",
    "            \n",
    "            result = []\n",
    "            for char in text:\n",
    "                if '가' <= char <= '힣':\n",
    "                    char_code = ord(char) - ord('가')\n",
    "                    jong = char_code % 28\n",
    "                    jung = ((char_code - jong) // 28) % 21\n",
    "                    cho = (((char_code - jong) // 28) - jung) // 21\n",
    "                    result.extend([CHOSUNG[cho], JUNGSUNG[jung], JONGSUNG[jong]])\n",
    "            return result\n",
    "\n",
    "        # 자모 단위로 분해하여 유사도 계산\n",
    "        char1 = decompose_hangul(word1)\n",
    "        char2 = decompose_hangul(word2)\n",
    "        \n",
    "        # Levenshtein distance 계산\n",
    "        def levenshtein_distance(s1, s2):\n",
    "            if len(s1) < len(s2):\n",
    "                return levenshtein_distance(s2, s1)\n",
    "            if len(s2) == 0:\n",
    "                return len(s1)\n",
    "            \n",
    "            previous_row = range(len(s2) + 1)\n",
    "            for i, c1 in enumerate(s1):\n",
    "                current_row = [i + 1]\n",
    "                for j, c2 in enumerate(s2):\n",
    "                    insertions = previous_row[j + 1] + 1\n",
    "                    deletions = current_row[j] + 1\n",
    "                    substitutions = previous_row[j] + (c1 != c2)\n",
    "                    current_row.append(min(insertions, deletions, substitutions))\n",
    "                previous_row = current_row\n",
    "            \n",
    "            return previous_row[-1]\n",
    "        \n",
    "        distance = levenshtein_distance(char1, char2)\n",
    "        max_len = max(len(char1), len(char2))\n",
    "        return 1 - (distance / max_len)\n",
    "\n",
    "    def _get_window_context(self, words, current_idx, window_size=2):\n",
    "        \"\"\"주변 단어들의 문맥 추출\"\"\"\n",
    "        start = max(0, current_idx - window_size)\n",
    "        end = min(len(words), current_idx + window_size + 1)\n",
    "        return ' '.join(words[start:end])\n",
    "\n",
    "    def correct_text(self, text, min_similarity=0.6):\n",
    "        \"\"\"텍스트 맞춤법 교정\"\"\"\n",
    "        # 문장 분리\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "        corrected_sentences = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            words = re.findall(r'\\b\\w+\\b', sentence)\n",
    "            corrected_words = words.copy()\n",
    "            \n",
    "            for i, word in enumerate(words):\n",
    "                if not self.hunspell.spell(word):\n",
    "                    suggestions = self.hunspell.suggest(word)\n",
    "                    if suggestions:\n",
    "                        # 문맥 추출\n",
    "                        context = self._get_window_context(words, i)\n",
    "                        \n",
    "                        # 각 제안에 대한 종합 점수 계산\n",
    "                        scored_suggestions = []\n",
    "                        for suggestion in suggestions:\n",
    "                            # 새로운 문맥 생성\n",
    "                            new_context = context.replace(word, suggestion)\n",
    "                            \n",
    "                            # 다양한 유사도 점수 계산\n",
    "                            contextual_score = self._calculate_contextual_similarity(context, new_context)\n",
    "                            phonetic_score = self._calculate_phonetic_similarity(word, suggestion)\n",
    "                            \n",
    "                            # N-gram 확률 계산\n",
    "                            ngram_score = self._calculate_ngram_probability(new_context)\n",
    "                            \n",
    "                            # 종합 점수 계산 (가중치 적용)\n",
    "                            total_score = (\n",
    "                                contextual_score * 0.4 +\n",
    "                                phonetic_score * 0.3 +\n",
    "                                ngram_score * 0.3\n",
    "                            )\n",
    "                            \n",
    "                            scored_suggestions.append((suggestion, total_score))\n",
    "                        \n",
    "                        # 가장 높은 점수의 제안 선택\n",
    "                        scored_suggestions.sort(key=lambda x: x[1], reverse=True)\n",
    "                        best_suggestion, best_score = scored_suggestions[0]\n",
    "                        \n",
    "                        # 최소 유사도 threshold를 넘는 경우에만 수정\n",
    "                        if best_score >= min_similarity:\n",
    "                            corrected_words[i] = best_suggestion\n",
    "                            # print(f\"\\n단어 '{word}' 교정:\")\n",
    "                            # print(f\"선택된 교정: {best_suggestion} (점수: {best_score:.4f})\")\n",
    "                            # for sugg, score in scored_suggestions[:3]:\n",
    "                            #     print(f\"- 후보: {sugg}, 점수: {score:.4f}\")\n",
    "            \n",
    "            corrected_sentence = ' '.join(corrected_words)\n",
    "            corrected_sentences.append(corrected_sentence)\n",
    "        \n",
    "        return ' '.join(corrected_sentences)\n",
    "\n",
    "    def _calculate_ngram_probability(self, context):\n",
    "        \"\"\"N-gram 확률 계산\"\"\"\n",
    "        words = context.split()\n",
    "        if len(words) < 2:\n",
    "            return 0.5  # 기본값\n",
    "        \n",
    "        total_prob = 0\n",
    "        count = 0\n",
    "        \n",
    "        for i in range(len(words) - 1):\n",
    "            bigram = tuple(words[i:i+2])\n",
    "            if bigram in self.ngram_model:\n",
    "                total_prob += self.ngram_model[bigram] / sum(self.ngram_model[bigram[0]].values())\n",
    "                count += 1\n",
    "        \n",
    "        return total_prob / max(1, count)\n",
    "\n",
    "    def train_ngram_model(self, corpus_text):\n",
    "        \"\"\"N-gram 모델 학습\"\"\"\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', corpus_text.strip())\n",
    "        for sentence in sentences:\n",
    "            words = sentence.split()\n",
    "            for i in range(len(words) - 1):\n",
    "                self.ngram_model[words[i]][words[i+1]] += 1\n",
    "\n",
    "\n",
    "    # 결과 확인\n",
    "    def add_comparison_columns(df):\n",
    "        \n",
    "        def calculate_spacing_similarity(row):\n",
    "            \"\"\"\n",
    "            띄어쓰기를 포함한 두 문장 간의 유사도를 계산하는 함수\n",
    "            \"\"\"\n",
    "            if pd.isna(row['res_sentence']) or pd.isna(row['cor_sentence']):\n",
    "                return 0\n",
    "            \n",
    "            # 문장의 띄어쓰기를 유지한 채로 비교\n",
    "            res = row['res_sentence']\n",
    "            cor = row['cor_sentence']\n",
    "            \n",
    "            # 완전히 같으면 1 반환\n",
    "            if res == cor:\n",
    "                return 1.0\n",
    "            \n",
    "            # 다르면 띄어쓰기를 포함한 유사도 계산\n",
    "            similarity = SequenceMatcher(None, res, cor).ratio()\n",
    "            \n",
    "            # 띄어쓰기 패턴의 정확도를 별도로 계산\n",
    "            res_spaces = [i for i, char in enumerate(res) if char == ' ']\n",
    "            cor_spaces = [i for i, char in enumerate(cor) if char == ' ']\n",
    "            \n",
    "            # 공통된 띄어쓰기 위치 수 계산\n",
    "            common_spaces = len(set(res_spaces) & set(cor_spaces))\n",
    "            total_spaces = len(set(res_spaces) | set(cor_spaces))\n",
    "            \n",
    "            # 띄어쓰기 정확도 (공통 띄어쓰기 / 전체 띄어쓰기)\n",
    "            spacing_accuracy = common_spaces / total_spaces if total_spaces > 0 else 1.0\n",
    "            \n",
    "            # 전체 유사도와 띄어쓰기 정확도를 조합 (가중치는 조정 가능)\n",
    "            final_ratio = (similarity + spacing_accuracy) / 2\n",
    "            \n",
    "            return round(final_ratio, 3)\n",
    "        \n",
    "        # check 컬럼 추가 (완전히 같으면 1, 다르면 0)\n",
    "        df['check'] = (df['res_sentence'] == df['cor_sentence']).astype(int)\n",
    "        \n",
    "        # ratio 컬럼 추가 (띄어쓰기를 고려한 유사도 계산)\n",
    "        df['ratio'] = df.apply(calculate_spacing_similarity, axis=1)\n",
    "        \n",
    "        return df\n",
    "\n",
    "# 수행\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 시작 시간 기록\n",
    "    _now_time = datetime.now().__str__()\n",
    "    print(f'[{_now_time}] ====== Data Load Start ======')\n",
    "    \n",
    "    # 데이터 로드\n",
    "    df = load_data('/home/yjtech2/Desktop/yurim/LLM/Data/맞춤법오류_자유게시판.json')\n",
    "    df = df.iloc[:100]\n",
    "    _now_time = datetime.now().__str__()\n",
    "    print(f'[{_now_time}] ====== Data Load Finished ======')\n",
    "    \n",
    "    # Spell Checker 초기화\n",
    "    print(f'[{_now_time}] ====== Initializing Spell Checker ======')\n",
    "    spell_checker = EnhancedSpellChecker(\n",
    "        dict_path='/home/yjtech2/Desktop/yurim/LLM/Pre_processing/spelling/hunspell/ko.dic',  # Hunspell 사전 파일 경로\n",
    "        aff_path='/home/yjtech2/Desktop/yurim/LLM/Pre_processing/spelling/hunspell/ko.aff',  # Hunspell affix 파일 경로\n",
    "    )\n",
    "    \n",
    "    print(f'[{_now_time}] ====== Processing Text Correction ======')\n",
    "    # 오류 문장에 대한 교정 수행\n",
    "    processed_data = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        err_sentence = row['err_sentence']\n",
    "        cor_sentence = row['cor_sentence']\n",
    "        \n",
    "        # 맞춤법 교정 수행\n",
    "        res_sentence = spell_checker.correct_text(err_sentence)\n",
    "        \n",
    "        processed_data.append({\n",
    "            'err_sentence': err_sentence,\n",
    "            'res_sentence': res_sentence,\n",
    "            'cor_sentence': cor_sentence\n",
    "        })\n",
    "    \n",
    "    # DataFrame 생성\n",
    "    processed_df = pd.DataFrame(processed_data)\n",
    "    \n",
    "    # 텍스트 클리닝 함수 정의\n",
    "    def text_clean(text):\n",
    "        pattern = '[^\\w\\s]'  # 특수기호제거\n",
    "        text = re.sub(pattern, '', text)\n",
    "        return text\n",
    "    \n",
    "    # 텍스트 클리닝 적용\n",
    "    processed_df['res_sentence'] = processed_df['res_sentence'].apply(text_clean)\n",
    "    processed_df['cor_sentence'] = processed_df['cor_sentence'].apply(text_clean)\n",
    "    \n",
    "    # 비교 컬럼 추가\n",
    "    result_df = spell_checker.add_comparison_columns(processed_df)\n",
    "    \n",
    "    # 결과 출력\n",
    "    _now_time = datetime.now().__str__()\n",
    "    print(f'[{_now_time}] ====== Evaluation Results ======')\n",
    "    print('정확도: ', (len(result_df[result_df['check'] == 1]) / len(result_df)))\n",
    "    print('비율 평균: ', sum(result_df['ratio']) / len(result_df))\n",
    "    \n",
    "    # 결과 저장 (선택사항)\n",
    "    result_df.to_csv('spell_check_results.csv', index=False, encoding='utf-8-sig')\n",
    "    print(f'[{_now_time}] ====== Results Saved ======')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-20 10:04:58.333196] ====== Data Load Start ======\n",
      "[2024-11-20 10:04:58.547831] ====== Data Load Finished ======\n",
      "[2024-11-20 10:04:58.547831] ====== Data Preprocessing Start ======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing spacing correction:  24%|██▍       | 24/100 [00:07<00:27,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'코로나떄문에'에 대한 제안이 없습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing spacing correction:  39%|███▉      | 39/100 [00:12<00:23,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'배배꼬아논거같타요'에 대한 제안이 없습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing spacing correction:  49%|████▉     | 49/100 [00:16<00:20,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'연대마스터님들'에 대한 제안이 없습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing spacing correction:  56%|█████▌    | 56/100 [00:18<00:18,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'혐오한다햇엇음'에 대한 제안이 없습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing spacing correction:  89%|████████▉ | 89/100 [00:29<00:03,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'수험생기만한건'에 대한 제안이 없습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing spacing correction: 100%|██████████| 100/100 [00:32<00:00,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-20 10:05:31.427945] ====== Data Preprocessing Finished ======\n",
      "비교 완료\n",
      "정확도:  0.07\n",
      "비율 평균:  0.72099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from hunspell import HunSpell\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# 데이터 불러오기\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        \n",
    "    data = []\n",
    "    for item in json_data['data']:\n",
    "        if 'annotation' in item:\n",
    "            annotation = item['annotation']\n",
    "            \n",
    "            if 'err_sentence' in annotation and 'cor_sentence' in annotation:\n",
    "                data.append({\n",
    "                    'err_sentence': annotation['err_sentence'],\n",
    "                    'cor_sentence': annotation['cor_sentence']\n",
    "                    })\n",
    "                \n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "def hunspell_basic(text):\n",
    "    try:\n",
    "        # Hunspell 객체 생성\n",
    "        hunspell = HunSpell(\n",
    "            '/home/yjtech2/Desktop/yurim/LLM/Pre_processing/spelling/hunspell/ko.dic',\n",
    "            '/home/yjtech2/Desktop/yurim/LLM/Pre_processing/spelling/hunspell/ko.aff'\n",
    "        )\n",
    "\n",
    "        # 결과 저장용 리스트\n",
    "        corrected_texts = []\n",
    "\n",
    "        # 문장에서 단어 추출\n",
    "        words = re.findall(r'\\b\\w+\\b', text)\n",
    "        \n",
    "        # 수정된 문장을 저장할 변수\n",
    "        corrected_sentence = text\n",
    "        \n",
    "        for word in words:\n",
    "            if not hunspell.spell(word):\n",
    "                suggestions = hunspell.suggest(word)\n",
    "                \n",
    "                if suggestions:\n",
    "                    corrected_word = suggestions[0]\n",
    "                    corrected_sentence = re.sub(r'\\b' + re.escape(word) + r'\\b', corrected_word, corrected_sentence, count=1)\n",
    "                    # print(f\"'{word}'를 '{corrected_word}'로 수정했습니다.\")\n",
    "                else:\n",
    "                    print(f\"'{word}'에 대한 제안이 없습니다.\")\n",
    "        \n",
    "        corrected_texts.append(corrected_sentence)\n",
    "        # print(f'\\n수정 전 문장: {text}')\n",
    "        # print(f'\\n수정 후 문장: {corrected_sentence}')\n",
    "        \n",
    "        return corrected_texts\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Hunspell 초기화 중 오류 발생: {e}\")\n",
    "        return []\n",
    "\n",
    "def process_dataframe(df, text_column):\n",
    "    # 진행바 표시\n",
    "    tqdm.pandas(desc=\"Processing spacing correction\")\n",
    "    \n",
    "    # 맞춤법 교정 적용\n",
    "    df[\"res_sentence\"] = df[text_column].progress_apply(hunspell_basic)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# 결과 확인\n",
    "def add_comparison_columns(df):\n",
    "    \n",
    "    def calculate_spacing_similarity(row):\n",
    "        \"\"\"\n",
    "        띄어쓰기를 포함한 두 문장 간의 유사도를 계산하는 함수\n",
    "        \"\"\"\n",
    "        if pd.isna(row['res_sentence']) or pd.isna(row['cor_sentence']):\n",
    "            return 0\n",
    "        \n",
    "        # 문장의 띄어쓰기를 유지한 채로 비교\n",
    "        res = row['res_sentence']\n",
    "        cor = row['cor_sentence']\n",
    "        \n",
    "        # 완전히 같으면 1 반환\n",
    "        if res == cor:\n",
    "            return 1.0\n",
    "        \n",
    "        # 다르면 띄어쓰기를 포함한 유사도 계산\n",
    "        similarity = SequenceMatcher(None, res, cor).ratio()\n",
    "        \n",
    "        # 띄어쓰기 패턴의 정확도를 별도로 계산\n",
    "        res_spaces = [i for i, char in enumerate(res) if char == ' ']\n",
    "        cor_spaces = [i for i, char in enumerate(cor) if char == ' ']\n",
    "        \n",
    "        # 공통된 띄어쓰기 위치 수 계산\n",
    "        common_spaces = len(set(res_spaces) & set(cor_spaces))\n",
    "        total_spaces = len(set(res_spaces) | set(cor_spaces))\n",
    "        \n",
    "        # 띄어쓰기 정확도 (공통 띄어쓰기 / 전체 띄어쓰기)\n",
    "        spacing_accuracy = common_spaces / total_spaces if total_spaces > 0 else 1.0\n",
    "        \n",
    "        # 전체 유사도와 띄어쓰기 정확도를 조합 (가중치는 조정 가능)\n",
    "        final_ratio = (similarity + spacing_accuracy) / 2\n",
    "        \n",
    "        return round(final_ratio, 3)\n",
    "    \n",
    "    # check 컬럼 추가 (완전히 같으면 1, 다르면 0)\n",
    "    df['check'] = (df['res_sentence'] == df['cor_sentence']).astype(int)\n",
    "    \n",
    "    # ratio 컬럼 추가 (띄어쓰기를 고려한 유사도 계산)\n",
    "    df['ratio'] = df.apply(calculate_spacing_similarity, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 수행\n",
    "if __name__ == \"__main__\":\n",
    "    _now_time = datetime.now().__str__()\n",
    "    print(f'[{_now_time}] ====== Data Load Start ======')\n",
    "    df = load_data('/home/yjtech2/Desktop/yurim/LLM/Data/맞춤법오류_자유게시판.json')\n",
    "    df = df.iloc[:100]\n",
    "    _now_time = datetime.now().__str__()\n",
    "    print(f'[{_now_time}] ====== Data Load Finished ======')\n",
    "    \n",
    "    print(f'[{_now_time}] ====== Data Preprocessing Start ======')\n",
    "    processed_df = process_dataframe(df, \"err_sentence\")\n",
    "    _now_time = datetime.now().__str__()\n",
    "    print(f'[{_now_time}] ====== Data Preprocessing Finished ======')\n",
    "\n",
    "    processed_df = processed_df[['err_sentence', 'res_sentence', 'cor_sentence']]\n",
    "    \n",
    "    def text_clean(text):\n",
    "        pattern = '([ㄱ-ㅎㅏ-ㅣ]+)'  # 한글 자음, 모음 제거\n",
    "        text = re.sub(pattern, '', text)\n",
    "        \n",
    "        pattern = '[^\\w\\s]'         # 특수기호제거\n",
    "        text = re.sub(pattern, '', text)\n",
    "\n",
    "        return text \n",
    "    \n",
    "    processed_df['res_sentence'] = processed_df['res_sentence'].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "    processed_df['res_sentence'] = processed_df['res_sentence'].apply(text_clean)\n",
    "    processed_df['cor_sentence'] = processed_df['cor_sentence'].apply(text_clean)\n",
    "    \n",
    "    result_df = add_comparison_columns(processed_df)\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(\"비교 완료\")\n",
    "    print('정확도: ', (len(result_df[result_df['check'] == 1]) / len(result_df)))\n",
    "    print('비율 평균: ', sum(result_df['ratio']) / len(result_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>err_sentence</th>\n",
       "      <th>res_sentence</th>\n",
       "      <th>cor_sentence</th>\n",
       "      <th>check</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>오늘도 다너를 외우러 와따요</td>\n",
       "      <td>오늘도 단어를 외우러 왕따요</td>\n",
       "      <td>오늘도 단어를 외우러 왔어요</td>\n",
       "      <td>0</td>\n",
       "      <td>0.933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>오늘은 추석 연휴동안 놀고 먹으며 학회 레포트를 쓰는 영상 준비해보았습니디</td>\n",
       "      <td>오늘은 추석 연휴 동안 놀고 먹으며 학회 리포트를 쓰는 영상 준비해보았습니다</td>\n",
       "      <td>오늘은 추석 연휴 동안 놀고먹으며 학회 리포트를 쓰는 영상 준비해 보았습니다</td>\n",
       "      <td>0</td>\n",
       "      <td>0.613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>요즘 새로 살까말까 고민중인데 대학원 가면 새로 살것 같기도 하구 그러네요</td>\n",
       "      <td>요즘 새로 살 까말까 고 민중인데 대학원 가면 새로 살 것 같기도 하구 그러네요</td>\n",
       "      <td>요즘 새로 살까 말까 고민 중인데 대학원 가면 새로 살 것 같기도 하고 그러네요</td>\n",
       "      <td>0</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>요즘 영어 문제르 안 풀어서 실력이 내려가고 있는거 가타요.</td>\n",
       "      <td>요즘 영어 문제를 안 풀어서 실력이 내려가고 있는가 강타요</td>\n",
       "      <td>요즘 영어 문제를 안 풀어서 실력이 내려가고 있는 거 같아요</td>\n",
       "      <td>0</td>\n",
       "      <td>0.796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>원래 좋아하는 건 잘 찾아 외우는 능력이 잇서서 말이죠</td>\n",
       "      <td>원래 좋아하는 건 잘 찾아 외우는 능력이 이서서 말이죠</td>\n",
       "      <td>원래 좋아하는 것은 잘 찾아 외우는 능력이 있어서 말이죠</td>\n",
       "      <td>0</td>\n",
       "      <td>0.514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>악마가 한 짓이 아닐가하고 의심하는게요</td>\n",
       "      <td>악마가 한 짓이 아닐 가하고 의심하는데요</td>\n",
       "      <td>악마가 한 짓이 아닐까 하고 의심하는데요</td>\n",
       "      <td>0</td>\n",
       "      <td>0.811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>알람이 안 떠가지궁 바로 확인할게요</td>\n",
       "      <td>알라임 안 떠가지 궁 바로 확인할게요</td>\n",
       "      <td>알람이 안 떠가지고 바로 확인할게요</td>\n",
       "      <td>0</td>\n",
       "      <td>0.553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>어떠케 공부하셨는지 여쭤봐두 될까요?</td>\n",
       "      <td>어떠케 공부하셨는지 여쭤봐다 될까요</td>\n",
       "      <td>어떻게 공부하셨는지 여쭤봐도 될까요</td>\n",
       "      <td>0</td>\n",
       "      <td>0.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>어떠케 접근하는게 베스트인가요</td>\n",
       "      <td>어떠케 접근하는가 페스트인가요</td>\n",
       "      <td>어떻게 접근하는 게 베스트인가요</td>\n",
       "      <td>0</td>\n",
       "      <td>0.489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>어릴 땐 부산에서 안 살았어서 거의 안 썼어요</td>\n",
       "      <td>어릴 땐 부산에서 안 살았어서 거의 안 썼어요</td>\n",
       "      <td>어릴 땐 부산에서 안 살아서 거의 안 썼어요</td>\n",
       "      <td>0</td>\n",
       "      <td>0.669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 err_sentence  \\\n",
       "0                             오늘도 다너를 외우러 와따요   \n",
       "1   오늘은 추석 연휴동안 놀고 먹으며 학회 레포트를 쓰는 영상 준비해보았습니디   \n",
       "2   요즘 새로 살까말까 고민중인데 대학원 가면 새로 살것 같기도 하구 그러네요   \n",
       "3           요즘 영어 문제르 안 풀어서 실력이 내려가고 있는거 가타요.   \n",
       "4              원래 좋아하는 건 잘 찾아 외우는 능력이 잇서서 말이죠   \n",
       "..                                        ...   \n",
       "95                      악마가 한 짓이 아닐가하고 의심하는게요   \n",
       "96                        알람이 안 떠가지궁 바로 확인할게요   \n",
       "97                       어떠케 공부하셨는지 여쭤봐두 될까요?   \n",
       "98                           어떠케 접근하는게 베스트인가요   \n",
       "99                  어릴 땐 부산에서 안 살았어서 거의 안 썼어요   \n",
       "\n",
       "                                    res_sentence  \\\n",
       "0                                오늘도 단어를 외우러 왕따요   \n",
       "1     오늘은 추석 연휴 동안 놀고 먹으며 학회 리포트를 쓰는 영상 준비해보았습니다   \n",
       "2   요즘 새로 살 까말까 고 민중인데 대학원 가면 새로 살 것 같기도 하구 그러네요   \n",
       "3               요즘 영어 문제를 안 풀어서 실력이 내려가고 있는가 강타요   \n",
       "4                 원래 좋아하는 건 잘 찾아 외우는 능력이 이서서 말이죠   \n",
       "..                                           ...   \n",
       "95                        악마가 한 짓이 아닐 가하고 의심하는데요   \n",
       "96                          알라임 안 떠가지 궁 바로 확인할게요   \n",
       "97                           어떠케 공부하셨는지 여쭤봐다 될까요   \n",
       "98                              어떠케 접근하는가 페스트인가요   \n",
       "99                     어릴 땐 부산에서 안 살았어서 거의 안 썼어요   \n",
       "\n",
       "                                    cor_sentence  check  ratio  \n",
       "0                                오늘도 단어를 외우러 왔어요      0  0.933  \n",
       "1     오늘은 추석 연휴 동안 놀고먹으며 학회 리포트를 쓰는 영상 준비해 보았습니다      0  0.613  \n",
       "2   요즘 새로 살까 말까 고민 중인데 대학원 가면 새로 살 것 같기도 하고 그러네요      0  0.833  \n",
       "3              요즘 영어 문제를 안 풀어서 실력이 내려가고 있는 거 같아요      0  0.796  \n",
       "4                원래 좋아하는 것은 잘 찾아 외우는 능력이 있어서 말이죠      0  0.514  \n",
       "..                                           ...    ...    ...  \n",
       "95                        악마가 한 짓이 아닐까 하고 의심하는데요      0  0.811  \n",
       "96                           알람이 안 떠가지고 바로 확인할게요      0  0.553  \n",
       "97                           어떻게 공부하셨는지 여쭤봐도 될까요      0  0.921  \n",
       "98                             어떻게 접근하는 게 베스트인가요      0  0.489  \n",
       "99                      어릴 땐 부산에서 안 살아서 거의 안 썼어요      0  0.669  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
