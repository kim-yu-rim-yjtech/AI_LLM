{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yjtech2/Desktop/yurim/anaconda3/envs/venv-llm/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 840000 entries, 0 to 839999\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count   Dtype \n",
      "---  ------   --------------   ----- \n",
      " 0   article  840000 non-null  object\n",
      " 1   summary  840000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 12.8+ MB\n",
      "None\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    JSON 데이터를 불러와 report와 summary만 Pandas DataFrame으로 변환\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        json_data = json.load(f)\n",
    "    \n",
    "    # 'report'와 'summary'만 추출\n",
    "    data = []\n",
    "    for item in json_data:\n",
    "        if 'article' in item and 'summary' in item:\n",
    "            data.append({\n",
    "                'article': item['article'],\n",
    "                'summary': item['summary']\n",
    "            })\n",
    "    \n",
    "    # DataFrame 생성\n",
    "    df = pd.DataFrame(data)\n",
    "    print(df.info())\n",
    "    # None 값을 빈 문자열로 대체\n",
    "    df = df.fillna('')\n",
    "    return df\n",
    "\n",
    "# JSON 파일 경로\n",
    "file_path = \"/home/yjtech2/Desktop/yurim/LLM/Data/summary/train/odor_complaints_dataset.json\"\n",
    "\n",
    "# 데이터 불러오기\n",
    "df = load_data(file_path)\n",
    "\n",
    "# 데이터셋 변환 및 분할\n",
    "data_dict = df.to_dict(orient='list')\n",
    "train_dataset = Dataset.from_dict(data_dict)\n",
    "train_dataset = train_dataset.select(range(len(train_dataset) - 100000, len(train_dataset)))\n",
    "print(len(train_dataset))\n",
    "valid_dataset = Dataset.from_dict({\"err_sentence\": [], \"cor_sentence\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 13:33:53.358088: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-22 13:33:53.383315: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-22 13:33:53.739262: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from typing import Dict\n",
    "from transformers import (\n",
    "    BartTokenizer,\n",
    "    BartForConditionalGeneration,\n",
    "    AdamW,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration, AdamW, DataCollatorForSeq2Seq\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "class KoBARTSummarizationTrainer:\n",
    "    def __init__(self, model_name: str, **kwargs):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)  # force_download 제거\n",
    "        self.model = BartForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=kwargs.get(\"learning_rate\", 2e-5))\n",
    "        self.max_length = kwargs.get(\"max_length\", 512)\n",
    "        self.training_args = kwargs\n",
    "        self.save_dir = kwargs.get(\"save_dir\", \"/home/yjtech2/Desktop/yurim/LLM/Pre_processing/summary/kobart/kobart_model\")\n",
    "\n",
    "        self.best_model_path = os.path.join(self.save_dir, \"pytorch_model.bin\")\n",
    "        self.tokenizer_path = self.save_dir\n",
    "\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "    def save_model_and_tokenizer(self):\n",
    "        \"\"\"모델과 토크나이저 저장\"\"\"\n",
    "        self.model.save_pretrained(self.save_dir)\n",
    "        self.tokenizer.save_pretrained(self.save_dir)\n",
    "        print(f\"Model and tokenizer saved at {self.save_dir}\")\n",
    "\n",
    "    def preprocess_data(self, examples: Dict) -> Dict:\n",
    "        \"\"\"데이터 전처리 함수\"\"\"\n",
    "        inputs = [f\"요약: {self._normalize_text(text)}\" for text in examples[\"article\"]]\n",
    "        model_inputs = self.tokenizer(\n",
    "            inputs,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"longest\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            labels = self.tokenizer(\n",
    "                examples[\"summary\"],\n",
    "                max_length=self.max_length,\n",
    "                padding=\"longest\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    def train(self, train_dataset, valid_dataset=None):\n",
    "        train_dataset = train_dataset.map(\n",
    "            self.preprocess_data,\n",
    "            batched=True,\n",
    "            remove_columns=train_dataset.column_names,\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer=self.tokenizer,\n",
    "            model=self.model,\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        train_dataloader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.training_args[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            collate_fn=data_collator\n",
    "        )\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(self.training_args[\"num_epochs\"]):\n",
    "            epoch_loss = 0\n",
    "            self.model.zero_grad()\n",
    "\n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels,\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                loss = loss / self.training_args.get(\"gradient_accumulation_steps\", 1)\n",
    "                loss.backward()\n",
    "\n",
    "                if (step + 1) % self.training_args.get(\"gradient_accumulation_steps\", 1) == 0:\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch + 1} completed. Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        self.save_model_and_tokenizer()\n",
    "\n",
    "    def predict(self, text: str) -> str:\n",
    "        inputs = self.tokenizer(\n",
    "            f\"요약: {self._normalize_text(text)}\",\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "        ).to(self.device)\n",
    "\n",
    "        outputs = self.model.generate(\n",
    "            inputs[\"input_ids\"], max_length=self.max_length, num_beams=5\n",
    "        )\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def _normalize_text(self, text: str) -> str:\n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "/home/yjtech2/Desktop/yurim/anaconda3/envs/venv-llm/lib/python3.8/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]/home/yjtech2/Desktop/yurim/anaconda3/envs/venv-llm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:4114: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 100000/100000 [00:04<00:00, 23497.90 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Loss: 100.3176\n",
      "Epoch 2 completed. Loss: 0.5048\n",
      "Epoch 3 completed. Loss: 0.6213\n",
      "Epoch 4 completed. Loss: 0.0541\n",
      "Epoch 5 completed. Loss: 0.0869\n",
      "Model and tokenizer saved at /home/yjtech2/Desktop/yurim/LLM/Pre_processing/summary/kobart/kobart_model\n"
     ]
    }
   ],
   "source": [
    "trainer = KoBARTSummarizationTrainer(\n",
    "    model_name=\"gogamza/kobart-summarization\",\n",
    "    max_length=512,\n",
    "    learning_rate=2e-5,\n",
    "    batch_size=8,\n",
    "    num_epochs=5,\n",
    "    gradient_accumulation_steps=8\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()  # GPU 메모리 초기화\n",
    "    trainer.train(train_dataset, valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "음식물 쓰레기 냄새 문제\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# 저장된 모델 경로\n",
    "saved_model_path = \"/home/yjtech2/Desktop/yurim/LLM/Pre_processing/summary/kobart/kobart_model\"\n",
    "\n",
    "# 모델과 토크나이저 불러오기\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(saved_model_path)\n",
    "model = BartForConditionalGeneration.from_pretrained(saved_model_path).to('cpu')\n",
    "\n",
    "# 테스트용 텍스트\n",
    "text = \"음식물 쓰레기 냄새가 너무 심하게 나고 있습니다\"\n",
    "inputs = tokenizer(\n",
    "    f\"요약: {text}\",\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=512,\n",
    "    truncation=True\n",
    ").to(\"cpu\")\n",
    "\n",
    "# 요약 생성\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=128,  # 요약문 길이 설정\n",
    "    num_beams=5,\n",
    "    length_penalty=0.6,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# 결과 디코딩 및 출력\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
