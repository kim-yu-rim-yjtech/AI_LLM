{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    JSON 데이터를 불러와 Pandas DataFrame으로 변환\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        json_data = json.load(f)\n",
    "        \n",
    "    data = []\n",
    "    for item in json_data['data']:\n",
    "        if 'text' in item and 'keyword' in item:\n",
    "            data.append({\n",
    "                'text': item['text'],\n",
    "                'keyword': item['keyword']\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(df.info())\n",
    "    # None 값을 빈 문자열로 대체\n",
    "    df = df.fillna('')\n",
    "    return df\n",
    "\n",
    "\n",
    "# JSON 파일 경로\n",
    "file_path_time = \"/home/yjtech2/Desktop/yurim/LLM/Data/smell_keyword/time_data.json\"\n",
    "time_df = load_data(file_path_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터를 70:30 비율로 나누기\n",
    "def split_data(df, train_ratio = 0.8):\n",
    "    \"\"\"\n",
    "    DataFrame을 train과 val로 나눔\n",
    "    \"\"\"\n",
    "    train_df, val_df = train_test_split(df, train_size = train_ratio, random_state = 42, shuffle = True)\n",
    "    return train_df, val_df\n",
    "\n",
    "# train, val로 나누기\n",
    "train_time_df, val_time_df = split_data(time_df)\n",
    "\n",
    "train_time_df.to_csv('/home/yjtech2/Desktop/yurim/LLM/Data/smell_keyword/train_time_df.csv', index = False)\n",
    "val_time_df.to_csv('/home/yjtech2/Desktop/yurim/LLM/Data/smell_keyword/val_time_df.csv', index = False)\n",
    "\n",
    "# 나눈 데이터 확인\n",
    "print('Time')\n",
    "print(\"Train Data:\")\n",
    "print(train_time_df.head())\n",
    "print(\"\\nValidation Data:\")\n",
    "print(val_time_df.head())\n",
    "\n",
    "# 데이터 크기 확인\n",
    "print(f\"Train size: {len(train_time_df)}, Validation size: {len(val_time_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_convert_to_list(value):\n",
    "    try:\n",
    "        # JSON 형식으로 변환 가능한지 확인\n",
    "        return json.loads(value) if isinstance(value, str) else value\n",
    "    except json.JSONDecodeError:\n",
    "        # 변환이 불가능한 경우 그대로 반환\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "train_time_df = pd.read_csv('/home/yjtech2/Desktop/yurim/LLM/Data/smell_keyword/train_time_df.csv')\n",
    "val_time_df = pd.read_csv('/home/yjtech2/Desktop/yurim/LLM/Data/smell_keyword/val_time_df.csv')\n",
    "\n",
    "\n",
    "# 데이터셋 전체 변환\n",
    "if train_time_df['keyword'].dtype == 'object':  # keyword가 문자열인지 확인\n",
    "    train_time_df['keyword'] = train_time_df['keyword'].apply(safe_convert_to_list)  # 또는 json.loads\n",
    "\n",
    "# val_df의 'keyword' 컬럼 문자열 -> 리스트 변환\n",
    "if val_time_df['keyword'].dtype == 'object':  # keyword가 문자열인지 확인\n",
    "    val_time_df['keyword'] = val_time_df['keyword'].apply(safe_convert_to_list)  # 또는 json.loads\n",
    "\n",
    "train_time_data_dict = train_time_df.to_dict(orient='list')\n",
    "time_train_dataset = Dataset.from_dict(train_time_data_dict)\n",
    "time_train_dataset\n",
    "print(len(time_train_dataset))\n",
    "\n",
    "val_time_data_dict = val_time_df.to_dict(orient='list')\n",
    "time_val_dataset = Dataset.from_dict(val_time_data_dict)\n",
    "print(len(time_val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from typing import Dict\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AdamW,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from mecab import MeCab\n",
    "\n",
    "class CustomKeyBERTTrainer:\n",
    "    def __init__(self, model_name: str, **kwargs):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        if self.device == \"cuda\":\n",
    "            print(f\"GPU Model: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=kwargs.get(\"learning_rate\", 1e-4))\n",
    "        self.max_length = kwargs.get(\"max_length\", 128)\n",
    "        self.training_args = kwargs\n",
    "        self.save_dir = kwargs.get(\"save_dir\", \"./time_models\")\n",
    "        self.mecab = MeCab()  # MeCab 초기화\n",
    "        self.best_model_path = os.path.join(self.save_dir, \"pytorch_model.bin\")\n",
    "        self.tokenizer_path = self.save_dir\n",
    "        \n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'epoch_times': [],\n",
    "            'best_epoch': 0\n",
    "        }\n",
    "\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        \n",
    "    def _mecab_tokenize(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        MeCab을 사용하여 입력 텍스트에서 명사, 형용사, 부사를 추출\n",
    "        \"\"\"\n",
    "        pos_tags = self.mecab.pos(text)  # 품사 태그 추출\n",
    "        # 원하는 품사 필터링: 명사(NNG, NNP), 형용사(VA), 부사(MAG, MAJ)\n",
    "        keyword = [\n",
    "            word for word, pos in pos_tags if pos in (\"NNG\", \"NNP\", \"VA\", \"MAG\", \"MAJ\")\n",
    "        ]\n",
    "        return \" \".join(keyword)  # 추출한 단어를 공백으로 연결하여 반환\n",
    "    \n",
    "    def preprocess_data(self, examples: Dict) -> Dict:\n",
    "        # MeCab 형태소 분석과 품사 필터링 적용\n",
    "        examples[\"text\"] = [self._mecab_tokenize(text) for text in examples[\"text\"]]\n",
    "        \n",
    "        # 기존 코드 유지\n",
    "        inputs = [f\"키워드 추출: {text}\" for text in examples[\"text\"]]\n",
    "        model_inputs = self.tokenizer(\n",
    "            inputs,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=None  # 텐서 변환을 DataCollator에 맡김\n",
    "        )\n",
    "\n",
    "        # 레이블(키워드) 처리\n",
    "        labels = [\", \".join(keyword) if keyword else \"\" for keyword in examples[\"keyword\"]]\n",
    "        \n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            tokenized_labels = self.tokenizer(\n",
    "                labels,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=None  # 텐서 변환을 DataCollator에 맡김\n",
    "            )\n",
    "\n",
    "        # -100으로 패딩 토큰을 마스킹\n",
    "        labels = tokenized_labels[\"input_ids\"]\n",
    "        for i in range(len(labels)):\n",
    "            for j in range(len(labels[i])):\n",
    "                if labels[i][j] == self.tokenizer.pad_token_id:\n",
    "                    labels[i][j] = -100\n",
    "\n",
    "        model_inputs[\"labels\"] = labels\n",
    "        return model_inputs\n",
    "        \n",
    "    def save_model_and_tokenizer(self, epoch=None, is_best=False):\n",
    "        \"\"\"\n",
    "        최고 성능 모델만 저장하고 이전 모델을 삭제\n",
    "        \"\"\"\n",
    "        if is_best:\n",
    "            # 이전 최고 모델 디렉토리 삭제\n",
    "            if os.path.exists(self.best_model_path):\n",
    "                print(f\"Deleting previous best model at {self.best_model_path}\")\n",
    "                os.system(f\"rm -rf {self.best_model_path}\")\n",
    "            \n",
    "            # 새로운 최고 모델 저장\n",
    "            save_path = os.path.join(self.save_dir, f\"best_model_epoch_{epoch}\")\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            self.model.save_pretrained(save_path)\n",
    "            self.tokenizer.save_pretrained(save_path)\n",
    "            torch.save(self.history, os.path.join(save_path, 'training_history.pt'))\n",
    "            print(f\"New best model saved at {save_path}\")\n",
    "\n",
    "            # 최고 모델 경로 업데이트\n",
    "            self.best_model_path = save_path\n",
    "\n",
    "    def calculate_metrics(self, predictions, labels):\n",
    "        predictions = torch.argmax(predictions, dim=-1)\n",
    "        correct = (predictions == labels).masked_fill(labels == -100, 0)\n",
    "        accuracy = correct.sum().item() / (labels != -100).sum().item()\n",
    "        return accuracy\n",
    "\n",
    "    def train(self, train_dataset, valid_dataset=None):\n",
    "        start_time = time.time()\n",
    "        print(f\"\\nStarting training at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"Training parameters:\")\n",
    "        print(f\"- Batch size: {self.training_args['batch_size']}\")\n",
    "        print(f\"- Learning rate: {self.training_args.get('learning_rate', '2e-5')}\")\n",
    "        print(f\"- Max length: {self.max_length}\")\n",
    "        print(f\"- Number of epochs: {self.training_args['num_epochs']}\")\n",
    "        print(f\"- Training samples: {len(train_dataset)}\")\n",
    "        if valid_dataset:\n",
    "            print(f\"- Validation samples: {len(valid_dataset)}\")\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "        # 데이터셋 전처리\n",
    "        print(\"Preprocessing training data...\")\n",
    "        train_dataset = train_dataset.map(\n",
    "            self.preprocess_data,\n",
    "            batched=True,\n",
    "            remove_columns=train_dataset.column_names,\n",
    "            desc=\"Processing training data\"\n",
    "        )\n",
    "\n",
    "        if valid_dataset is not None:\n",
    "            print(\"Preprocessing validation data...\")\n",
    "            valid_dataset = valid_dataset.map(\n",
    "                self.preprocess_data,\n",
    "                batched=True,\n",
    "                remove_columns=valid_dataset.column_names,\n",
    "                desc=\"Processing validation data\"\n",
    "            )\n",
    "\n",
    "        # DataCollator 설정\n",
    "        data_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer=self.tokenizer,\n",
    "            model=self.model,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # DataLoader 설정 (num_workers=0으로 변경하여 멀티프로세싱 관련 오류 방지)\n",
    "        train_dataloader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.training_args[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            collate_fn=data_collator,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        if valid_dataset is not None:\n",
    "            valid_dataloader = torch.utils.data.DataLoader(\n",
    "                valid_dataset,\n",
    "                batch_size=self.training_args[\"batch_size\"],\n",
    "                shuffle=False,\n",
    "                collate_fn=data_collator,\n",
    "                num_workers=0,\n",
    "                pin_memory=True\n",
    "            )\n",
    "        best_val_loss = float('inf')\n",
    "        early_stopping_counter = 0\n",
    "        early_stopping_patience = self.training_args.get('patience', 3)\n",
    "\n",
    "        for epoch in range(self.training_args[\"num_epochs\"]):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            self.model.train()\n",
    "            epoch_loss = 0\n",
    "            epoch_accuracy = 0\n",
    "            train_steps = 0\n",
    "            \n",
    "            progress_bar = tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}\")\n",
    "            batch_losses = []\n",
    "            batch_accuracies = []\n",
    "            \n",
    "            for batch_idx, batch in enumerate(progress_bar):\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels,\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss # 손실 계산\n",
    "                accuracy = self.calculate_metrics(outputs.logits, labels)\n",
    "                \n",
    "                loss.backward()# 손실 역전파 \n",
    "                self.optimizer.step() # 가중치 업데이트\n",
    "                self.optimizer.zero_grad() # 그래디언트 초기화\n",
    "\n",
    "                batch_losses.append(loss.item())\n",
    "                batch_accuracies.append(accuracy)\n",
    "                \n",
    "                current_loss = np.mean(batch_losses[-100:])\n",
    "                current_accuracy = np.mean(batch_accuracies[-100:])\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f'{current_loss:.4f}',\n",
    "                    'accuracy': f'{current_accuracy:.4f}',\n",
    "                    'batch': f'{batch_idx + 1}/{len(train_dataloader)}'\n",
    "                })\n",
    "\n",
    "            avg_train_loss = np.mean(batch_losses)\n",
    "            avg_train_accuracy = np.mean(batch_accuracies)\n",
    "\n",
    "            if valid_dataset is not None:\n",
    "                self.model.eval()\n",
    "                val_losses = []\n",
    "                val_accuracies = []\n",
    "\n",
    "                print(\"\\nRunning validation...\")\n",
    "                with torch.no_grad():\n",
    "                    for batch in tqdm(valid_dataloader, desc=\"Validating\"):\n",
    "                        input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                        attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                        labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "                        outputs = self.model(\n",
    "                            input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            labels=labels,\n",
    "                        )\n",
    "                        \n",
    "                        loss = outputs.loss\n",
    "                        accuracy = self.calculate_metrics(outputs.logits, labels)\n",
    "                        \n",
    "                        val_losses.append(loss.item())\n",
    "                        val_accuracies.append(accuracy)\n",
    "\n",
    "                avg_val_loss = np.mean(val_losses)\n",
    "                avg_val_accuracy = np.mean(val_accuracies)\n",
    "\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    early_stopping_counter = 0\n",
    "                    self.history['best_epoch'] = epoch + 1\n",
    "                    print(f\"\\nNew best validation loss: {best_val_loss:.4f}\")\n",
    "                    self.save_model_and_tokenizer(epoch + 1, is_best=True)  # 최고 모델만 저장\n",
    "                else:\n",
    "                    early_stopping_counter += 1\n",
    "\n",
    "\n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "            self.history['epoch_times'].append(epoch_time)\n",
    "            self.history['train_loss'].append(avg_train_loss)\n",
    "            if valid_dataset is not None:\n",
    "                self.history['val_loss'].append(avg_val_loss)\n",
    "\n",
    "            # Print epoch summary\n",
    "            print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "            print(f\"Time taken: {epoch_time:.2f} seconds\")\n",
    "            print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "            print(f\"Training accuracy: {avg_train_accuracy:.4f}\")\n",
    "            if valid_dataset is not None:\n",
    "                print(f\"Validation loss: {avg_val_loss:.4f}\")\n",
    "                print(f\"Validation accuracy: {avg_val_accuracy:.4f}\")\n",
    "                print(f\"Best validation loss so far: {best_val_loss:.4f}\")\n",
    "                print(f\"Early stopping counter: {early_stopping_counter}/{early_stopping_patience}\")\n",
    "\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print(\"\\nEarly stopping triggered.\")\n",
    "                break\n",
    "    def predict(self, text: str) -> str:\n",
    "        \"\"\"모델 추론\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            f\"키워드 추출: {self._normalize_text(text)}\",\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "        ).to(self.device)\n",
    "\n",
    "        outputs = self.model.generate(\n",
    "            inputs[\"input_ids\"], max_length=self.max_length, num_beams=5\n",
    "        )\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def _normalize_text(self, text: str) -> str:\n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # 멀티프로세싱 경고 방지\n",
    "\n",
    "trainer = CustomKeyBERTTrainer(\n",
    "    model_name=\"facebook/bart-base\", # t5-base     skt/kobart-base-v2\n",
    "    max_length=128,\n",
    "    learning_rate=1e-4,\n",
    "    batch_size=8,\n",
    "    num_epochs=10,\n",
    "    gradient_accumulation_steps=8,\n",
    "    patience=3 # 몇 에폭마다 체크포인트 저장할지\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()  # GPU 메모리 초기화\n",
    "    trainer.train(time_train_dataset, time_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "def test_model(text, saved_model_path, type):\n",
    "\n",
    "    # 모델과 토크나이저 불러오기\n",
    "    tokenizer = AutoTokenizer.from_pretrained(saved_model_path, local_files_only=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(saved_model_path, local_files_only=True).to('cpu')\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        f\"키워드 추출: {text}\",\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=128,\n",
    "        truncation=True\n",
    "    ).to(\"cpu\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=128,\n",
    "        num_beams=5,\n",
    "        length_penalty=0.7,\n",
    "        repetition_penalty=1.2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"원래 문장: \", text)\n",
    "    print(f\"{type}키워드 추출: {result}\")\n",
    "    \n",
    "\n",
    "text = '저희 포스코에서 아침에 쓰레기장 인근 불쾌한 냄새로 인해 불편을 겪고 있어 연락드립니다. 감사합니다. 도시 환경 관리팀입니다. 포스코에서 발생한 산성 냄새 문제와 관련해 상담 드리겠습니다. 최근 포스코에서 1시 산성 냄새이 축축하게 나고 있어 주변 환경 개선 필요로 큰 불편을 겪고 있습니다.\\\n",
    "    현재 포스코에서 발생한 쓰레기장 인근 불쾌한 냄새 문제에 대해 조치 중입니다. 1시에 완료될 예정입니다. 네, 처리가 되도록 부탁드립니다. 네, 알겠습니다. 잘 부탁드립니다. 감사합니다. 빠르게 처리하겠습니다.'\n",
    "        \n",
    "\n",
    "print()\n",
    "\n",
    "test_model(text, '/home/yjtech2/Desktop/yurim/LLM/Pre_processing/smell_keyword/time_models/best_model_epoch_6', 'time')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
