{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6177900 entries, 0 to 6177899\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Dtype \n",
      "---  ------    ----- \n",
      " 0   text      object\n",
      " 1   keywords  object\n",
      "dtypes: object(2)\n",
      "memory usage: 94.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    JSON 데이터를 불러와 Pandas DataFrame으로 변환\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        json_data = json.load(f)\n",
    "        \n",
    "    data = []\n",
    "    for item in json_data['data']:\n",
    "        if 'text' in item and 'keywords' in item:\n",
    "            data.append({\n",
    "                'text': item['text'],\n",
    "                'keywords': item['keywords']\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(df.info())\n",
    "    # None 값을 빈 문자열로 대체\n",
    "    df = df.fillna('')\n",
    "    return df\n",
    "\n",
    "\n",
    "# JSON 파일 경로\n",
    "file_path_train = \"/home/yjtech2/Desktop/yurim/LLM/Data/smell_keyword/smell_keyword_train.json\"\n",
    "df = load_data(file_path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>쓰레기 매립장에서 악취가 쾌쾌하게 발생하고 있습니다. 이로 인해 주민들의 일상생활에...</td>\n",
       "      <td>[쓰레기 매립장, 악취, 쾌쾌하게]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>쓰레기 매립장에서 악취가 쾌쾌하게 발생하고 있습니다. 이는 주변 환경을 심각하게 오...</td>\n",
       "      <td>[쓰레기 매립장, 악취, 쾌쾌하게]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>쓰레기 매립장에서 악취가 쾌쾌하게 발생하고 있습니다. 이로 인해 주민들이 극심한 불...</td>\n",
       "      <td>[쓰레기 매립장, 악취, 쾌쾌하게]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>쓰레기 매립장에서 악취가 쾌쾌하게 발생하고 있습니다. 이 문제로 인해 지역 주민들이...</td>\n",
       "      <td>[쓰레기 매립장, 악취, 쾌쾌하게]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>쓰레기 매립장에서 악취가 쾌쾌하게 발생하고 있습니다. 주변의 자연 생태계까지 악영향...</td>\n",
       "      <td>[쓰레기 매립장, 악취, 쾌쾌하게]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text             keywords\n",
       "0  쓰레기 매립장에서 악취가 쾌쾌하게 발생하고 있습니다. 이로 인해 주민들의 일상생활에...  [쓰레기 매립장, 악취, 쾌쾌하게]\n",
       "1  쓰레기 매립장에서 악취가 쾌쾌하게 발생하고 있습니다. 이는 주변 환경을 심각하게 오...  [쓰레기 매립장, 악취, 쾌쾌하게]\n",
       "2  쓰레기 매립장에서 악취가 쾌쾌하게 발생하고 있습니다. 이로 인해 주민들이 극심한 불...  [쓰레기 매립장, 악취, 쾌쾌하게]\n",
       "3  쓰레기 매립장에서 악취가 쾌쾌하게 발생하고 있습니다. 이 문제로 인해 지역 주민들이...  [쓰레기 매립장, 악취, 쾌쾌하게]\n",
       "4  쓰레기 매립장에서 악취가 쾌쾌하게 발생하고 있습니다. 주변의 자연 생태계까지 악영향...  [쓰레기 매립장, 악취, 쾌쾌하게]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df['keywords']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample = df.sample(n = 100)\n",
    "random_sample.to_csv('./mini_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "                                                      text  \\\n",
      "5918002  도심 개발 공사장에서 불쾌한 냄새가 오랫동안 남아있는 발생하고 있습니다. 이 문제로...   \n",
      "5957914  운동장 공사장에서 먼지가 오랫동안 남아있는 발생하고 있습니다. 공사 현장에서 발생한...   \n",
      "2907278  사료 공장에서 말린 사료 냄새가 압박감 있게 발생하고 있습니다. 악취 문제로 인해 ...   \n",
      "268308   공원 주변 쓰레기장에서 침투성 냄새가 거슬리게 발생하고 있습니다. 이 문제로 인해 ...   \n",
      "4675819  집 근처 쓰레기통에서 마른 흙 냄새가 혼합되게 발생하고 있습니다. 냄새로 인해 창문...   \n",
      "\n",
      "                           keywords  \n",
      "5918002         [도심 개발 공사장, 불쾌한 냄새]  \n",
      "5957914               [운동장 공사장, 먼지]  \n",
      "2907278   [사료 공장, 말린 사료 냄새, 압박감 있게]  \n",
      "268308   [공원 주변 쓰레기장, 침투성 냄새, 거슬리게]  \n",
      "4675819  [집 근처 쓰레기통, 마른 흙 냄새, 혼합되게]  \n",
      "\n",
      "Validation Data:\n",
      "                                                      text  \\\n",
      "5961388  운동장 공사장에서 기름 냄새가 스산하게 발생하고 있습니다. 공사장 문제로 주민들이 ...   \n",
      "5604857  공공시설 공사 현장에서 고무 타는 냄새가 역하게 발생하고 있습니다. 환경 단체가 문...   \n",
      "5996629  철도 터널 공사장에서 고무 타는 냄새가 자극적으로 발생하고 있습니다. 먼지와 냄새가...   \n",
      "3496443  해가 뜨기 전에서 바람을 타고 퍼지는 냄새가 날카롭게 발생하고 있습니다. 주민들이 ...   \n",
      "679929   도로 공사 현장 쓰레기장에서 더러운 냄새가 끈질기게 발생하고 있습니다. 악취 문제 ...   \n",
      "\n",
      "                              keywords  \n",
      "5961388         [운동장 공사장, 기름 냄새, 스산하게]  \n",
      "5604857                   [공공시설 공사 현장]  \n",
      "5996629                             []  \n",
      "3496443                             []  \n",
      "679929   [도로 공사 현장 쓰레기장, 더러운 냄새, 끈질기게]  \n",
      "Train size: 4324530, Validation size: 1853370\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터를 70:30 비율로 나누기\n",
    "def split_data(df, train_ratio = 0.7):\n",
    "    \"\"\"\n",
    "    DataFrame을 train과 val로 나눔\n",
    "    \"\"\"\n",
    "    train_df, val_df = train_test_split(df, train_size = train_ratio, random_state = 42, shuffle = True)\n",
    "    return train_df, val_df\n",
    "\n",
    "# train, val로 나누기\n",
    "train_df, val_df = split_data(df)\n",
    "\n",
    "train_df.to_csv('/home/yjtech2/Desktop/yurim/LLM/Data/smell_keyword/train_df.csv', index = False)\n",
    "val_df.to_csv('/home/yjtech2/Desktop/yurim/LLM/Data/smell_keyword/val_df.csv', index = False)\n",
    "\n",
    "# 나눈 데이터 확인\n",
    "print(\"Train Data:\")\n",
    "print(train_df.head())\n",
    "print(\"\\nValidation Data:\")\n",
    "print(val_df.head())\n",
    "\n",
    "# 데이터 크기 확인\n",
    "print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4324530\n",
      "1853370\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "train_df = pd.read_csv('/home/yjtech2/Desktop/yurim/LLM/Data/smell_keyword/train_df.csv')\n",
    "val_df = pd.read_csv('/home/yjtech2/Desktop/yurim/LLM/Data/smell_keyword/val_df.csv')\n",
    "\n",
    "\n",
    "# 데이터셋 전체 변환\n",
    "if train_df['keywords'].dtype == 'object':  # keywords가 문자열인지 확인\n",
    "    train_df['keywords'] = train_df['keywords'].apply(eval)  # 또는 json.loads\n",
    "\n",
    "# val_df의 'keywords' 컬럼 문자열 -> 리스트 변환\n",
    "if val_df['keywords'].dtype == 'object':  # keywords가 문자열인지 확인\n",
    "    val_df['keywords'] = val_df['keywords'].apply(eval)  # 또는 json.loads\n",
    "\n",
    "train_data_dict = train_df.to_dict(orient='list')\n",
    "train_dataset = Dataset.from_dict(train_data_dict)\n",
    "train_dataset\n",
    "print(len(train_dataset))\n",
    "\n",
    "val_data_dict = val_df.to_dict(orient='list')\n",
    "val_dataset = Dataset.from_dict(val_data_dict)\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['도심 개발 공사장', '불쾌한 냄새']\n",
      "도심 개발 공사장\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0]['keywords'])  \n",
    "print(train_dataset[0]['keywords'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '운동장 공사장에서 기름 냄새가 스산하게 발생하고 있습니다. 공사장 문제로 주민들이 민원을 계속 제기하고 있습니다',\n",
       " 'keywords': ['운동장 공사장', '기름 냄새', '스산하게']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train Dataset Size: 4324530\n",
      "Sampled Train Dataset Size: 4324\n",
      "Original Validation Dataset Size: 1853370\n",
      "Sampled Validation Dataset Size: 1853\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "def sample_dataset(dataset, fraction):\n",
    "    \"\"\"\n",
    "    데이터셋에서 지정된 비율만큼 샘플링\n",
    "    \"\"\"\n",
    "    sample_size = int(len(dataset) * fraction)\n",
    "    sampled_indices = random.sample(range(len(dataset)), sample_size)  # 랜덤 인덱스 선택\n",
    "    return dataset.select(sampled_indices)\n",
    "\n",
    "# Train과 Val 데이터셋 샘플링\n",
    "train_sampled_dataset = sample_dataset(train_dataset, 0.001)\n",
    "val_sampled_dataset = sample_dataset(val_dataset, 0.001)\n",
    "\n",
    "# 샘플링 후 데이터 크기 확인\n",
    "print(\"Original Train Dataset Size:\", len(train_dataset))\n",
    "print(\"Sampled Train Dataset Size:\", len(train_sampled_dataset))\n",
    "print(\"Original Validation Dataset Size:\", len(val_dataset))\n",
    "print(\"Sampled Validation Dataset Size:\", len(val_sampled_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['폐기물 소각 공장']\n"
     ]
    }
   ],
   "source": [
    "# 데이터 타입 확인\n",
    "print(type(val_sampled_dataset[0]['keywords']))  # <class 'str'>일 가능성이 큼\n",
    "\n",
    "# keywords 내용 확인\n",
    "print(val_sampled_dataset[0]['keywords'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 10:16:43.103757: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-27 10:16:43.297316: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-27 10:16:43.674948: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from typing import Dict\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AdamW,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "class CustomKeyBERTTrainer:\n",
    "    def __init__(self, model_name: str, **kwargs):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        if self.device == \"cuda\":\n",
    "            print(f\"GPU Model: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=kwargs.get(\"learning_rate\", 2e-5))\n",
    "        self.max_length = kwargs.get(\"max_length\", 128)\n",
    "        self.training_args = kwargs\n",
    "        self.save_dir = kwargs.get(\"save_dir\", \"./best_model\")\n",
    "        \n",
    "        self.best_model_path = os.path.join(self.save_dir, \"pytorch_model.bin\")\n",
    "        self.tokenizer_path = self.save_dir\n",
    "        \n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'epoch_times': [],\n",
    "            'best_epoch': 0\n",
    "        }\n",
    "\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "    def preprocess_data(self, examples: Dict) -> Dict:\n",
    "        # 입력 텍스트 처리\n",
    "        inputs = [f\"키워드 추출: {text}\" for text in examples[\"text\"]]\n",
    "        model_inputs = self.tokenizer(\n",
    "            inputs,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=None  # 텐서 변환을 DataCollator에 맡김\n",
    "        )\n",
    "\n",
    "        # 레이블(키워드) 처리\n",
    "        labels = [\", \".join(keywords) if keywords else \"\" for keywords in examples[\"keywords\"]]\n",
    "        \n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            tokenized_labels = self.tokenizer(\n",
    "                labels,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=None  # 텐서 변환을 DataCollator에 맡김\n",
    "            )\n",
    "\n",
    "        # -100으로 패딩 토큰을 마스킹\n",
    "        labels = tokenized_labels[\"input_ids\"]\n",
    "        for i in range(len(labels)):\n",
    "            for j in range(len(labels[i])):\n",
    "                if labels[i][j] == self.tokenizer.pad_token_id:\n",
    "                    labels[i][j] = -100\n",
    "\n",
    "        model_inputs[\"labels\"] = labels\n",
    "        return model_inputs\n",
    "        \n",
    "    def save_model_and_tokenizer(self, epoch=None, is_best=False):\n",
    "        \"\"\"\n",
    "        최고 성능 모델만 저장하고 이전 모델을 삭제\n",
    "        \"\"\"\n",
    "        if is_best:\n",
    "            # 이전 최고 모델 디렉토리 삭제\n",
    "            if os.path.exists(self.best_model_path):\n",
    "                print(f\"Deleting previous best model at {self.best_model_path}\")\n",
    "                os.system(f\"rm -rf {self.best_model_path}\")\n",
    "            \n",
    "            # 새로운 최고 모델 저장\n",
    "            save_path = os.path.join(self.save_dir, f\"best_model_epoch_{epoch}\")\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            self.model.save_pretrained(save_path)\n",
    "            self.tokenizer.save_pretrained(save_path)\n",
    "            torch.save(self.history, os.path.join(save_path, 'training_history.pt'))\n",
    "            print(f\"New best model saved at {save_path}\")\n",
    "\n",
    "            # 최고 모델 경로 업데이트\n",
    "            self.best_model_path = save_path\n",
    "\n",
    "    def calculate_metrics(self, predictions, labels):\n",
    "        predictions = torch.argmax(predictions, dim=-1)\n",
    "        correct = (predictions == labels).masked_fill(labels == -100, 0)\n",
    "        accuracy = correct.sum().item() / (labels != -100).sum().item()\n",
    "        return accuracy\n",
    "\n",
    "    def train(self, train_dataset, valid_dataset=None):\n",
    "        start_time = time.time()\n",
    "        print(f\"\\nStarting training at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"Training parameters:\")\n",
    "        print(f\"- Batch size: {self.training_args['batch_size']}\")\n",
    "        print(f\"- Learning rate: {self.training_args.get('learning_rate', '2e-5')}\")\n",
    "        print(f\"- Max length: {self.max_length}\")\n",
    "        print(f\"- Number of epochs: {self.training_args['num_epochs']}\")\n",
    "        print(f\"- Training samples: {len(train_dataset)}\")\n",
    "        if valid_dataset:\n",
    "            print(f\"- Validation samples: {len(valid_dataset)}\")\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "        # 데이터셋 전처리\n",
    "        print(\"Preprocessing training data...\")\n",
    "        train_dataset = train_dataset.map(\n",
    "            self.preprocess_data,\n",
    "            batched=True,\n",
    "            remove_columns=train_dataset.column_names,\n",
    "            desc=\"Processing training data\"\n",
    "        )\n",
    "\n",
    "        if valid_dataset is not None:\n",
    "            print(\"Preprocessing validation data...\")\n",
    "            valid_dataset = valid_dataset.map(\n",
    "                self.preprocess_data,\n",
    "                batched=True,\n",
    "                remove_columns=valid_dataset.column_names,\n",
    "                desc=\"Processing validation data\"\n",
    "            )\n",
    "\n",
    "        # DataCollator 설정\n",
    "        data_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer=self.tokenizer,\n",
    "            model=self.model,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # DataLoader 설정 (num_workers=0으로 변경하여 멀티프로세싱 관련 오류 방지)\n",
    "        train_dataloader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.training_args[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            collate_fn=data_collator,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        if valid_dataset is not None:\n",
    "            valid_dataloader = torch.utils.data.DataLoader(\n",
    "                valid_dataset,\n",
    "                batch_size=self.training_args[\"batch_size\"],\n",
    "                shuffle=False,\n",
    "                collate_fn=data_collator,\n",
    "                num_workers=0,\n",
    "                pin_memory=True\n",
    "            )\n",
    "        best_val_loss = float('inf')\n",
    "        early_stopping_counter = 0\n",
    "        early_stopping_patience = self.training_args.get('patience', 3)\n",
    "\n",
    "        for epoch in range(self.training_args[\"num_epochs\"]):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            self.model.train()\n",
    "            epoch_loss = 0\n",
    "            epoch_accuracy = 0\n",
    "            train_steps = 0\n",
    "            \n",
    "            progress_bar = tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}\")\n",
    "            batch_losses = []\n",
    "            batch_accuracies = []\n",
    "            \n",
    "            for batch_idx, batch in enumerate(progress_bar):\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels,\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss # 손실 계산\n",
    "                accuracy = self.calculate_metrics(outputs.logits, labels)\n",
    "                \n",
    "                loss.backward()# 손실 역전파 \n",
    "                self.optimizer.step() # 가중치 업데이트\n",
    "                self.optimizer.zero_grad() # 그래디언트 초기화\n",
    "\n",
    "                batch_losses.append(loss.item())\n",
    "                batch_accuracies.append(accuracy)\n",
    "                \n",
    "                current_loss = np.mean(batch_losses[-100:])\n",
    "                current_accuracy = np.mean(batch_accuracies[-100:])\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f'{current_loss:.4f}',\n",
    "                    'accuracy': f'{current_accuracy:.4f}',\n",
    "                    'batch': f'{batch_idx + 1}/{len(train_dataloader)}'\n",
    "                })\n",
    "\n",
    "            avg_train_loss = np.mean(batch_losses)\n",
    "            avg_train_accuracy = np.mean(batch_accuracies)\n",
    "\n",
    "            if valid_dataset is not None:\n",
    "                self.model.eval()\n",
    "                val_losses = []\n",
    "                val_accuracies = []\n",
    "\n",
    "                print(\"\\nRunning validation...\")\n",
    "                with torch.no_grad():\n",
    "                    for batch in tqdm(valid_dataloader, desc=\"Validating\"):\n",
    "                        input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                        attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                        labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "                        outputs = self.model(\n",
    "                            input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            labels=labels,\n",
    "                        )\n",
    "                        \n",
    "                        loss = outputs.loss\n",
    "                        accuracy = self.calculate_metrics(outputs.logits, labels)\n",
    "                        \n",
    "                        val_losses.append(loss.item())\n",
    "                        val_accuracies.append(accuracy)\n",
    "\n",
    "                avg_val_loss = np.mean(val_losses)\n",
    "                avg_val_accuracy = np.mean(val_accuracies)\n",
    "\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    early_stopping_counter = 0\n",
    "                    self.history['best_epoch'] = epoch + 1\n",
    "                    print(f\"\\nNew best validation loss: {best_val_loss:.4f}\")\n",
    "                    self.save_model_and_tokenizer(epoch + 1, is_best=True)  # 최고 모델만 저장\n",
    "                else:\n",
    "                    early_stopping_counter += 1\n",
    "\n",
    "\n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "            self.history['epoch_times'].append(epoch_time)\n",
    "            self.history['train_loss'].append(avg_train_loss)\n",
    "            if valid_dataset is not None:\n",
    "                self.history['val_loss'].append(avg_val_loss)\n",
    "\n",
    "            # Print epoch summary\n",
    "            print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "            print(f\"Time taken: {epoch_time:.2f} seconds\")\n",
    "            print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "            print(f\"Training accuracy: {avg_train_accuracy:.4f}\")\n",
    "            if valid_dataset is not None:\n",
    "                print(f\"Validation loss: {avg_val_loss:.4f}\")\n",
    "                print(f\"Validation accuracy: {avg_val_accuracy:.4f}\")\n",
    "                print(f\"Best validation loss so far: {best_val_loss:.4f}\")\n",
    "                print(f\"Early stopping counter: {early_stopping_counter}/{early_stopping_patience}\")\n",
    "\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print(\"\\nEarly stopping triggered.\")\n",
    "                break\n",
    "    def predict(self, text: str) -> str:\n",
    "        \"\"\"모델 추론\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            f\"키워드 추출: {self._normalize_text(text)}\",\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "        ).to(self.device)\n",
    "\n",
    "        outputs = self.model.generate(\n",
    "            inputs[\"input_ids\"], max_length=self.max_length, num_beams=5\n",
    "        )\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def _normalize_text(self, text: str) -> str:\n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Model: NVIDIA GeForce RTX 4080\n",
      "Available GPU memory: 15.59 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yjtech2/Desktop/yurim/anaconda3/envs/venv-llm/lib/python3.8/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training at: 2024-11-27 10:16:51\n",
      "Training parameters:\n",
      "- Batch size: 8\n",
      "- Learning rate: 0.0001\n",
      "- Max length: 128\n",
      "- Number of epochs: 10\n",
      "- Training samples: 4324\n",
      "- Validation samples: 1853\n",
      "\n",
      "==================================================\n",
      "\n",
      "Preprocessing training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "629fad9f546d454b903d7195a76f00b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing training data:   0%|          | 0/4324 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yjtech2/Desktop/yurim/anaconda3/envs/venv-llm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:4114: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing validation data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "355b5b497a4f469b822928041453d032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing validation data:   0%|          | 0/1853 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 541/541 [00:29<00:00, 18.29it/s, loss=0.0564, accuracy=0.9812, batch=541/541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 232/232 [00:03<00:00, 62.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best validation loss: 0.0290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yjtech2/Desktop/yurim/anaconda3/envs/venv-llm/lib/python3.8/site-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved at ./best_model/best_model_epoch_1\n",
      "\n",
      "Epoch 1 Summary:\n",
      "Time taken: 33.98 seconds\n",
      "Average training loss: 0.1317\n",
      "Training accuracy: 0.9630\n",
      "Validation loss: 0.0290\n",
      "Validation accuracy: 0.9901\n",
      "Best validation loss so far: 0.0290\n",
      "Early stopping counter: 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 541/541 [00:29<00:00, 18.32it/s, loss=0.0725, accuracy=0.9834, batch=541/541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 232/232 [00:03<00:00, 62.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best validation loss: 0.0229\n",
      "Deleting previous best model at ./best_model/best_model_epoch_1\n",
      "New best model saved at ./best_model/best_model_epoch_2\n",
      "\n",
      "Epoch 2 Summary:\n",
      "Time taken: 33.83 seconds\n",
      "Average training loss: 0.0404\n",
      "Training accuracy: 0.9883\n",
      "Validation loss: 0.0229\n",
      "Validation accuracy: 0.9943\n",
      "Best validation loss so far: 0.0229\n",
      "Early stopping counter: 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 541/541 [00:29<00:00, 18.28it/s, loss=0.0214, accuracy=0.9947, batch=541/541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 232/232 [00:03<00:00, 62.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best validation loss: 0.0052\n",
      "Deleting previous best model at ./best_model/best_model_epoch_2\n",
      "New best model saved at ./best_model/best_model_epoch_3\n",
      "\n",
      "Epoch 3 Summary:\n",
      "Time taken: 33.86 seconds\n",
      "Average training loss: 0.0207\n",
      "Training accuracy: 0.9946\n",
      "Validation loss: 0.0052\n",
      "Validation accuracy: 0.9984\n",
      "Best validation loss so far: 0.0052\n",
      "Early stopping counter: 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 541/541 [00:29<00:00, 18.26it/s, loss=0.0361, accuracy=0.9925, batch=541/541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 232/232 [00:03<00:00, 62.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      "Time taken: 33.34 seconds\n",
      "Average training loss: 0.0216\n",
      "Training accuracy: 0.9955\n",
      "Validation loss: 0.0062\n",
      "Validation accuracy: 0.9990\n",
      "Best validation loss so far: 0.0052\n",
      "Early stopping counter: 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 541/541 [00:29<00:00, 18.25it/s, loss=0.0499, accuracy=0.9899, batch=541/541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 232/232 [00:03<00:00, 62.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best validation loss: 0.0025\n",
      "Deleting previous best model at ./best_model/best_model_epoch_3\n",
      "New best model saved at ./best_model/best_model_epoch_5\n",
      "\n",
      "Epoch 5 Summary:\n",
      "Time taken: 33.89 seconds\n",
      "Average training loss: 0.0349\n",
      "Training accuracy: 0.9926\n",
      "Validation loss: 0.0025\n",
      "Validation accuracy: 0.9993\n",
      "Best validation loss so far: 0.0025\n",
      "Early stopping counter: 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 541/541 [00:29<00:00, 18.29it/s, loss=0.0066, accuracy=0.9987, batch=541/541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 232/232 [00:03<00:00, 62.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best validation loss: 0.0003\n",
      "Deleting previous best model at ./best_model/best_model_epoch_5\n",
      "New best model saved at ./best_model/best_model_epoch_6\n",
      "\n",
      "Epoch 6 Summary:\n",
      "Time taken: 33.86 seconds\n",
      "Average training loss: 0.0142\n",
      "Training accuracy: 0.9973\n",
      "Validation loss: 0.0003\n",
      "Validation accuracy: 0.9999\n",
      "Best validation loss so far: 0.0003\n",
      "Early stopping counter: 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 541/541 [00:29<00:00, 18.26it/s, loss=0.0189, accuracy=0.9963, batch=541/541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 232/232 [00:03<00:00, 62.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Summary:\n",
      "Time taken: 33.34 seconds\n",
      "Average training loss: 0.0135\n",
      "Training accuracy: 0.9973\n",
      "Validation loss: 0.0042\n",
      "Validation accuracy: 0.9993\n",
      "Best validation loss so far: 0.0003\n",
      "Early stopping counter: 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 541/541 [00:29<00:00, 18.25it/s, loss=0.0132, accuracy=0.9974, batch=541/541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 232/232 [00:03<00:00, 62.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Summary:\n",
      "Time taken: 33.37 seconds\n",
      "Average training loss: 0.0144\n",
      "Training accuracy: 0.9971\n",
      "Validation loss: 0.0038\n",
      "Validation accuracy: 0.9994\n",
      "Best validation loss so far: 0.0003\n",
      "Early stopping counter: 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 541/541 [00:29<00:00, 18.26it/s, loss=0.0086, accuracy=0.9987, batch=541/541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 232/232 [00:03<00:00, 62.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Summary:\n",
      "Time taken: 33.34 seconds\n",
      "Average training loss: 0.0195\n",
      "Training accuracy: 0.9958\n",
      "Validation loss: 0.0017\n",
      "Validation accuracy: 0.9996\n",
      "Best validation loss so far: 0.0003\n",
      "Early stopping counter: 3/3\n",
      "\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # 멀티프로세싱 경고 방지\n",
    "\n",
    "trainer = CustomKeyBERTTrainer(\n",
    "    model_name=\"facebook/bart-base\", # t5-base     skt/kobart-base-v2\n",
    "    max_length=128,\n",
    "    learning_rate=1e-4,\n",
    "    batch_size=8,\n",
    "    num_epochs=10,\n",
    "    gradient_accumulation_steps=8,\n",
    "    patience=3 # 몇 에폭마다 체크포인트 저장할지\n",
    ")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()  # GPU 메모리 초기화\n",
    "    trainer.train(train_sampled_dataset, val_sampled_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('/home/yjtech2/Desktop/yurim/LLM/Pre_processing/smell_keyword/best_model/best_model_epoch_4')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/yjtech2/Desktop/yurim/LLM/Pre_processing/smell_keyword/best_model/best_model_epoch_4\")\n",
    "print(\"Model and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원래 문장:  신호등 근처에서 음식물 쓰레기 냄새가 진동합니다.\n",
      "키워드 추출:  신호등 근처, 음식물 쓰레기 냄새, 진동합니다\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# 저장된 모델 경로\n",
    "saved_model_path = \"/home/yjtech2/Desktop/yurim/LLM/Pre_processing/smell_keyword/best_model/best_model_epoch_6\"\n",
    "\n",
    "# 모델과 토크나이저 불러오기\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_model_path, local_files_only=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(saved_model_path, local_files_only=True).to('cpu')\n",
    "\n",
    "# 예측 테스트\n",
    "text = '신호등 근처에서 음식물 쓰레기 냄새가 진동합니다.'\n",
    "inputs = tokenizer(\n",
    "    f\"키워드 추출: {text}\",\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=128,\n",
    "    truncation=True\n",
    ").to(\"cpu\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=128,\n",
    "    num_beams=5,\n",
    "    length_penalty=0.7,\n",
    "    repetition_penalty=1.2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"원래 문장: \", text)\n",
    "print(\"키워드 추출: \", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '폐기물 소각 공장에서 찌르는 냄새가 매캐하게 발생하고 있습니다. 이는 장기적인 건강 문제로 이어질 가능성이 있습니다',\n",
       " 'keywords': ['폐기물 소각 공장']}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_sampled_dataset[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
