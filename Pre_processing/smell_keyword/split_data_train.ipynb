{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 카테고리 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_time_and_date_combinations():\n",
    "    \"\"\"\n",
    "    시간과 날짜의 모든 가능한 조합 생성, 시간 없는 날짜 형식 포함\n",
    "    \"\"\"\n",
    "    # 가능한 모든 시간\n",
    "    time_of_day = [\n",
    "        \"아침\", \"오전\", \"낮\", \"오후\", \"저녁\", \"밤\", \"새벽\",\n",
    "        \"정오\", \"자정\"\n",
    "    ]\n",
    "    specific_times = [\n",
    "        f\"오전 {hour}시부터 {hour+1}시\" for hour in range(6, 12)\n",
    "    ] + [\n",
    "        f\"오후 {hour}시부터 {hour+1}시\" for hour in range(1, 12)\n",
    "    ] + [\n",
    "        f\"새벽 {hour}시\" for hour in range(1, 6)\n",
    "    ] + [\n",
    "        f\"저녁 {hour}시\" for hour in range(6, 9)\n",
    "    ] + [\n",
    "        f\"밤 {hour}시\" for hour in range(9, 12)\n",
    "    ]\n",
    "\n",
    "    # 가능한 모든 날짜\n",
    "    relative_dates = [\n",
    "        \"오늘\", \"어제\", \"그제\", \"모레\", \"글피\",\n",
    "        \"지난주\", \"이번주\", \"다음주\",\n",
    "        \"지난달\", \"이번달\", \"다음달\",\n",
    "        \"작년\", \"올해\", \"내년\"\n",
    "    ]\n",
    "\n",
    "    specific_dates = [\n",
    "        f\"{month}월 {day}일\" for month in range(1, 13) for day in range(1, 29)\n",
    "    ]\n",
    "\n",
    "    # 0월 0일 추가\n",
    "    specific_dates.append(\"0월 0일\")\n",
    "\n",
    "    relative_with_numbers = [\n",
    "        f\"{days}일 전\" for days in range(1, 31)\n",
    "    ] + [\n",
    "        f\"{days}일 후\" for days in range(1, 31)\n",
    "    ] + [\n",
    "        f\"{months}개월 전\" for months in range(1, 13)\n",
    "    ] + [\n",
    "        f\"{months}개월 후\" for months in range(1, 13)\n",
    "    ] + [\n",
    "        f\"{years}년 전\" for years in range(1, 6)\n",
    "    ] + [\n",
    "        f\"{years}년 후\" for years in range(1, 6)\n",
    "    ]\n",
    "\n",
    "    all_dates = relative_dates + specific_dates + relative_with_numbers\n",
    "    all_times = time_of_day + specific_times\n",
    "\n",
    "    # 날짜만 있는 형식 추가\n",
    "    date_only_combinations = all_dates  # 시간 없는 조합은 날짜 자체만\n",
    "\n",
    "    # 날짜 + 시간 조합 생성\n",
    "    date_time_combinations = [f\"{date} {time}\" for date in all_dates for time in all_times]\n",
    "\n",
    "    # 최종 조합: 날짜만 있는 형식 + 날짜 + 시간\n",
    "    all_combinations = date_only_combinations + date_time_combinations\n",
    "    return all_combinations\n",
    "\n",
    "\n",
    "\n",
    "def add_all_combinations_to_categories(categories):\n",
    "    \"\"\"\n",
    "    모든 카테고리에 시간과 날짜의 모든 조합 추가\n",
    "    \"\"\"\n",
    "    combinations = generate_all_time_and_date_combinations()\n",
    "    for category, data in categories.items():\n",
    "        # time_of_days 필드가 있는 경우에만 처리\n",
    "        if \"time_of_days\" in data:\n",
    "            data[\"time_of_days\"].extend(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "    \"쓰레기 관련\": {\n",
    "        \"locations\": [\n",
    "            # 기존 항목\n",
    "            \"쓰레기 매립장\", \"쓰레기 처리장\", \"재활용 센터\", \"음식물 쓰레기장\", \"불법 폐기물 처리장\",\n",
    "            \"폐기물 소각장\", \"주변 골목길\", \"공동 주택 쓰레기장\", \"아파트 단지 쓰레기장\", \"학교 근처 쓰레기장\",\n",
    "            \"병원 쓰레기장\", \"공원 주변 쓰레기장\", \"도로변 쓰레기장\", \"지하철 역 주변\", \"대형마트 쓰레기장\",\n",
    "            \"버스 정류장 근처\", \"도시 외곽 쓰레기장\", \"항만 주변\", \"농촌 지역 쓰레기장\", \"도시 중심가 쓰레기장\",\n",
    "            \"포항시 북구\", \"포항시 남구\", \"포스코\", \"포항항\", \"포항대학교\",\n",
    "            \"포항 철강 산업단지\", \"포항 주변 해안\", \"포항시내\", \"포항 공원\", \"포항 해상\",\n",
    "            \"포항역\", \"포항 근처 산\", \"포항 해수욕장\", \"포항시 외곽\", \"포항 근처 도로\",\n",
    "            \"포항 남부 해안도로\", \"포항 북부 해안도로\", \"포항 근교 농촌\", \"포항 일대 농업지역\", \"포항 주요 광장\",\n",
    "\n",
    "            # 추가된 항목\n",
    "            \"도심 주택가\", \"산업단지 내 쓰레기장\", \"지역 전통시장 주변\", \"유원지 인근 쓰레기장\",\n",
    "            \"쇼핑몰 주차장\", \"고속도로 휴게소\", \"도시 하수구 인근\", \"관광 명소 주변 쓰레기장\",\n",
    "            \"공공 화장실 주변\", \"소형 공장 단지 인근\", \"고속버스 터미널 근처\", \"주거지 외곽 지역\",\n",
    "            \"도심 야시장 주변\", \"레스토랑 및 카페 거리\", \"물류 창고 단지\", \"해변가 주차장\",\n",
    "            \"국립 공원 근처\", \"고가도로 하부\", \"공동묘지 주변\", \"농촌 마을 회관 근처\",\n",
    "            \"항구 내 부두\", \"지하 주차장\", \"폐쇄된 공장 지역\", \"농수산물 시장\", \"고속도로 톨게이트 주변\",\n",
    "            \"공공 운동장\", \"도심 광장\", \"폐쇄된 학교 부지\", \"공동 거주 시설\", \"철도 건널목 근처\",\n",
    "            \"공장 배출구 인근\", \"전통 문화 유적지 주변\", \"농업 생산 기지\", \"바닷가 모래사장\",\n",
    "            \"스포츠 경기장 주변\", \"대형 쇼핑센터\", \"고등학교 쓰레기장\", \"주말 농장 쓰레기장\",\n",
    "            \"공장 근처 공터\", \"도시 중심가 골목\", \"강변 주택가\", \"수영장 주변\", \"낚시터 인근\",\n",
    "            \"항구 배후 단지\", \"대규모 물류 단지\", \"폐쇄된 쓰레기장\", \"공원 산책로 인근\",\n",
    "            \"도시 경계선 공터\", \"주요 사거리 근처\", \"산악 트레킹 코스 입구\", \"지방 도로변\",\n",
    "            \"소규모 공업 단지\", \"주말 장터 근처\", \"주민 운동 시설 인근\", \"해양 레저 단지\",\n",
    "            \"새로 조성된 산업 단지\", \"폐쇄된 고속도로 구역\", \"산림 보호구역 입구\"\n",
    "        ],\n",
    "\n",
    "\n",
    "        \"smell_types\": [\n",
    "            # 기존 항목\n",
    "            \"악취\", \"화학 냄새\", \"부패한 냄새\", \"쓰레기 냄새\", \"찌든 냄새\", \"타는 냄새\", \"고약한 냄새\",\n",
    "            \"불쾌한 냄새\", \"탄 냄새\", \"침투성 냄새\", \"메스꺼운 냄새\", \"가스 냄새\", \"산성 냄새\", \"날카로운 냄새\",\n",
    "            \"오염된 냄새\", \"지속적으로 나는 냄새\", \"폭발적 악취\", \"썩은 물질 냄새\", \"퇴비 냄새\", \"더러운 냄새\",\n",
    "\n",
    "            # 추가된 항목\n",
    "            \"쓰레기 더미 냄새\", \"섞인 음식물 냄새\", \"부패된 해산물 냄새\", \"냄새가 심한 재활용 쓰레기 냄새\",\n",
    "            \"소각장 배출물 냄새\", \"낡은 가구 냄새\", \"물비린내\", \"담배 꽁초 냄새\", \"플라스틱 태운 냄새\",\n",
    "            \"부패된 고기 냄새\", \"강한 페인트 잔여물 냄새\", \"오래된 음료수 잔여물 냄새\", \"젖은 쓰레기 냄새\",\n",
    "            \"찌든 유기물 냄새\", \"과일 썩은 냄새\", \"녹조로 인한 썩은 냄새\", \"압축된 쓰레기 냄새\", \"쓰레기장에서 나는 악취\",\n",
    "            \"비닐봉지에서 나는 냄새\", \"오래된 종이 냄새\", \"불에 탄 쓰레기 냄새\", \"플라스틱 부서지는 냄새\",\n",
    "            \"찌든 유리병 냄새\", \"산업 폐기물 냄새\", \"공공장소 쓰레기 냄새\", \"부패된 유기물 냄새\", \"잿더미 냄새\"\n",
    "        ],\n",
    "\n",
    "        \"intensity_adjectives\": [\n",
    "            # 기존 항목\n",
    "            \"쾌쾌하게\", \"심하게\", \"지속적으로\", \"불쾌하게\", \"찌들게\", \"강하게\", \"독하게\", \"자극적으로\",\n",
    "            \"비린내처럼\", \"지독하게\", \"매캐하게\", \"묵직하게\", \"축축하게\", \"역하게\", \"찌르는 듯이\", \"강렬하게\",\n",
    "            \"폭발적으로\", \"극도로\", \"심각하게\", \"거슬리게\",\n",
    "\n",
    "            # 수정된 항목\n",
    "            \"독하게\", \"타는 듯한\", \"병적으로\",\n",
    "\n",
    "            # 추가된 항목\n",
    "            \"금방 사라지지 않는\", \"강렬하게 스며드는\", \"엄청나게 탁한\", \"비누로도 씻기 어려운\", \"차가운 냄새로 확산되는\",\n",
    "            \"냄새가 계속해서 강하게 남는\", \"살아있는 듯한\", \"산처럼 거세지는\", \"강한 압력에 의해 발생하는\", \"무겁게 느껴지는\",\n",
    "            \"심하게 퍼지는\", \"끝없이 쌓이는\", \"단숨에 퍼지는\", \"끈적하게 남는\", \"거대한 폭발처럼\", \"상처를 자극하는\",\n",
    "            \"내장을 자극하는\", \"심장을 쥐어짜는\", \"돌이킬 수 없는\", \"강제로 밀려오는\", \"강제로 끌어당기는\", \"쏟아져 나오는\",\n",
    "            \"숨을 쉬기 어려운\", \"심장을 멎게 하는\", \"강제적으로 압도하는\", \"지속적으로 억누르는\", \"계속해서 감도는\",\n",
    "            \"끝없이 강해지는\", \"피할 수 없는\", \"계속해서 강하게 압박하는\", \"바람처럼 휘몰아치는\", \"산처럼 무겁고 강한\"\n",
    "        ],\n",
    "\n",
    "\n",
    "        \"issues\": [\n",
    "            \"쓰레기 처리 미비\", \"불법 투기된 폐기물\", \"재활용 분리수거 미흡\", \"음식물 쓰레기 악취\", \"폐기물 소각으로 인한 매연\",\n",
    "            \"쓰레기 적체\", \"쓰레기차 통행 불편\", \"폐기물 배출 지연\", \"유해 가스 발생\", \"불법 폐기물 매립\",\n",
    "            \"쓰레기통 부족\", \"도로에 쓰레기 방치\", \"재활용 품목 혼합\", \"쓰레기장 인근 불쾌한 냄새\", \"쓰레기 처리 불완전\",\n",
    "            \"도로에 쓰레기 던져짐\", \"소각장 연기 발생\", \"고장난 재활용 기계\", \"쓰레기통 불법 방치\", \"분리배출 혼잡\"\n",
    "        ],\n",
    "\n",
    "        \"departments\": [\n",
    "            \"환경 관리팀\", \"청소 관리팀\", \"재활용 관리팀\", \"폐기물 처리팀\", \"소각장 운영팀\", \"대기 관리팀\",\n",
    "            \"도시 재활용팀\", \"환경 안전팀\", \"교통 관리팀\", \"환경 연구팀\", \"자원 회수팀\", \"공공 서비스팀\",\n",
    "            \"청소 공공팀\", \"환경 감시팀\", \"기후 변화 대응팀\", \"시설 운영팀\", \"도시 환경 관리팀\", \"폐기물 분석팀\",\n",
    "            \"오염 물질 처리팀\", \"재활용 기술팀\"\n",
    "        ],\n",
    "\n",
    "        \"effects\": [\n",
    "            \"생활 불편\", \"호흡기 문제\", \"냄새로 인한 불쾌감\", \"알레르기 증상\", \"건강에 해로움\", \"환경 오염\",\n",
    "            \"시민 불만\", \"민원 발생\", \"주변 환경 개선 필요\", \"지역 이미지 훼손\", \"공공 안전 문제\", \"도시 미관 저하\",\n",
    "            \"공기 질 악화\", \"교통 혼잡\", \"관광지 방문 감소\", \"쾌적한 환경 저하\", \"수면 방해\", \"민원 처리 지연\",\n",
    "            \"공공서비스 감소\", \"경제적 손실\"\n",
    "        ],\n",
    "        \"time_of_days\": [\n",
    "        ]\n",
    "     },\n",
    "\n",
    "\n",
    "\n",
    "    \"날씨 관련\": {\n",
    "        \"locations\": [\n",
    "            # 기존 항목\n",
    "            \"포항시 북구\", \"포항시 남구\", \"포스코\", \"포항항\", \"포항대학교\",\n",
    "            \"포항 철강 산업단지\", \"포항 주변 해안\", \"포항시내\", \"포항 공원\", \"포항 해상\",\n",
    "            \"포항역\", \"포항 근처 산\", \"포항 해수욕장\", \"포항시 외곽\", \"포항 근처 도로\",\n",
    "            \"포항 남부 해안도로\", \"포항 북부 해안도로\", \"포항 근교 농촌\", \"포항 일대 농업지역\", \"포항 주요 광장\",\n",
    "\n",
    "            # 추가된 항목\n",
    "            \"국립 경주공원\", \"포항 북항\", \"죽도시장 인근\", \"흥해읍\", \"기계면\",\n",
    "            \"포항시 공단 지역\", \"연일읍\", \"장기면\", \"구룡포 해수욕장\", \"호미곶 광장\",\n",
    "            \"송도해안도로\", \"영일대 해수욕장\", \"포항 송도 해변\", \"해안 절벽 지역\", \"남구 해양공원\",\n",
    "            \"청림동 해안도로\", \"양덕동 공단 지역\", \"구룡포항\", \"호미곶 일대\", \"장량동 주택가\",\n",
    "            \"도심 상업지구\", \"송라면 농촌 지역\", \"포항 해양 레저 단지\", \"동해안 국립공원\",\n",
    "            \"장성동 공원\", \"포항 북구 농업 지역\", \"포항 남구 수산시장\", \"도구해변\", \"청하면 근교\",\n",
    "            \"내연산 자연휴양림\", \"기계천 인근\", \"형산강 하구\", \"포항 종합운동장\", \"죽도시장 부근\",\n",
    "            \"포항 문화예술회관\", \"포항공대 캠퍼스\", \"포항항 제1부두\", \"포항항 제2부두\", \"영일만 공단\",\n",
    "            \"영일만 신항\", \"한동대 캠퍼스\", \"양덕동 산업단지\", \"동빈항\", \"오천읍 항만지역\",\n",
    "            \"포항 해양문화관\", \"석병산 등산로\", \"청림천 하천변\", \"포항 온천지구\", \"구룡포 대게거리\",\n",
    "            \"포항 시립미술관 인근\", \"환호공원\", \"포항 해양경찰서 주변\", \"포항 여객터미널\",\n",
    "            \"포항시청 인근\", \"포항 남부시장\", \"포항 북부시장\", \"포항 영일대 광장\", \"포항 시민공원\",\n",
    "            \"초곡리 근교\", \"도구천 하천변\", \"호미곶등대\", \"호미곶 바람의 언덕\", \"양덕 초등학교 근처\",\n",
    "            \"포항제철고등학교 부근\", \"남구 신흥 주택지\", \"죽도초등학교 인근\", \"청림고등학교 근교\",\n",
    "            \"포항 도시 외곽 산책로\", \"포항 남구 탄광촌 유적지\", \"구룡포 전통 어시장\"\n",
    "        ],\n",
    "\n",
    "        \"smell_types\": [\n",
    "            # 기존 항목\n",
    "            \"산성 냄새\", \"습한 냄새\", \"상쾌한 냄새\", \"짙은 미세먼지\", \"소나기 후 냄새\",\n",
    "            \"염분이 섞인 냄새\", \"포근한 바람 냄새\", \"비 오는 날 냄새\", \"진한 해풍 냄새\", \"우박 냄새\",\n",
    "            \"장마철 냄새\", \"선선한 냄새\", \"습기 섞인 냄새\", \"더운 날씨의 냄새\", \"뿌연 공기\",\n",
    "            \"자연의 냄새\", \"먼지 냄새\", \"미세먼지 냄새\", \"구름 냄새\", \"비 온 후 냄새\",\n",
    "\n",
    "            # 추가된 항목\n",
    "            \"차가운 이슬 냄새\", \"바람에 실린 꽃 향기\", \"축축한 낙엽 냄새\", \"산불 연기 냄새\", \"건조한 먼지 냄새\",\n",
    "            \"햇볕에 말린 흙 냄새\", \"얼음이 녹은 냄새\", \"산골짜기 물 냄새\", \"폭염 후의 공기 냄새\", \"폭우 후의 땅 냄새\",\n",
    "            \"겨울 아침의 냄새\", \"해돋이 전 공기 냄새\", \"밤 공기 냄새\", \"맑은 날 공기 냄새\", \"서리 낀 풀 냄새\",\n",
    "            \"황사 냄새\", \"강풍에 섞인 바다 냄새\", \"맑고 차가운 산 공기 냄새\", \"안개 속의 축축한 냄새\", \"비 내리는 도시 냄새\",\n",
    "\n",
    "            # 추가된 세부 항목\n",
    "            \"폭풍우 전의 정적 냄새\", \"햇빛에 바래진 나무 냄새\", \"강가의 물냄새\", \"비 내린 후 물이 고인 곳의 냄새\",\n",
    "            \"맑은 시냇물 냄새\", \"호수의 고요한 냄새\", \"물속의 식물 냄새\", \"짙은 수몰 냄새\", \"담수의 시원한 냄새\",\n",
    "            \"물에 섞인 흙 냄새\", \"강변의 젖은 흙 냄새\", \"바닷물의 염분 냄새\", \"여름 폭우 후 냄새 나는 호수\"\n",
    "        ],\n",
    "        \n",
    "        \"intensity_adjectives\": [\n",
    "            # 기존 항목\n",
    "            \"강하게\", \"약하게\", \"지속적으로\", \"급격하게\", \"급히\", \"덥게\", \"차갑게\", \"어두운\",\n",
    "            \"차츰\", \"가끔\", \"심하게\", \"낮게\", \"매우\", \"불규칙하게\", \"짧게\", \"긴 시간 동안\", \"폭발적으로\",\n",
    "            \"차분하게\", \"점차적으로\", \"습기 찬\",\n",
    "\n",
    "            # 추가된 항목\n",
    "            \"부드럽게 스며드는\", \"가볍게 퍼지는\", \"날카롭게 퍼지는\", \"천천히 퍼지는\", \"촉촉하게 남아 있는\", \n",
    "            \"빠르게 스쳐 지나가는\", \"서서히 농도가 짙어지는\", \"차가운 공기에 섞여 드는\", \"한꺼번에 확 퍼지는\", \n",
    "            \"거세게 휘몰아치는\", \"맑고 강하게\", \"온화하게 지속되는\", \"비에 씻긴 듯한\", \"눈에 띄게 확산되는\", \n",
    "            \"따뜻하게 느껴지는\", \"끈적하게 남아 있는\", \"냉랭하게 퍼지는\", \"흐릿하게 스며드는\", \"반복적으로 강해지는\", \n",
    "            \"뚜렷하고 선명하게\", \"무겁게 감도는\", \"냄새가 계속 쌓이는\", \"침투하듯 퍼지는\", \"어두워지며 강해지는\", \n",
    "            \"거친 바람처럼 확산되는\", \"바람에 실려 퍼지는\", \"미세하게 퍼지는\", \"누그러지지 않는\", \"촉각적으로 다가오는\", \n",
    "            \"지속적으로 눅눅한\", \"상처처럼 따가운\", \"강한 열기로 퍼지는\", \"습도와 함께 증폭되는\", \"무겁게 압도하는\", \n",
    "            \"천천히 퍼져나가는\", \"산들바람처럼 스며드는\", \"기온 상승에 따라 강해지는\", \"지속적으로 누적되는\", \"악취가 휘몰아치는\"\n",
    "        ],\n",
    "\n",
    "\n",
    "        \"issues\": [\n",
    "            \"갑작스러운 비\", \"폭염\", \"대설\", \"장마\", \"강풍\", \"미세먼지\", \"안개\", \"강한 비바람\",\n",
    "            \"황사\", \"온도 급변\", \"건조한 날씨\", \"태풍\", \"기온 차이\", \"눈보라\", \"뇌우\", \"열대야\",\n",
    "            \"저기압\", \"고기압\", \"풍속 증가\", \"불안정한 날씨\"\n",
    "        ],\n",
    "\n",
    "\n",
    "        \"departments\": [\n",
    "            \"기상 예보팀\", \"기후 연구팀\", \"미세먼지 대책팀\", \"기상 안전팀\", \"기후 변화 대응팀\",\n",
    "            \"대기 관리팀\", \"기상 기술팀\", \"폭염 대책팀\", \"기상 정보 제공팀\", \"강수량 관리팀\",\n",
    "            \"기후 모델링 팀\", \"태풍 연구팀\", \"날씨 분석팀\", \"기상 측정팀\", \"온도 측정팀\", \"기상 해석팀\",\n",
    "            \"대기 오염 관리팀\", \"기상 데이터 분석팀\", \"산악 기상팀\", \"기후 변화 예측팀\"\n",
    "        ],\n",
    "\n",
    "        \"effects\": [\n",
    "            \"교통 혼잡\", \"건강 문제\", \"미세먼지 영향\", \"농작물 피해\", \"산불 위험\", \"대기 오염 증가\",\n",
    "            \"집중력 저하\", \"눈길 사고\", \"체온 조절 어려움\", \"습기 문제\", \"기상 난민 발생\", \"수확량 감소\",\n",
    "            \"기후 변화로 인한 생활 불편\", \"폭염으로 인한 탈진\", \"강풍으로 인한 시설 피해\", \"눈사태 발생\",\n",
    "            \"시민 불편\", \"야외 활동 제한\", \"소방 활동 어려움\", \"기상 예보 오류\"\n",
    "        ],\n",
    "        \"time_of_days\": [\n",
    "        ]\n",
    "     },\n",
    "\n",
    "\n",
    "    \"공장 관련\": {\n",
    "        \"locations\": [\n",
    "            # 기존 항목\n",
    "            \"화학 공장\", \"플라스틱 제조 공장\", \"폐수 처리 공장\", \"섬유 공장\", \"금속 가공 공장\",\n",
    "            \"자동차 부품 제조 공장\", \"전자 부품 공장\", \"석유 정제 공장\", \"시멘트 공장\", \"비료 공장\",\n",
    "            \"고무 공장\", \"제지 공장\", \"가구 제조 공장\", \"유리 공장\", \"주조 공장\",\n",
    "            \"플라스틱 가공 공장\", \"코팅 공장\", \"제약 공장\", \"화장품 제조 공장\", \"고체 연료 공장\",\n",
    "            \"전자 폐기물 처리 공장\", \"아스팔트 공장\", \"타이어 제조 공장\", \"도료 제조 공장\", \"농약 공장\",\n",
    "            \"음료 제조 공장\", \"고속 인쇄 공장\", \"전력 생산 공장\", \"스틸 가공 공장\", \"세제 제조 공장\",\n",
    "            \"복합 소재 제조 공장\", \"케이블 제조 공장\", \"냉각 기기 제조 공장\", \"반도체 공장\", \"방직 공장\",\n",
    "            \"폐기물 소각 공장\", \"세라믹 제조 공장\", \"폴리머 공장\", \"석탄 연료 공장\", \"합성수지 공장\",\n",
    "\n",
    "            # 추가된 항목\n",
    "            \"제철소\", \"금속 재활용 공장\", \"폐기물 재활용 공장\", \"전기 자동차 배터리 제조 공장\", \"항공기 부품 제조 공장\",\n",
    "            \"항공우주 기술 공장\", \"고압 가스 제조 공장\", \"태양광 패널 제조 공장\", \"풍력 터빈 부품 공장\", \"친환경 소재 제조 공장\",\n",
    "            \"식품 가공 공장\", \"유제품 공장\", \"육류 가공 공장\", \"초콜릿 제조 공장\", \"곡물 가공 공장\",\n",
    "            \"알루미늄 압출 공장\", \"유기 화학물 제조 공장\", \"화장품 포장 공장\", \"의약품 포장 공장\", \"산업용 로봇 제조 공장\",\n",
    "            \"LED 조명 제조 공장\", \"반도체 장비 공장\", \"3D 프린터 제조 공장\", \"유리 섬유 공장\", \"방수 소재 제조 공장\",\n",
    "            \"전기 절연체 제조 공장\", \"비료 혼합 공장\", \"합성 고무 제조 공장\", \"산업용 기계 제조 공장\", \"재생 플라스틱 공장\",\n",
    "            \"탄소 섬유 제조 공장\", \"고강도 강철 제조 공장\", \"폐배터리 재활용 공장\", \"드론 제조 공장\", \"의료 장비 제조 공장\",\n",
    "            \"스포츠 용품 공장\", \"음향 장비 제조 공장\", \"수처리 설비 제조 공장\", \"가정용 전자기기 공장\", \"산업용 배터리 공장\",\n",
    "            \"해양 플랜트 공장\", \"터빈 블레이드 공장\", \"엔진 부품 가공 공장\", \"에어컨 부품 공장\", \"방산 장비 제조 공장\",\n",
    "            \"비행기 조립 공장\", \"농기계 제조 공장\", \"철도 차량 제조 공장\", \"우주선 부품 제조 공장\", \"건축 자재 제조 공장\",\n",
    "            \"전기 케이블 공장\", \"도시 가스 설비 공장\", \"모듈형 주택 공장\", \"대형 선박 제조 공장\", \"소형 모터 제조 공장\",\n",
    "            \"의류 제조 공장\", \"천연 섬유 가공 공장\", \"에너지 저장 시스템 공장\", \"수소 에너지 공장\", \"리튬 이온 배터리 공장\",\n",
    "            \"태양열 발전 설비 공장\", \"소형 가전 제조 공장\", \"청소 장비 제조 공장\", \"전동 공구 공장\", \"포장재 제조 공장\",\n",
    "            \"유기농 식품 공장\", \"스마트 가전 공장\", \"초저온 냉각 장치 제조 공장\", \"지능형 로봇 제조 공장\", \"물류 자동화 시스템 공장\"\n",
    "        ],\n",
    "        \"smell_types\": [\n",
    "            # 기존 항목\n",
    "            \"화학약품 냄새\", \"타는 냄새\", \"매운 냄새\", \"오염된 물 냄새\", \"기름 냄새\",\n",
    "            \"산성 냄새\", \"날카로운 냄새\", \"지속적인 냄새\", \"부식 냄새\", \"찌든 냄새\",\n",
    "            \"유기물 냄새\", \"불쾌한 냄새\", \"탄 냄새\", \"매캐한 냄새\", \"고약한 냄새\",\n",
    "            \"찌르는 냄새\", \"폭발적인 악취\", \"가스 냄새\", \"비린내\", \"혼합된 화학 냄새\",\n",
    "            \"플라스틱 녹는 냄새\", \"고무 타는 냄새\", \"독성 화학물 냄새\", \"미세 먼지 냄새\", \"금속성 냄새\",\n",
    "            \"염료 냄새\", \"공기 중의 찌든 냄새\", \"방부제 냄새\", \"부패 냄새\", \"미묘한 가스 냄새\",\n",
    "            \"강한 접착제 냄새\", \"분말 화학 냄새\", \"오존 냄새\", \"소각된 폐기물 냄새\", \"연소 냄새\",\n",
    "\n",
    "            # 추가된 항목\n",
    "            \"고온 증기 냄새\", \"화학 반응 중 발생한 냄새\", \"중화제 냄새\", \"아크 용접 냄새\", \"고체 연료 연소 냄새\",\n",
    "            \"부식성 기체 냄새\", \"분말 금속 냄새\", \"합성 가스 냄새\", \"유황 냄새\", \"질소 화합물 냄새\",\n",
    "            \"고압 공정 냄새\", \"파라핀 냄새\", \"산화된 금속 냄새\", \"폐수 증발 냄새\", \"강렬한 탄화 냄새\",\n",
    "            \"오염된 공기 냄새\", \"연마재 냄새\", \"고온 금속 냄새\", \"폴리머 처리 냄새\", \"공업용 에폭시 냄새\",\n",
    "            \"페인트 희석제 냄새\", \"연료 누출 냄새\", \"고온에서 가열된 화학물 냄새\", \"전기적 소각 냄새\", \"기체 혼합 냄새\",\n",
    "            \"고체 화학물 분해 냄새\", \"소성된 플라스틱 냄새\", \"공업용 윤활유 냄새\", \"화학 반응 부산물 냄새\", \"촉매 작용 중 발생한 냄새\",\n",
    "\n",
    "            # 공장 관련 추가 항목\n",
    "            \"제조 공정 중 발생한 냄새\", \"원료 처리 냄새\", \"기계 부품 기름 냄새\", \"공장 내 배출 가스 냄새\",\n",
    "            \"열처리 냄새\", \"금속 가공 냄새\", \"연속 생산 공정 냄새\", \"모터 및 기계 작동 냄새\", \"기계 설비 점검 냄새\",\n",
    "            \"연마 및 절단 공정 냄새\", \"플라스틱 성형 냄새\", \"고온 가열된 금속 냄새\", \"공장 작업장 냄새\",\n",
    "            \"배터리 제조 냄새\", \"인쇄 공정 냄새\", \"플라스틱 압출 냄새\", \"화학 공정 부산물 냄새\", \"스프레이 도장 냄새\",\n",
    "            \"금속 냄새\", \"산업용 페인트 냄새\", \"자동차 조립 공정 냄새\", \"화학적 물질 증발 냄새\", \"포장지 처리 냄새\",\n",
    "            \"배기 시스템에서 발생한 냄새\", \"인쇄기에서 나는 냄새\", \"제품 포장 냄새\", \"소각 및 재활용 냄새\", \"농업 생산 공정 냄새\"\n",
    "        ],\n",
    "        \"intensity_adjectives\": [\n",
    "            # 기존 항목\n",
    "            \"습하게\", \"축축하게\", \"뜨겁게\", \"서늘하게\", \"쾌쾌하게\",\n",
    "            \"지독하게\", \"자극적으로\", \"비릿하게\", \"무겁게\", \"매캐하게\",\n",
    "            \"강하게\", \"불쾌하게\", \"날카롭게\", \"침투성 있게\", \"섞인 듯이\",\n",
    "            \"지속적으로\", \"심하게\", \"뿌옇게\", \"잔뜩\", \"독하게\",\n",
    "            \"질식할 듯이\", \"스산하게\", \"뚜렷하게\", \"가득히\", \"찌르는 듯이\",\n",
    "            \"폭발적으로\", \"스며들 듯이\", \"거슬리게\", \"끈질기게\", \"오랫동안 남아있게\",\n",
    "            \"혼탁하게\", \"무겁고 답답하게\", \"강렬하게\", \"끈적하게\", \"날카롭게 퍼지게\",\n",
    "\n",
    "            # 추가된 항목\n",
    "            \"서서히 짙어지는\", \"갑작스럽게 확산되는\", \"무겁게 내려앉는\", \"가볍게 스며드는\", \"매우 빠르게 확산되는\",\n",
    "            \"바람을 타고 퍼지는\", \"순식간에 퍼지는\", \"잔향이 깊게 남는\", \"짧지만 강렬하게\", \"주변을 완전히 뒤덮는\",\n",
    "            \"냄새가 점점 심해지는\", \"코를 찌를 정도로 날카로운\", \"압도적으로 퍼지는\", \"열기와 함께 퍼지는\", \"피할 수 없이 강렬한\",\n",
    "            \"숨쉬기 어려울 정도로\", \"바닥에서 올라오는 듯한\", \"지속적으로 자극적인\", \"순간적으로 강해지는\", \"공기 중에 깊게 스며드는\",\n",
    "            \"공장에서 발생하는\", \"기계에서 나는\", \"화학물질로 가득한\", \"금속처럼 강한\", \"기름진 냄새로 가득한\",\n",
    "            \"연기처럼 짙게 퍼지는\", \"알칼리성 냄새가 나는\", \"배기구에서 나오는\", \"불에 타는 듯한\", \"고농도로 확산되는\",\n",
    "            \"침착하게 퍼지는\", \"시멘트처럼 탁한\", \"강한 자극이 지속되는\", \"자동차 연료 냄새처럼\", \"기계소리와 함께 울리는\"\n",
    "        ],\n",
    "\n",
    "\n",
    "        \"effects\": [\n",
    "            \"호흡이 어려움\", \"불쾌감\", \"피로감\", \"두통\", \"심한 자극\",\n",
    "            \"산소 부족\", \"기운이 떨어짐\", \"눈이 따끔거림\", \"집중력이 떨어짐\", \"숨쉬기 힘듦\",\n",
    "            \"어지럼증\", \"메스꺼움\", \"고통스러움\", \"불쾌한 기분\", \"화학물질로 인한 부작용\",\n",
    "            \"기계 소리로 인한 스트레스\", \"피부 자극\", \"호흡기 질환 유발\", \"불안감\", \"우울한 기분\"\n",
    "        ],\n",
    "        \"issues\": [\n",
    "            \"화학물질 유출\", \"공기 오염\", \"소음 문제\", \"산업 폐기물 처리 문제\", \"고온 방출 문제\",\n",
    "            \"화학 가스 누출\", \"폐수 방류\", \"타는 냄새 발생\", \"연기 발생\", \"불법 배출\",\n",
    "            \"불완전 연소\", \"가스 폭발 위험\", \"기계 고장\", \"부주의로 인한 사고\", \"공기 질 악화\",\n",
    "            \"소음 공해\", \"환경 파괴\", \"미세먼지 발생\", \"기계 오작동\", \"산업 재해\"\n",
    "        ],\n",
    "        \"departments\": [\n",
    "            \"환경 관리부\", \"안전 관리부\", \"시설 관리부\", \"기계 설비 부서\", \"전기 관리부\",\n",
    "            \"화학 부서\", \"폐기물 처리 부서\", \"사고 예방 부서\", \"노동 안전 부서\", \"품질 관리 부서\",\n",
    "            \"기술 부서\", \"연구개발 부서\", \"운영 부서\", \"제조 부서\", \"정비 부서\",\n",
    "            \"에너지 관리 부서\", \"운송 부서\", \"공정 관리 부서\", \"위험물 관리 부서\", \"프로젝트 관리 부서\"\n",
    "        ],\n",
    "        \"time_of_days\": [\n",
    "        ]\n",
    "     },\n",
    "\n",
    "    \"축산 관련\": {\n",
    "        \"locations\": [\n",
    "            # 기존 항목\n",
    "            \"축사\", \"가축 사육장\", \"도살장\", \"젖소 농장\", \"돼지 농장\",\n",
    "            \"닭 농장\", \"가금류 농장\", \"양 농장\", \"산란계 농장\", \"육계 농장\",\n",
    "            \"소 사육장\", \"염소 농장\", \"오리 농장\", \"타조 농장\", \"사료 공장\",\n",
    "            \"가축 폐기물 처리장\", \"동물 보호소\", \"동물 실험 시설\", \"농업 단지\", \"대규모 축산 시설\",\n",
    "            \"우시장\", \"작은 동물 농장\", \"방목된 가축 사육지\", \"계란 생산 농장\", \"유제품 가공 공장\",\n",
    "            \"축산 폐수 처리장\", \"말 농장\", \"사슴 농장\", \"낙타 농장\", \"산림지 축산 단지\",\n",
    "            \"농촌 지역 축사\", \"가축 퇴비 저장소\", \"야생동물 보호 구역\", \"동물 사육 체험장\", \"생축 경매장\",\n",
    "            \"가축 운송 시설\", \"가축 질병 격리 시설\", \"고지대 목장\", \"호수 인근 축사\", \"냄새 민감 지역 축사\",\n",
    "\n",
    "            # 추가된 항목\n",
    "            \"가축 방역 시설\", \"동물 백신 생산 공장\", \"우유 저온 저장고\", \"방목형 젖소 목장\", \"동물 훈련장\",\n",
    "            \"고기 가공 공장\", \"축산물 유통 센터\", \"사료 연구소\", \"가축 번식 센터\", \"동물 병원 인근 시설\",\n",
    "            \"도시 외곽 축사 단지\", \"대형 동물 목장\", \"동물 재활 센터\", \"지붕형 사육 시설\", \"농촌 마을 축사\",\n",
    "            \"수출용 가축 시설\", \"수산물 가공 축사\", \"소규모 방목지\", \"습지 인근 축사\", \"가축 이동 경로\",\n",
    "            \"생물 다양성 보호 구역\", \"양봉 농장\", \"염전 인근 목장\", \"야생동물 복원지\", \"가축 사료 분배 센터\",\n",
    "            \"신선 우유 가공 시설\", \"포장육 제조 공장\", \"동물 복지 친화형 농장\", \"산악 지역 목장\", \"도축 전 검사 시설\",\n",
    "            \"공동 경작지 축사\", \"친환경 축산 단지\", \"저온 창고 축산 시설\", \"계절 방목지\", \"호수 근처 목초지\",\n",
    "            \"가축 교육 체험장\", \"작물과 축산 융합지\", \"바이오매스 에너지 목장\", \"유기농 사료 제조 공장\", \"동물 치료 시설\",\n",
    "            \"종축 시험장\", \"밀집형 가축 농장\", \"동물 클리닉\", \"농촌 테마파크\", \"산림 보호 축사\",\n",
    "            \"농업 시험 연구소\", \"동물 사료 혼합 공장\", \"방사형 축사\", \"기후 친화형 목장\", \"고지대 방목 시설\"\n",
    "        ],\n",
    "        \"smell_types\": [\n",
    "            # 기존 항목\n",
    "            \"악취\", \"분뇨 냄새\", \"동물 배설물 냄새\", \"소 똥 냄새\", \"돼지 똥 냄새\",\n",
    "            \"새우 배설물 냄새\", \"시큼한 냄새\", \"퇴비 냄새\", \"동물 사체 냄새\", \"음식물 냄새\",\n",
    "            \"동물의 땀 냄새\", \"단백질 냄새\", \"육류 냄새\", \"비린 냄새\", \"불쾌한 냄새\",\n",
    "            \"새끼 동물 냄새\", \"소화불량 냄새\", \"기름 냄새\", \"가축 사료 냄새\", \"발효된 냄새\",\n",
    "            \"침전된 물질 냄새\", \"동물 사료 냄새\", \"자극적인 냄새\", \"축산물 가공 냄새\", \"석회 냄새\",\n",
    "\n",
    "            # 추가된 항목\n",
    "            \"소독약 냄새\", \"가스 냄새\", \"썩은 풀 냄새\", \"농축된 암모니아 냄새\", \"젖은 동물 냄새\",\n",
    "            \"동물 방목지 냄새\", \"부패된 사료 냄새\", \"연소된 건초 냄새\", \"유기물 부패 냄새\", \"고기 굽는 냄새\",\n",
    "            \"축사 배기구 냄새\", \"발효된 퇴비 냄새\", \"부패된 알 냄새\", \"분해 중인 동물 냄새\", \"가축 운송 트럭 냄새\",\n",
    "            \"저장된 배설물 냄새\", \"젖소 목장 냄새\", \"양 사육장 냄새\", \"가축 장비에서 나는 냄새\", \"비료 제조 과정 냄새\",\n",
    "            \"우사 내부 냄새\", \"건초 저장고 냄새\", \"폐수 처리장 냄새\", \"젖은 흙 냄새\", \"축산 폐기물 냄새\",\n",
    "\n",
    "            # 추가 제안\n",
    "            \"소 똥 냄새에 혼합된 기름 냄새\", \"가축 사육장 특유의 냄새\", \"과도하게 발효된 퇴비 냄새\",\n",
    "            \"육류 처리 공정 냄새\", \"말린 풀 냄새\", \"농장 내부 공기 냄새\", \"가축의 배설물이 부패한 냄새\",\n",
    "            \"건초 더미 냄새\", \"소의 사료가 부패한 냄새\", \"수의학 치료 후의 특이한 냄새\", \"전염병 발생 지역 냄새\",\n",
    "            \"젖소와 관련된 특유의 냄새\", \"발효된 우유 냄새\", \"동물 건강 관리용 약물 냄새\", \"농장 주변 공기 냄새\",\n",
    "            \"농업 폐기물 냄새\", \"양식장에서 나는 냄새\", \"가축의 배설물 처리 냄새\", \"비료 보관소 냄새\"\n",
    "        ],\n",
    "        \"intensity_adjectives\": [\n",
    "            # 기존 항목\n",
    "            \"강하게\", \"지속적으로\", \"심하게\", \"불쾌하게\", \"역하게\",\n",
    "            \"찌들게\", \"매캐하게\", \"매우\", \"강렬하게\", \"매우 강하게\",\n",
    "            \"거세게\", \"자욱하게\", \"잔뜩\", \"지독하게\", \"짙게\",\n",
    "            \"심각하게\", \"스며들 듯이\", \"찬물에 푹 담근 듯이\", \"집요하게\", \"끈질기게\",\n",
    "\n",
    "            # 추가된 항목\n",
    "            \"날카롭게 퍼지는\", \"냄새가 한꺼번에 밀려오는\", \"서서히 스며드는\", \"머리가 아플 정도로\",\n",
    "            \"숨 막힐 정도로 강한\", \"무겁게 눌러오는\", \"끈적이게 남아 있는\", \"습기와 섞여 지속적으로\",\n",
    "            \"쾌쾌하게 스며드는\", \"농도가 점점 짙어지는\", \"냄새가 확 퍼지는\", \"비오는 날처럼 퍼지는\",\n",
    "            \"잔향이 강하게 남는\", \"자극이 지속적으로 반복되는\", \"코끝을 찌르는\", \"열기와 함께 퍼지는\",\n",
    "            \"바람을 타고 퍼지는\", \"농도가 매우 짙게\", \"주변에 완전히 스며드는\", \"강도가 불규칙적으로 변하는\",\n",
    "            \n",
    "            # 추가된 축산 관련 항목\n",
    "            \"고약하게 퍼지는\", \"구린내처럼 퍼지는\", \"지속적으로 짙어지는\", \"소독되지 않은 듯한\", \"강하게 눅눅한\",\n",
    "            \"심하게 얽힌\", \"썩은 듯한 냄새가 나는\", \"짙은 분뇨 냄새\", \"가축에서 나는\", \"똥내처럼 짙게 퍼지는\",\n",
    "            \"동물의 체취처럼\", \"비위가 약해지는\", \"소리 없이 짙어지는\", \"끈질기게 감도는\", \"피가 나듯한 냄새\",\n",
    "            \"불쾌한 냄새가 진동하는\", \"걸러지지 않은 듯한\", \"깨끗하지 않은 환경에서 나는\", \"기름기처럼 끈적한\",\n",
    "            \"굳어져 있는 듯한\", \"산더미처럼 쌓인\", \"온몸을 감싸는\", \"깊게 스며든 냄새\", \"악취가 사라지지 않는\"\n",
    "        ],\n",
    "\n",
    "        \"effects\": [\n",
    "            \"호흡 곤란\", \"메스꺼움\", \"불쾌감\", \"피로감\", \"두통\",\n",
    "            \"기운이 빠짐\", \"눈 자극\", \"어지러움\", \"냄새로 인한 스트레스\", \"심한 자극\",\n",
    "            \"구역질\", \"기분 나쁨\", \"화학적 반응\", \"심한 불쾌감\", \"호흡기 질환\",\n",
    "            \"피부 자극\", \"산소 부족\", \"집중력 저하\", \"호흡기 알레르기\", \"불안감\"\n",
    "        ],\n",
    "        \"issues\": [\n",
    "            \"동물 배설물 처리 문제\", \"악취 발생\", \"환경 오염\", \"위생 문제\", \"사료 관리 문제\",\n",
    "            \"동물 건강 문제\", \"동물 도살 문제\", \"가축 질병 발생\", \"미세먼지 발생\", \"불법 축산물 유통\",\n",
    "            \"동물 학대 문제\", \"동물 복지 문제\", \"축산 폐기물 처리 문제\", \"소음 문제\", \"분뇨 유출\",\n",
    "            \"해충 문제\", \"하수도 문제\", \"악취와 건강 문제\", \"작물 오염\", \"가축 재배 문제\"\n",
    "        ],\n",
    "        \"departments\": [\n",
    "            \"환경 관리부\", \"농업 부서\", \"축산업 부서\", \"식품 위생 부서\", \"동물 관리 부서\",\n",
    "            \"사료 관리 부서\", \"농업 안전 부서\", \"위생 관리 부서\", \"농업 시설 관리 부서\", \"질병 관리 부서\",\n",
    "            \"동물 복지 부서\", \"농업 교육 부서\", \"환경 보호 부서\", \"농업 연구 부서\", \"농장 안전 부서\",\n",
    "            \"소음 및 악취 관리 부서\", \"농업 생산 부서\", \"폐기물 처리 부서\", \"축산업 기술 부서\", \"수의학 부서\"\n",
    "        ],\n",
    "        \"time_of_days\": [\n",
    "\n",
    "        ]\n",
    "     },\n",
    "\n",
    "    \"생활 악취 관련\": {\n",
    "        \"locations\": [\n",
    "            # 기존 항목\n",
    "            \"주택가\", \"집\", \"아파트 단지\", \"공동 주택\", \"주차장\", \"상가 근처\",\n",
    "            \"학교 주변\", \"병원 주변\", \"도시 근교\", \"공원\", \"놀이터\",\n",
    "            \"버스 정류장\", \"지하철 역\", \"길거리\", \"대형마트\", \"택시 승강장\",\n",
    "            \"레스토랑 주변\", \"카페 근처\", \"도심지\", \"주택가 골목길\", \"공공 화장실\"\n",
    "\n",
    "            # 추가된 항목\n",
    "            \"공공 도서관 근처\", \"운동장\", \"공원 벤치 주변\", \"수영장 입구\", \"극장 주변\",\n",
    "            \"공공 주택 정원\", \"도시 외곽의 버스 정류장\", \"대형 병원 응급실 근처\", \"길거리 노점상\",\n",
    "            \"전통시장 골목\", \"지하 주차장\", \"도심 공원 내 산책로\", \"유치원 주변\", \"어린이 놀이터 근처\",\n",
    "            \"택배 물류 창고 근처\", \"주요 도심 광장\", \"도시 재개발 지역\", \"쓰레기통이 많은 골목길\",\n",
    "            \"공공 체육관\", \"헬스장 근처\", \"지역 경로당\", \"노인 복지 센터 인근\", \"공동묘지 근처\",\n",
    "            \"하수 처리장 근처\", \"지하수 펌프 시설\", \"고가도로 밑\", \"낡은 건물 외벽 주변\", \"철도역 근처\",\n",
    "            \"공공 정류장 쉼터\", \"아파트 쓰레기 분리수거장\", \"폐쇄된 건물 입구\", \"노숙인 쉼터 근처\", \"도심 가로수길\",\n",
    "            \"쇼핑센터 내부 화장실\", \"푸드코트\", \"전기차 충전소\", \"도시 관광 명소\", \"산책로 주변 상점\",\n",
    "            \"호수 인근 주택가\", \"주말 장터\", \"가로등 아래\", \"강변 근처 주차장\", \"공원 내 분수대 주변\",\n",
    "            \"놀이터 그네 옆\", \"소규모 공원 인근\", \"산속 펜션 근처\", \"커뮤니티 센터 근처\", \"교외 지역 마을길\",\n",
    "            \"도심지 보행자 전용도로\", \"전원주택 단지\", \"동네 산책길\", \"조경용 연못 주변\", \"도시 중심 상업지구\",\n",
    "            \"지역 행사장\", \"거리 공연장 근처\", \"지하철 환승 구간\", \"공공 빌딩 옥상 공원\", \"지하 쇼핑센터 출입구\",\n",
    "            \"차량 정체 구간 근처\", \"골목길 식당가\", \"중소형 마트 주차장\", \"주유소 근처\", \"버스 차고지 인근\",\n",
    "            \"도로변 쓰레기 수거장\", \"공공 세탁소\", \"도시 야시장\", \"사무실 건물 후문\", \"아파트 커뮤니티 센터\",\n",
    "            \"도심 방범 초소 근처\", \"생활 편의시설\", \"공용 화장실 인근\", \"개방된 창고 지역\", \"농산물 시장 입구\",\n",
    "            \"대규모 콘서트홀 근처\", \"야외 플리마켓\", \"공공 안전센터\", \"지하 식품 매장 입구\", \"도심 카페 거리\"\n",
    "        ],\n",
    "\n",
    "        \"smell_types\": [\n",
    "            # 기존 항목\n",
    "            \"악취\", \"음식물 냄새\", \"쓰레기 냄새\", \"하수구 냄새\", \"화학 냄새\", \"수둇물 냄새\",\n",
    "            \"배수구 냄새\", \"쓰레기통 냄새\", \"담배 냄새\", \"곰팡이 냄새\", \"사람 땀 냄새\",\n",
    "            \"동물 냄새\", \"오래된 냄새\", \"불쾌한 냄새\", \"자극적인 냄새\", \"타는 냄새\",\n",
    "            \"강한 냄새\", \"화장실 냄새\", \"음악 센터 냄새\", \"비린내\", \"가스 냄새\",\n",
    "\n",
    "            # 추가된 항목\n",
    "            \"조리 후 남은 냄새\", \"불타는 고기 냄새\", \"오염된 물 냄새\", \"썩은 음식 냄새\", \"이끼 냄새\",\n",
    "            \"밀폐된 공간 냄새\", \"냉동고 냄새\", \"찜찜한 냄새\", \"오염된 배관 냄새\", \"낡은 나무 냄새\",\n",
    "            \"젖은 카펫 냄새\", \"스프레이 냄새\", \"강한 세제 냄새\", \"소독약 냄새\", \"기름 냄새\",\n",
    "            \"공장 배출 가스 냄새\", \"화재 잔여물 냄새\", \"고기 썩은 냄새\", \"양념된 음식 냄새\", \"비린 고기 냄새\",\n",
    "            \"고온 처리 후 냄새\", \"곰팡이가 핀 음식 냄새\", \"과일 썩은 냄새\", \"계란 썩은 냄새\", \"축축한 흙 냄새\",\n",
    "\n",
    "            # 추가 제안\n",
    "            \"눅눅한 음식 냄새\", \"고장난 냉장고 냄새\", \"입 냄새\", \"화장실 청소 후 냄새\", \"대기 오염 냄새\",\n",
    "            \"옷장 냄새\", \"젖은 빨래 냄새\", \"남은 음식물 냄새\", \"곰팡이가 핀 천 냄새\", \"빗물에 젖은 냄새\",\n",
    "            \"수영장 물 냄새\", \"불쾌한 향수 냄새\", \"상한 유제품 냄새\", \"상한 고기 냄새\", \"냉장고 안 냄새\",\n",
    "            \"쓰레기장 냄새\", \"부패된 채소 냄새\", \"버려진 음식 냄새\", \"자취방 냄새\", \"물빠진 천 냄새\",\n",
    "            \"노후된 배수구 냄새\", \"고장난 세탁기 냄새\"\n",
    "        ],\n",
    "        \"intensity_adjectives\": [\n",
    "            # 기존 항목\n",
    "            \"강하게\", \"지속적으로\", \"심하게\", \"불쾌하게\", \"역하게\",\n",
    "            \"찌들게\", \"매캐하게\", \"매우\", \"강렬하게\", \"매우 강하게\",\n",
    "            \"거세게\", \"자욱하게\", \"잔뜩\", \"지독하게\", \"짙게\",\n",
    "            \"심각하게\", \"스며들 듯이\", \"찬물에 푹 담근 듯이\", \"집요하게\", \"끈질기게\",\n",
    "\n",
    "            # 추가된 항목\n",
    "            \"날카롭게 퍼지는\", \"냄새가 한꺼번에 밀려오는\", \"서서히 스며드는\", \"머리가 아플 정도로\",\n",
    "            \"숨 막힐 정도로 강한\", \"무겁게 눌러오는\", \"끈적이게 남아 있는\", \"습기와 섞여 지속적으로\",\n",
    "            \"쾌쾌하게 스며드는\", \"농도가 점점 짙어지는\", \"냄새가 확 퍼지는\", \"비오는 날처럼 퍼지는\",\n",
    "            \"잔향이 강하게 남는\", \"자극이 지속적으로 반복되는\", \"코끝을 찌르는\", \"열기와 함께 퍼지는\",\n",
    "            \"바람을 타고 퍼지는\", \"농도가 매우 짙게\", \"주변에 완전히 스며드는\", \"강도가 불규칙적으로 변하는\",\n",
    "\n",
    "            # 추가된 생활악취 관련 항목\n",
    "            \"음식물 냄새처럼\", \"곰팡이 냄새가 나는\", \"불쾌한 찌든 냄새\", \"먼지와 혼합된\", \"화장실에서 나는\",\n",
    "            \"기름진 냄새\", \"찝찝한 냄새\", \"오래된 듯한\", \"불쾌한 배수관 냄새\", \"소독되지 않은 듯한\",\n",
    "            \"눅눅한 냄새\", \"새로운 페인트 냄새처럼\", \"깨끗하지 않은 환경에서 나는\", \"짙은 쓰레기 냄새\",\n",
    "            \"담배 연기 냄새처럼\", \"비위가 상하는\", \"냄새가 벽에 배어드는\", \"피곤할 때 나는\", \"자주 나는 냄새\",\n",
    "            \"쓰레기 냄새가 나는\", \"빨래에서 나는 습한 냄새\", \"공기 중에 오래 남는\", \"찌꺼기처럼 남아있는\",\n",
    "            \"껌 냄새처럼 퍼지는\", \"가습기에서 나는 냄새\", \"악취가 스며드는\", \"낡은 가구에서 나는\", \n",
    "            \"기름과 세제의 혼합 냄새\"\n",
    "        ],\n",
    "\n",
    "\n",
    "        \"effects\": [\n",
    "            \"호흡 곤란\", \"메스꺼움\", \"불쾌감\", \"피로감\", \"두통\",\n",
    "            \"기운이 빠짐\", \"눈 자극\", \"어지러움\", \"냄새로 인한 스트레스\",\n",
    "            \"심한 자극\", \"구역질\", \"기분 나쁨\", \"화학적 반응\", \"심한 불쾌감\",\n",
    "            \"호흡기 질환\", \"피부 자극\", \"산소 부족\", \"집중력 저하\", \"호흡기 알레르기\",\n",
    "            \"불안감\"\n",
    "    \t],\n",
    "\n",
    "        \"issues\": [\n",
    "            \"냄새로 인한 생활 불편\", \"주민 건강에 미치는 영향\", \"주민 불만 증가\",\n",
    "            \"공기 질 저하\", \"불쾌한 환경\", \"심각한 생활 환경 문제\", \"청결 부족\",\n",
    "            \"위생 상태 문제\", \"불쾌한 공기\", \"악취 배출 통제 문제\", \"환경 오염\",\n",
    "            \"냄새로 인한 스트레스\", \"심각한 공공 문제\", \"일상 생활 불편\",\n",
    "            \"집안 환경의 악취\", \"사회적 불편\", \"소음과 냄새의 복합적 문제\", \"냄새 민감도 증가\"\n",
    "        ],\n",
    "        \"departments\": [\n",
    "            \"환경부\", \"보건복지부\", \"지자체 환경 부서\", \"시청 환경 관리 부서\",\n",
    "            \"주택 관리 부서\", \"상수도 관리 부서\", \"도시 관리 부서\",\n",
    "            \"보건소\", \"지역 환경 협의체\", \"위생 관리 부서\", \"공공건물 관리 부서\",\n",
    "            \"지자체 보건 부서\", \"하수도 관리 부서\", \"교통부\", \"민원센터\",\n",
    "            \"주차장 관리 부서\", \"상가 관리 부서\", \"도시 재개발 부서\",\n",
    "            \"환경정화 부서\", \"공공서비스 부서\"\n",
    "        ],\n",
    "        \"time_of_days\": [\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    \"건설현장 관련\": {\n",
    "        \"locations\": [\n",
    "            # 기존 항목\n",
    "            \"주택 공사 현장\", \"상업용 건물 공사 현장\", \"도로 건설 현장\", \"다리 건설 현장\",\n",
    "            \"철도 건설 현장\", \"지하철 공사 현장\", \"도시 재개발 현장\", \"고속도로 공사 현장\",\n",
    "            \"기반 시설 공사 현장\", \"상하수도 공사 현장\", \"아파트 단지 건설 현장\",\n",
    "            \"상가 및 빌딩 건설 현장\", \"학교 건설 현장\", \"병원 건설 현장\", \"공장 건설 현장\",\n",
    "            \"터널 건설 현장\", \"산업단지 건설 현장\", \"고층 건물 공사 현장\", \"주차장 건설 현장\",\n",
    "            \"공항 건설 현장\",\n",
    "\n",
    "            # 추가된 항목\n",
    "            \"항만 시설 건설 현장\", \"스포츠 경기장 건설 현장\", \"휴게소 공사 현장\", \"도심 공원 조성 공사 현장\",\n",
    "            \"농업 기반 시설 건설 현장\", \"방파제 건설 현장\", \"에너지 플랜트 공사 현장\", \"풍력 발전소 건설 현장\",\n",
    "            \"태양광 발전소 건설 현장\", \"댐 건설 현장\", \"수로 확장 공사 현장\", \"하천 정비 공사 현장\",\n",
    "            \"도시 미관 개선 공사 현장\", \"복합 쇼핑몰 건설 현장\", \"지하 매설 시설 공사 현장\", \"산림 복구 공사 현장\",\n",
    "            \"공공 광장 건설 현장\", \"폐쇄된 건물 재건축 현장\", \"도심 공공 시설 건설 현장\", \"국제 컨벤션 센터 공사 현장\",\n",
    "            \"주택가 도로 확장 공사 현장\", \"신도시 개발 현장\", \"지역 도서관 건설 현장\", \"군사 기지 시설 공사 현장\",\n",
    "            \"노후 건물 철거 현장\", \"어린이 놀이터 조성 현장\", \"시민공원 주차장 공사 현장\", \"대형 교량 공사 현장\",\n",
    "            \"산악 지역 도로 공사 현장\", \"해안 방어벽 건설 현장\", \"방음벽 설치 현장\", \"공동 주택 복원 공사 현장\",\n",
    "            \"전원주택 단지 개발 현장\", \"지하철 승강장 공사 현장\", \"지하 상가 건설 현장\", \"고속철도 터미널 공사 현장\",\n",
    "            \"도심 도로 보수 공사 현장\", \"폐수 처리장 공사 현장\", \"하수도 증설 공사 현장\", \"도시 외곽 고속도로 건설 현장\",\n",
    "            \"산업용 창고 공사 현장\", \"물류 단지 조성 현장\", \"군사 방어 시설 공사 현장\", \"지속 가능 건축 프로젝트 현장\",\n",
    "            \"지방 중소 도시 개발 현장\", \"지역 박물관 건설 현장\", \"유휴 토지 개발 현장\", \"학교 운동장 공사 현장\",\n",
    "            \"고층 호텔 건설 현장\", \"초고층 타워 공사 현장\", \"지하 방공호 건설 현장\", \"복합 문화 센터 건설 현장\",\n",
    "            \"스마트 시티 기반 시설 공사 현장\", \"해양 연구소 건설 현장\", \"대형 콘서트 홀 공사 현장\", \"농촌 마을 기반 시설 공사 현장\"\n",
    "        ],\n",
    "\n",
    "        \"smell_types\": [\n",
    "            # 기존 항목\n",
    "            \"시멘트 냄새\", \"타는 냄새\", \"화학 물질 냄새\", \"아스팔트 냄새\", \"석유 냄새\",\n",
    "            \"배관 설치 냄새\", \"청소 화학 냄새\", \"페인트 냄새\", \"솔벤트 냄새\", \"금속 냄새\",\n",
    "            \"붕괴된 건축물 냄새\", \"조합 재료 냄새\", \"비산 먼지 냄새\", \"기계 오일 냄새\",\n",
    "            \"유해 화학물질 냄새\", \"기계 연료 냄새\", \"시멘트 분말 냄새\", \"섬유 화학 냄새\",\n",
    "            \"건설 자재 냄새\", \"유리 및 플라스틱 재료 냄새\",\n",
    "\n",
    "            # 추가된 항목\n",
    "            \"방수 코팅 냄새\", \"목재 가공 냄새\", \"접착제 냄새\", \"고무 타는 냄새\", \"가스 누출 냄새\",\n",
    "            \"연소된 폐기물 냄새\", \"합성수지 냄새\", \"석탄 가루 냄새\", \"절단된 금속 냄새\", \"방부제 냄새\",\n",
    "            \"방열재 냄새\", \"플라스틱 용융 냄새\", \"부패된 잔해 냄새\", \"녹슨 철 냄새\", \"산성 증기 냄새\",\n",
    "            \"연마 공정 냄새\", \"섬유 강화 플라스틱 냄새\", \"방사된 연기 냄새\", \"오염된 하수 냄새\", \"석회석 냄새\",\n",
    "            \"주조 공정 냄새\", \"도료 혼합 냄새\", \"천장 단열재 냄새\", \"파쇄된 돌 냄새\", \"건식 재료 냄새\",\n",
    "            \"기타 건축 폐기물 냄새\", \"폴리우레탄 냄새\", \"열 처리 냄새\", \"아연 코팅 냄새\", \"폐기된 자재 냄새\",\n",
    "\n",
    "            # 추가 제안\n",
    "            \"벽돌 가공 냄새\", \"조적 공정 냄새\", \"광물 가루 냄새\", \"건설기계 배기 냄새\", \"철근 가공 냄새\",\n",
    "            \"비닐 방수막 냄새\", \"타르 냄새\", \"염화비닐 냄새\", \"구리 배선 냄새\", \"고온 작업장 냄새\",\n",
    "            \"건설 폐기물 냄새\", \"우레탄폼 냄새\", \"세라믹 타일 냄새\", \"조명기구 설치 냄새\", \"비계 설치 냄새\"\n",
    "        ],\n",
    "\n",
    "\n",
    "        \"intensity_adjectives\": [\n",
    "            # 기존 항목\n",
    "            \"강하게\", \"지속적으로\", \"심하게\", \"자욱하게\", \"불쾌하게\",\n",
    "            \"심각하게\", \"매캐하게\", \"화학적으로\", \"묵직하게\", \"자극적으로\",\n",
    "            \"독하게\", \"지독하게\", \"폭발적으로\", \"강렬하게\", \"비산성으로\",\n",
    "            \"눈에 띄게\", \"빈번하게\", \"거슬리게\", \"질식할 정도로\", \"짙게\",\n",
    "\n",
    "            # 추가된 항목\n",
    "            \"빠르게 퍼지는\", \"강렬히 스며드는\", \"강한 농도로\", \"일시적으로\", \"점진적으로 강해지는\",\n",
    "            \"압도적으로\", \"코를 찌르는\", \"강한 잔향이 남는\", \"서서히 스며드는\", \"강렬한 초기 강도\",\n",
    "            \"산성 기운이 강한\", \"흡입 시 자극적인\", \"눈이 아릴 정도로\", \"침투력이 높은\", \"숨막히게 강한\",\n",
    "            \"산뜻하지 못한\", \"축적된 듯한\", \"더운 공기에 섞인\", \"잦은 반복으로\", \"혼합된 자극적인\",\n",
    "\n",
    "            # 추가된 건설현장 관련 항목\n",
    "            \"건설 자재 냄새가 섞인\", \"시멘트 냄새처럼\", \"비산된 먼지와 섞인\", \"연료 냄새로 가득한\", \"철강 냄새가 나는\",\n",
    "            \"기계에서 나는 기름 냄새\", \"타이어 굴러가는 소리와 함께 나는\", \"불에 탄 듯한 냄새\", \"젖은 흙에서 나는\",\n",
    "            \"쇠가 닳은 듯한 냄새\", \"땅속에서 올라오는 듯한\", \"불완전 연소된 듯한\", \"덩어리처럼 굳어져 있는 냄새\",\n",
    "            \"강한 화학물질 냄새\", \"주변 공기 중에 퍼지는\", \"기름과 연기가 섞인\", \"가벼운 먼지 냄새가 나는\", \"갑작스레 밀려오는\"\n",
    "        ],\n",
    "\n",
    "\n",
    "        \"effects\": [\n",
    "            \"호흡 곤란\", \"눈 자극\", \"피로감\", \"어지러움\", \"불쾌감\",\n",
    "            \"기침\", \"두통\", \"기운이 빠짐\", \"화학적 반응\", \"호흡기 질환\",\n",
    "            \"피부 자극\", \"집중력 저하\", \"스트레스\", \"심리적 불편\", \"소음으로 인한 스트레스\"\n",
    "   \t\t],\n",
    "        \"issues\": [\n",
    "            \"주변 주민 불편\", \"건설 근로자의 건강 문제\", \"대기 오염\", \"화학물질 유출\",\n",
    "            \"공사로 인한 소음과 악취\", \"공공 건강 위협\", \"도로 교통 문제\", \"공기 질 저하\",\n",
    "            \"근로자 안전 문제\", \"주거지 근처 건설 현장\", \"주차 공간 부족\", \"지속적인 냄새 문제\",\n",
    "            \"구조물 붕괴 위험\", \"지속적인 공사 소음\", \"공사 진행 중 환경 오염\", \"화학 물질 누출 위험\",\n",
    "            \"시멘트 먼지 문제\", \"불완전한 작업 환경\", \"비효율적인 건설 작업\", \"작업 환경으로 인한 스트레스\"\n",
    "        ],\n",
    "        \"departments\": [\n",
    "            \"건설부\", \"환경부\", \"시청 건설 관리 부서\", \"안전 관리 부서\", \"환경 오염 관리 부서\",\n",
    "            \"공공 안전 부서\", \"건설 현장 관리 부서\", \"위험 관리 부서\", \"주택 관리 부서\",\n",
    "            \"도로 공사 관리 부서\", \"교통 관리 부서\", \"지자체 환경 관리 부서\", \"위생 관리 부서\",\n",
    "            \"재활용 및 폐기물 관리 부서\", \"시설 공사 관리 부서\", \"시설 안전 부서\",\n",
    "            \"노동부\", \"보건소\", \"건설 근로자 안전 부서\", \"공공 서비스 부서\"\n",
    "        ],\n",
    "        \"time_of_days\": [\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_all_combinations_to_categories(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 템플릿 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "citizen_greeting_templates = [\n",
    "    \"{location}에서 {time_of_day}에 발생한 {smell_type} {intensity_adjective} {issue}로 연락드렸습니다.\",\n",
    "    \"안녕하세요. {smell_type} {intensity_adjective} 문제로 {location}에서 {time_of_day}에 전화드렸습니다.\",\n",
    "    \"저희 {location}에서 {smell_type} {intensity_adjective} 문제로 불편을 겪고 있습니다. {time_of_day}에 발생한 {issue}와 관련해 문의 드립니다.\",\n",
    "    \"여보세요? {issue}로 인해 {location}에서 {time_of_day}에 {smell_type} {intensity_adjective}가 발생했습니다.\",\n",
    "    \"안녕하세요. {time_of_day}에 {smell_type} {intensity_adjective}로 {location}에서 문제가 발생하였습니다. 문의 드립니다.\",\n",
    "    \"{location}에서 {time_of_day}에 발생한 {smell_type}이 {intensity_adjective} 나고 있어, {issue}에 대해 불편을 겪고 있습니다.\",\n",
    "    \"최근 {time_of_day}에 {location}에서 {smell_type} {intensity_adjective} 문제로 연락드립니다.\",\n",
    "    \"{location}에서 {smell_type} {intensity_adjective} {issue}가 발생하여 불편을 겪고 있습니다. {time_of_day}에 발생한 문제입니다.\",\n",
    "    \"안녕하세요. {location}에서 {time_of_day}에 발생한 {smell_type}이 {intensity_adjective}로 문제를 일으켰습니다.\",\n",
    "    \"여보세요? {location}에서 {time_of_day}에 발생한 {issue} {smell_type} {intensity_adjective}로 인해 연락드립니다.\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "agent_greeting_templates = [\n",
    "    \"네, 민원센터입니다. {location}에서 발생한 {smell_type} {intensity_adjective} 문제에 대해 상담 드리겠습니다.\",\n",
    "    \"안녕하세요. {time_of_day}에 {location}에서 발생한 {smell_type} {intensity_adjective} 문제에 대해 확인하겠습니다.\",\n",
    "    \"감사합니다. {department}입니다. {time_of_day}에 발생한 {location} 문제를 도와드리겠습니다.\",\n",
    "    \"안녕하세요. {location}에서 발생한 {time_of_day} {smell_type} {intensity_adjective} 문제를 상담 드리겠습니다.\",\n",
    "    \"네, {location}의 {issue}에 대해 신속히 확인하겠습니다. {time_of_day}에 발생한 문제입니다.\",\n",
    "    \"안녕하세요. {location}에서 발생한 {smell_type} {intensity_adjective} 문제에 대해 무엇을 도와드릴까요?\",\n",
    "    \"감사합니다. {department}입니다. {location}에서 발생한 {time_of_day} {smell_type} 문제를 도와드리겠습니다.\",\n",
    "    \"안녕하세요. {location} 관련 문제에 대해 도움을 드리겠습니다. {time_of_day}에 발생한 문제입니다.\",\n",
    "    \"네, {location}의 {smell_type} 문제에 대해 {intensity_adjective} 상황을 상담 드리겠습니다.\",\n",
    "    \"안녕하세요. {time_of_day}에 {location}에서 발생한 {issue} 문제를 확인하겠습니다.\"\n",
    "]\n",
    "\n",
    "\n",
    "citizen_complaint_templates = [\n",
    "    \"저희 {location}에서 {time_of_day}에 {smell_type}이 {intensity_adjective} 나고 있습니다. {effect}\",\n",
    "    \"최근 {location}에서 {time_of_day}에 {smell_type} {intensity_adjective} {issue}로 불편을 겪고 있습니다. {effect}\",\n",
    "    \"{location}에서 {time_of_day}에 {smell_type}이 {intensity_adjective} 나고 있어 {effect} 문제가 발생하고 있습니다.\",\n",
    "    \"저희 {location}에서 {time_of_day}에 {smell_type}이 {intensity_adjective} 나고 있어 {effect}로 불편을 겪고 있습니다.\",\n",
    "    \"저희 {location}에서 {time_of_day}에 {smell_type} {intensity_adjective} {issue}로 인해 심각한 불편을 겪고 있습니다. {effect}\",\n",
    "    \"{location}에서 {time_of_day}에 {smell_type} {intensity_adjective} 때문에 {effect}가 발생하고 있습니다.\",\n",
    "    \"최근 {location}에서 {time_of_day}에 {smell_type}이 {intensity_adjective} 나고 있어 {effect}로 큰 불편을 겪고 있습니다.\",\n",
    "    \"{location}에서 {time_of_day}에 {smell_type}이 {intensity_adjective} 나고 있으며, {effect}로 생활에 불편이 많습니다.\",\n",
    "    \"{location}에서 {time_of_day}에 발생한 {smell_type} {intensity_adjective} {issue}로 인해 {effect}로 불편함을 겪고 있습니다.\",\n",
    "    \"{location}에서 {time_of_day}에 {smell_type} {intensity_adjective} 때문에 {effect} 문제가 발생하고 있습니다.\",\n",
    "    \"저희 {location}에서 {time_of_day}에 발생한 {smell_type} {intensity_adjective} {issue}로 인해 {effect}가 발생 중입니다.\",\n",
    "    \"최근 {location}에서 {time_of_day}에 {smell_type}이 {intensity_adjective} 나고 있어 {effect}로 불편을 겪고 있습니다.\",\n",
    "    \"{location}에서 {time_of_day}에 {smell_type}이 {intensity_adjective} 나고 있어 {effect}로 매우 불편합니다.\",\n",
    "    \"{location}에서 {time_of_day}에 발생한 {smell_type} {intensity_adjective} {issue}로 인해 {effect}로 생활에 어려움을 겪고 있습니다.\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "agent_response_templates = [\n",
    "\t\"불편을 드려 대단히 죄송합니다. {effect} 해결을 위해 {location}에 대한 현장 점검을 실시하겠습니다. 잠시만 기다려 주시겠습니까?\",\n",
    "\t\"해당 문제를 해결하기 위해 최선을 다하겠습니다. 조치를 취할 수 있도록 {location}을 빠르게 점검하겠습니다.\",\n",
    "\t\"죄송합니다. {location}에서 발생한 문제에 대해 조치를 취하고 있습니다. 조금만 기다려 주세요.\",\n",
    "\t\"불편을 드려 정말 죄송합니다. {effect} 해결을 위해 신속히 대응하겠습니다.\",\n",
    "\t\"현재 {location}에 대한 점검을 시작하여 문제를 해결하겠습니다. 잠시만 기다려 주세요.\",\n",
    "\t\"문제를 해결하기 위해 최선을 다하겠습니다. 현장 점검 후 즉시 조치하겠습니다.\",\n",
    "\t\"불편을 드려 대단히 죄송합니다. 문제를 해결하기 위해 {location}을 점검하겠습니다.\",\n",
    "\t\"문제를 즉시 해결할 수 있도록 {location}에 대한 점검을 진행하겠습니다. 조금만 기다려 주세요.\",\n",
    "\t\"불편을 드려 대단히 죄송합니다. 해당 문제에 대해 최선의 조치를 취할 것입니다.\",\n",
    "\t\"죄송합니다. {location}에서 발생한 문제를 해결하기 위해 현장 점검을 진행하겠습니다.\",\n",
    "\t\n",
    "\t\"불편을 드려 대단히 죄송합니다. {time_of_day}에 발생한 {issue} 문제를 해결하기 위해 {location}에 대한 점검을 실시하겠습니다.\",\n",
    "\t\"현재 {location}에서 발생한 {issue} 문제에 대해 조치 중입니다. {time_of_day}에 완료될 예정입니다.\",\n",
    "\t\"죄송합니다. {time_of_day}에 {location}에서 발생한 문제를 해결하기 위해 신속히 대응하고 있습니다.\",\n",
    "\t\"불편을 드려 정말 죄송합니다. {time_of_day}에 발생한 {effect} 문제를 해결하기 위해 현장 점검을 진행 중입니다.\",\n",
    "\t\"{location}에서 발생한 {issue} 문제에 대해 {time_of_day}에 즉시 점검을 시작하겠습니다.\"\n",
    "]\n",
    "\n",
    "citizen_response_templates = [\n",
    "\t\"네, 그러니 빨리 좀 처리해주세요.\",\n",
    "\t\"그럼 신속히 해결해주세요.\",\n",
    "\t\"알겠습니다. 빠른 처리 부탁드립니다.\",\n",
    "\t\"네, 확인해 주세요. 빠른 해결을 바랍니다.\",\n",
    "\t\"알겠습니다. 최대한 빨리 처리해 주세요.\",\n",
    "\t\"그럼 문제를 빨리 해결해 주세요.\",\n",
    "\t\"네, 기다리겠습니다. 빨리 처리해 주세요.\",\n",
    "\t\"그럼 빠르게 해결해 주세요.\",\n",
    "\t\"알겠습니다. 처리 좀 해주세요.\",\n",
    "\t\"네, 처리가 되도록 부탁드립니다.\"\n",
    "]\n",
    "\n",
    "agent_acknowledge_templates = [\n",
    "\t\"네, 알겠습니다.\",\n",
    "\t\"확인 후 조치를 취하겠습니다.\",\n",
    "\t\"즉시 처리하겠습니다. 조금만 기다려 주세요.\",\n",
    "\t\"빠르게 조치를 취하겠습니다. 감사합니다.\",\n",
    "\t\"네, 확인 후 바로 처리하겠습니다.\",\n",
    "\t\"확인 후 처리하도록 하겠습니다.\",\n",
    "\t\"네, 바로 조치를 취하겠습니다.\",\n",
    "\t\"확인해 보고 빠르게 처리하겠습니다.\",\n",
    "\t\"알겠습니다. 최대한 빠르게 처리하겠습니다.\",\n",
    "\t\"네, 신속하게 처리하겠습니다.\"\n",
    "]\n",
    "\n",
    "citizen_farewell_templates = [\n",
    "\t\"수고하세요.\",\n",
    "\t\"감사합니다.\",\n",
    "\t\"좋은 하루 되세요.\",\n",
    "\t\"빠른 처리 감사합니다.\",\n",
    "\t\"수고 많으십니다.\",\n",
    "\t\"잘 부탁드립니다.\",\n",
    "\t\"고맙습니다.\",\n",
    "\t\"감사합니다. 처리 잘 부탁드립니다.\",\n",
    "\t\"수고하셨습니다.\",\n",
    "\t\"감사합니다. 좋은 하루 되세요.\"\n",
    "]\n",
    "\n",
    "agent_farewell_templates = [\n",
    "\t\"네, 감사합니다.\",\n",
    "\t\"저희가 도와드리겠습니다. 감사합니다.\",\n",
    "\t\"언제든지 연락 주세요. 감사합니다.\",\n",
    "\t\"감사합니다. 빠르게 처리하겠습니다.\",\n",
    "\t\"네, 처리 후 다시 연락드리겠습니다. 감사합니다.\",\n",
    "\t\"네, 감사합니다. 좋은 하루 되세요.\",\n",
    "\t\"저희가 도와드릴 수 있어 기쁩니다. 감사합니다.\",\n",
    "\t\"언제든지 연락주세요. 감사합니다.\",\n",
    "\t\"네, 처리 후 다시 연락드리겠습니다. 감사합니다.\",\n",
    "\t\"감사합니다. 좋은 하루 되세요.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 조합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "import random\n",
    "\n",
    "# 템플릿 순환\n",
    "citizen_greeting_cycler = cycle(citizen_greeting_templates)\n",
    "agent_greeting_cycler = cycle(agent_greeting_templates)\n",
    "citizen_complaint_cycler = cycle(citizen_complaint_templates)\n",
    "agent_response_cycler = cycle(agent_response_templates)\n",
    "\n",
    "def generate_phone_conversation(location, issue, department, smell_type, intensity_adjective, effect, time_of_day):\n",
    "    # 순환 템플릿 사용\n",
    "    citizen_greeting = next(citizen_greeting_cycler).format(\n",
    "        location=location, issue=issue, smell_type=smell_type, department=department, intensity_adjective=intensity_adjective, time_of_day=time_of_day\n",
    "    )\n",
    "    agent_greeting = next(agent_greeting_cycler).format(\n",
    "        location=location, department=department, issue=issue, smell_type=smell_type, time_of_day=time_of_day, intensity_adjective=intensity_adjective\n",
    "    )\n",
    "    \n",
    "    citizen_complaint = next(citizen_complaint_cycler).format(\n",
    "        location=location, smell_type=smell_type, intensity_adjective=intensity_adjective, effect=effect, issue=issue, time_of_day=time_of_day\n",
    "    )\n",
    "    agent_response = next(agent_response_cycler).format(\n",
    "        location=location, effect=effect, time_of_day=time_of_day, issue=issue\n",
    "    )\n",
    "    # 랜덤 선택 템플릿 사용\n",
    "    citizen_response = random.choice(citizen_response_templates)\n",
    "    agent_acknowledge = random.choice(agent_acknowledge_templates)\n",
    "    citizen_farewell = random.choice(citizen_farewell_templates)\n",
    "    agent_farewell = random.choice(agent_farewell_templates)\n",
    "    \n",
    "    conversation = f\"{citizen_greeting} {agent_greeting} {citizen_complaint} {agent_response} {citizen_response} {agent_acknowledge} {citizen_farewell} {agent_farewell}\"\n",
    "    \n",
    "    return conversation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 테스크에 맞게 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from itertools import cycle, product\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_task_data_with_max_samples(task, categories, max_samples):\n",
    "    \"\"\"\n",
    "    주요 요소를 모두 한 번씩 포함한 후, 랜덤으로 데이터를 추가하여 max_samples까지 생성.\n",
    "    task: \"locations\", \"smell_types\", \"time_of_day\" 중 하나\n",
    "    categories: 카테고리 데이터 딕셔너리\n",
    "    max_samples: 생성할 데이터의 최대 샘플 수\n",
    "    \"\"\"\n",
    "    task_data = []\n",
    "    num_categories = len(categories)\n",
    "    max_samples_per_category = max_samples // num_categories  # 카테고리당 샘플 수\n",
    "\n",
    "    for category, details in categories.items():\n",
    "        sample_count = 0  # 각 카테고리별 샘플 수\n",
    "        print(f\"Processing category: {category}\")\n",
    "\n",
    "        # 주요 요소 가져오기\n",
    "        main_elements = details[task]\n",
    "        other_elements = {\n",
    "            \"locations\": details[\"locations\"],\n",
    "            \"issues\": details[\"issues\"],\n",
    "            \"departments\": details[\"departments\"],\n",
    "            \"smell_types\": details[\"smell_types\"],\n",
    "            \"intensity_adjectives\": details[\"intensity_adjectives\"],\n",
    "            \"effects\": details[\"effects\"],\n",
    "            \"time_of_days\": details[\"time_of_days\"]\n",
    "        }\n",
    "\n",
    "        # 가능한 모든 조합 계산\n",
    "        total_combinations = len(main_elements)\n",
    "        for key, value in other_elements.items():\n",
    "            if key != task:\n",
    "                total_combinations *= len(value)\n",
    "\n",
    "        print(f\"Total combinations for category '{category}': {total_combinations}\")\n",
    "\n",
    "        # Step 1: 주요 요소의 모든 값을 한 번씩 포함\n",
    "        for main_element in main_elements:\n",
    "            if sample_count >= max_samples_per_category:\n",
    "                break\n",
    "\n",
    "            random_elements = {key: random.choice(value) for key, value in other_elements.items() if key != task}\n",
    "            text = generate_phone_conversation(\n",
    "                location=random_elements[\"locations\"] if task != \"locations\" else main_element,\n",
    "                issue=random_elements[\"issues\"],\n",
    "                department=random_elements[\"departments\"],\n",
    "                smell_type=random_elements[\"smell_types\"] if task != \"smell_types\" else main_element,\n",
    "                intensity_adjective=random_elements[\"intensity_adjectives\"],\n",
    "                effect=random_elements[\"effects\"],\n",
    "                time_of_day=random_elements[\"time_of_days\"] if task != \"time_of_days\" else main_element\n",
    "            )\n",
    "            task_data.append({\"categories\": category, \"text\": text, \"keyword\": main_element})\n",
    "            sample_count += 1\n",
    "\n",
    "        # Step 2: 랜덤으로 추가하여 max_samples_per_category까지 생성\n",
    "        while sample_count < max_samples_per_category:\n",
    "            main_element = random.choice(main_elements)\n",
    "            random_elements = {key: random.choice(value) for key, value in other_elements.items() if key != task}\n",
    "            text = generate_phone_conversation(\n",
    "                location=random_elements[\"locations\"] if task != \"locations\" else main_element,\n",
    "                issue=random_elements[\"issues\"],\n",
    "                department=random_elements[\"departments\"],\n",
    "                smell_type=random_elements[\"smell_types\"] if task != \"smell_types\" else main_element,\n",
    "                intensity_adjective=random_elements[\"intensity_adjectives\"],\n",
    "                effect=random_elements[\"effects\"],\n",
    "                time_of_day=random_elements[\"time_of_days\"] if task != \"time_of_days\" else main_element\n",
    "            )\n",
    "            task_data.append({\"categories\": category, \"text\": text, \"keyword\": main_element})\n",
    "            sample_count += 1\n",
    "\n",
    "    return task_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트에 따라 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: 쓰레기 관련\n",
      "Total combinations for category '쓰레기 관련': 34270482400000\n",
      "Processing category: 날씨 관련\n",
      "Total combinations for category '날씨 관련': 38071850400000\n",
      "Processing category: 공장 관련\n",
      "Total combinations for category '공장 관련': 96874008000000\n",
      "Processing category: 축산 관련\n",
      "Total combinations for category '축산 관련': 53765683200000\n",
      "Processing category: 생활 악취 관련\n",
      "Total combinations for category '생활 악취 관련': 55172162304000\n",
      "Processing category: 건설현장 관련\n",
      "Total combinations for category '건설현장 관련': 29070319200000\n",
      "Processing category: 쓰레기 관련\n",
      "Total combinations for category '쓰레기 관련': 34270482400000\n",
      "Processing category: 날씨 관련\n",
      "Total combinations for category '날씨 관련': 38071850400000\n",
      "Processing category: 공장 관련\n",
      "Total combinations for category '공장 관련': 96874008000000\n",
      "Processing category: 축산 관련\n",
      "Total combinations for category '축산 관련': 53765683200000\n",
      "Processing category: 생활 악취 관련\n",
      "Total combinations for category '생활 악취 관련': 55172162304000\n",
      "Processing category: 건설현장 관련\n",
      "Total combinations for category '건설현장 관련': 29070319200000\n",
      "Processing category: 쓰레기 관련\n",
      "Total combinations for category '쓰레기 관련': 34270482400000\n",
      "Processing category: 날씨 관련\n",
      "Total combinations for category '날씨 관련': 38071850400000\n",
      "Processing category: 공장 관련\n",
      "Total combinations for category '공장 관련': 96874008000000\n",
      "Processing category: 축산 관련\n",
      "Total combinations for category '축산 관련': 53765683200000\n",
      "Processing category: 생활 악취 관련\n",
      "Total combinations for category '생활 악취 관련': 55172162304000\n",
      "Processing category: 건설현장 관련\n",
      "Total combinations for category '건설현장 관련': 29070319200000\n"
     ]
    }
   ],
   "source": [
    "# 발생 장소 데이터 생성\n",
    "location_data = generate_task_data_with_max_samples(\"locations\", categories, 3000)\n",
    "\n",
    "# 냄새 종류 데이터 생성\n",
    "smell_type_data = generate_task_data_with_max_samples(\"smell_types\", categories, 300000)\n",
    "\n",
    "# 시간 데이터 생성\n",
    "time_data = generate_task_data_with_max_samples(\"time_of_days\", categories, 300000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋이 저장되었습니다: /home/yjtech/Desktop/LLM/Data/smell_keyword/location_data.json\n",
      "샘플 데이터: {'categories': '쓰레기 관련', 'text': '쓰레기 매립장에서 내년 밤 9시에 발생한 불쾌한 냄새 찌들게 불법 투기된 폐기물로 연락드렸습니다. 네, 민원센터입니다. 쓰레기 매립장에서 발생한 불쾌한 냄새 찌들게 문제에 대해 상담 드리겠습니다. 저희 쓰레기 매립장에서 내년 밤 9시에 발생한 불쾌한 냄새 찌들게 불법 투기된 폐기물로 인해 호흡기 문제가 발생 중입니다. 불편을 드려 대단히 죄송합니다. 호흡기 문제 해결을 위해 쓰레기 매립장에 대한 현장 점검을 실시하겠습니다. 잠시만 기다려 주시겠습니까? 네, 기다리겠습니다. 빨리 처리해 주세요. 네, 알겠습니다. 수고하세요. 저희가 도와드리겠습니다. 감사합니다.', 'keyword': '쓰레기 매립장'}\n",
      "데이터셋이 저장되었습니다: /home/yjtech/Desktop/LLM/Data/smell_keyword/smell_type_data.json\n",
      "샘플 데이터: {'categories': '쓰레기 관련', 'text': '소형 공장 단지 인근에서 오늘 오후 9시부터 10시에 발생한 악취 극도로 음식물 쓰레기 악취로 연락드렸습니다. 네, 민원센터입니다. 소형 공장 단지 인근에서 발생한 악취 극도로 문제에 대해 상담 드리겠습니다. 저희 소형 공장 단지 인근에서 오늘 오후 9시부터 10시에 악취이 극도로 나고 있습니다. 냄새로 인한 불쾌감 불편을 드려 대단히 죄송합니다. 냄새로 인한 불쾌감 해결을 위해 소형 공장 단지 인근에 대한 현장 점검을 실시하겠습니다. 잠시만 기다려 주시겠습니까? 알겠습니다. 처리 좀 해주세요. 네, 알겠습니다. 잘 부탁드립니다. 네, 처리 후 다시 연락드리겠습니다. 감사합니다.', 'keyword': '악취'}\n",
      "데이터셋이 저장되었습니다: /home/yjtech/Desktop/LLM/Data/smell_keyword/time_data.json\n",
      "샘플 데이터: {'categories': '쓰레기 관련', 'text': '대형 쇼핑센터에서 오늘에 발생한 악취 무겁게 느껴지는 도로에 쓰레기 던져짐로 연락드렸습니다. 네, 민원센터입니다. 대형 쇼핑센터에서 발생한 악취 무겁게 느껴지는 문제에 대해 상담 드리겠습니다. 대형 쇼핑센터에서 오늘에 발생한 악취 무겁게 느껴지는 도로에 쓰레기 던져짐로 인해 냄새로 인한 불쾌감로 불편함을 겪고 있습니다. 불편을 드려 대단히 죄송합니다. 냄새로 인한 불쾌감 해결을 위해 대형 쇼핑센터에 대한 현장 점검을 실시하겠습니다. 잠시만 기다려 주시겠습니까? 그럼 문제를 빨리 해결해 주세요. 알겠습니다. 최대한 빠르게 처리하겠습니다. 고맙습니다. 네, 처리 후 다시 연락드리겠습니다. 감사합니다.', 'keyword': '오늘'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def save_json(data, output_file):\n",
    "    final_output = {\n",
    "        \"data\": data\n",
    "    }\n",
    "\n",
    "    # JSON 저장\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_output, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"데이터셋이 저장되었습니다: {output_file}\")\n",
    "    print(f\"샘플 데이터: {data[0]}\")\n",
    "    \n",
    "save_json(location_data, '/home/yjtech/Desktop/LLM/Data/smell_keyword/location_data.json')\n",
    "save_json(smell_type_data, '/home/yjtech/Desktop/LLM/Data/smell_keyword/smell_type_data.json')\n",
    "save_json(time_data, '/home/yjtech/Desktop/LLM/Data/smell_keyword/time_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 600000 entries, 0 to 599999\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count   Dtype \n",
      "---  ------   --------------   ----- \n",
      " 0   text     600000 non-null  object\n",
      " 1   keyword  600000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 9.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    JSON 데이터를 불러와 Pandas DataFrame으로 변환\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        json_data = json.load(f)\n",
    "        \n",
    "    data = []\n",
    "    for item in json_data['data']:\n",
    "        if 'text' in item and 'keyword' in item:\n",
    "            data.append({\n",
    "                'text': item['text'],\n",
    "                'keyword': item['keyword']\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(df.info())\n",
    "    # None 값을 빈 문자열로 대체\n",
    "    df = df.fillna('')\n",
    "    return df\n",
    "\n",
    "\n",
    "# JSON 파일 경로\n",
    "file_path_location = \"/home/yjtech/Desktop/LLM/Data/smell_keyword/location_data.json\"\n",
    "location_df = load_data(file_path_location)\n",
    "\n",
    "file_path_smell = \"/home/yjtech/Desktop/LLM/Data/smell_keyword/smell_type_data.json\"\n",
    "smell_df = load_data(file_path_smell)\n",
    "\n",
    "file_path_time = \"/home/yjtech/Desktop/LLM/Data/smell_keyword/time_data.json\"\n",
    "time_df = load_data(file_path_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train, val 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location\n",
      "Train Data:\n",
      "                                                     text         keyword\n",
      "254099  안녕하세요. 군사 방어 시설 공사 현장에서 7월 17일 오전 10시부터 11시에 건...  군사 방어 시설 공사 현장\n",
      "53461   안녕하세요. 동해안 국립공원에서 12월 15일 저녁 8시에 맑고 차가운 산 공기 냄...        동해안 국립공원\n",
      "247984  안녕하세요. 최근 공공 체육관에서 1월 14일 오후 7시부터 8시에 발생한 찜찜한 ...          공공 체육관\n",
      "209067  안녕하세요. 어린이 놀이터 근처에서 12월 8일 새벽 5시에 발생한 오염된 배관 냄...      어린이 놀이터 근처\n",
      "185997  안녕하세요. 고지대 목장에서 15일 전 저녁 8시에 발생한 젖은 흙 냄새으로 생활에...          고지대 목장\n",
      "\n",
      "Validation Data:\n",
      "                                                     text      keyword\n",
      "4941    안녕하세요. 재활용 센터에서 4월 8일 오후 3시부터 4시에 녹조로 인한 썩은 냄새...       재활용 센터\n",
      "51775   포항 근처 산에서 7월 9일 밤에 햇볕에 말린 흙 냄새이 한꺼번에 확 퍼지는 나고 ...      포항 근처 산\n",
      "115253  여보세요? 농기계 제조 공장에서 9일 후 오전 9시부터 10시에 소음 공해로 인해 ...    농기계 제조 공장\n",
      "299321  안녕하세요. 댐 건설 현장에서 8월 16일 자정에 석탄 가루 냄새 관련하여 전화드렸...      댐 건설 현장\n",
      "173570  여보세요? 동물 백신 생산 공장에서 10월 13일 새벽 3시에 발생한 동물 학대 문...  동물 백신 생산 공장\n",
      "Train size: 240000, Validation size: 60000\n",
      "\n",
      "Smell\n",
      "Train Data:\n",
      "                                                     text      keyword\n",
      "254099  안녕하세요. 다리 건설 현장에서 2월 14일 새벽 4시에 공사 진행 중 환경 오염로...   플라스틱 용융 냄새\n",
      "53461   안녕하세요. 호미곶등대에서 28일 전 새벽 4시에 선선한 냄새 관련하여 전화드렸습니...       선선한 냄새\n",
      "247984  안녕하세요. 최근 지하 쇼핑센터 출입구에서 11월 5일 오후 6시부터 7시에 발생한...           악취\n",
      "209067  안녕하세요. 버스 정류장에서 9월 22일 밤 10시에 발생한 음식물 냄새으로 생활에...       음식물 냄새\n",
      "185997  안녕하세요. 호수 인근 축사에서 23일 전 오전 6시부터 7시에 발생한 분해 중인 ...  분해 중인 동물 냄새\n",
      "\n",
      "Validation Data:\n",
      "                                                     text    keyword\n",
      "4941    안녕하세요. 포항대학교에서 4월 7일 오후 6시부터 7시에 침투성 냄새 관련하여 전...     침투성 냄새\n",
      "51775   흥해읍에서 12월 6일 낮에 더운 날씨의 냄새이 어두운 나고 있어 문의 드립니다. ...  더운 날씨의 냄새\n",
      "115253  여보세요? 산업용 배터리 공장에서 9월 18일 오전 11시부터 12시에 가스 폭발 ...   기체 혼합 냄새\n",
      "299321  안녕하세요. 해안 방어벽 건설 현장에서 2년 후 오전 11시부터 12시에 고무 타는...   고무 타는 냄새\n",
      "173570  여보세요? 가축 폐기물 처리장에서 28일 전 오전 7시부터 8시에 발생한 악취 발생...   동물의 땀 냄새\n",
      "Train size: 240000, Validation size: 60000\n",
      "\n",
      "Time\n",
      "Train Data:\n",
      "                                                     text            keyword\n",
      "254099  안녕하세요. 초고층 타워 공사 현장에서 4월 1일 새벽 3시에 불완전한 작업 환경로...        4월 1일 새벽 3시\n",
      "53461   안녕하세요. 포스코에서 3월 12일 오후 5시부터 6시에 더운 날씨의 냄새 관련하여...  3월 12일 오후 5시부터 6시\n",
      "247984  안녕하세요. 최근 아파트 단지에서 3년 후 밤 10시에 발생한 수둇물 냄새 문제로 ...         3년 후 밤 10시\n",
      "209067  안녕하세요. 상가 근처에서 8월 24일 오전에 발생한 쓰레기 냄새으로 생활에 큰 불...          8월 24일 오전\n",
      "185997  안녕하세요. 바이오매스 에너지 목장에서 8월 22일에 발생한 새끼 동물 냄새으로 생...             8월 22일\n",
      "\n",
      "Validation Data:\n",
      "                                                     text             keyword\n",
      "4941    안녕하세요. 공동 주택 쓰레기장에서 4월 24일 오후 5시부터 6시에 썩은 물질 냄...   4월 24일 오후 5시부터 6시\n",
      "51775   포항 북부시장에서 1월 22일 밤 10시에 겨울 아침의 냄새이 불규칙하게 나고 있어...        1월 22일 밤 10시\n",
      "115253  여보세요? 고속 인쇄 공장에서 20일 후 자정에 화학 가스 누출로 인해 문의 드립니...            20일 후 자정\n",
      "299321  안녕하세요. 군사 기지 시설 공사 현장에서 11월 2일 오후 9시부터 10시에 방부...  11월 2일 오후 9시부터 10시\n",
      "173570  여보세요? 밀집형 가축 농장에서 5월 11일 오후에 발생한 사료 관리 문제로 연락드...           5월 11일 오후\n",
      "Train size: 240000, Validation size: 60000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터를 70:30 비율로 나누기\n",
    "def split_data(df, train_ratio = 0.8):\n",
    "    \"\"\"\n",
    "    DataFrame을 train과 val로 나눔\n",
    "    \"\"\"\n",
    "    train_df, val_df = train_test_split(df, train_size = train_ratio, random_state = 42, shuffle = True)\n",
    "    return train_df, val_df\n",
    "\n",
    "# train, val로 나누기\n",
    "train_location_df, val_location_df = split_data(location_df)\n",
    "train_smell_df, val_smell_df = split_data(smell_df)\n",
    "train_time_df, val_time_df = split_data(time_df)\n",
    "\n",
    "\n",
    "train_location_df.to_csv('/home/yjtech/Desktop/LLM/Data/smell_keyword/train_location_df.csv', index = False)\n",
    "val_location_df.to_csv('/home/yjtech/Desktop/LLM/Data/smell_keyword/val_location_df.csv', index = False)\n",
    "\n",
    "train_smell_df.to_csv('/home/yjtech/Desktop/LLM/Data/smell_keyword/train_smell_df.csv', index = False)\n",
    "val_smell_df.to_csv('/home/yjtech/Desktop/LLM/Data/smell_keyword/val_smell_df.csv', index = False)\n",
    "\n",
    "train_time_df.to_csv('/home/yjtech/Desktop/LLM/Data/smell_keyword/train_time_df.csv', index = False)\n",
    "val_time_df.to_csv('/home/yjtech/Desktop/LLM/Data/smell_keyword/val_time_df.csv', index = False)\n",
    "\n",
    "# 나눈 데이터 확인\n",
    "print(\"Location\")\n",
    "print(\"Train Data:\")\n",
    "print(train_location_df.head())\n",
    "print(\"\\nValidation Data:\")\n",
    "print(val_location_df.head())\n",
    "\n",
    "# 데이터 크기 확인\n",
    "print(f\"Train size: {len(train_location_df)}, Validation size: {len(val_location_df)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "print('Smell')\n",
    "print(\"Train Data:\")\n",
    "print(train_smell_df.head())\n",
    "print(\"\\nValidation Data:\")\n",
    "print(val_smell_df.head())\n",
    "\n",
    "# 데이터 크기 확인\n",
    "print(f\"Train size: {len(train_smell_df)}, Validation size: {len(val_smell_df)}\")\n",
    "\n",
    "print()\n",
    "print('Time')\n",
    "print(\"Train Data:\")\n",
    "print(train_time_df.head())\n",
    "print(\"\\nValidation Data:\")\n",
    "print(val_time_df.head())\n",
    "\n",
    "# 데이터 크기 확인\n",
    "print(f\"Train size: {len(train_time_df)}, Validation size: {len(val_time_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>254099</th>\n",
       "      <td>안녕하세요. 초고층 타워 공사 현장에서 4월 1일 새벽 3시에 불완전한 작업 환경로...</td>\n",
       "      <td>4월 1일 새벽 3시</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53461</th>\n",
       "      <td>안녕하세요. 포스코에서 3월 12일 오후 5시부터 6시에 더운 날씨의 냄새 관련하여...</td>\n",
       "      <td>3월 12일 오후 5시부터 6시</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247984</th>\n",
       "      <td>안녕하세요. 최근 아파트 단지에서 3년 후 밤 10시에 발생한 수둇물 냄새 문제로 ...</td>\n",
       "      <td>3년 후 밤 10시</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209067</th>\n",
       "      <td>안녕하세요. 상가 근처에서 8월 24일 오전에 발생한 쓰레기 냄새으로 생활에 큰 불...</td>\n",
       "      <td>8월 24일 오전</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185997</th>\n",
       "      <td>안녕하세요. 바이오매스 에너지 목장에서 8월 22일에 발생한 새끼 동물 냄새으로 생...</td>\n",
       "      <td>8월 22일</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119879</th>\n",
       "      <td>안녕하세요. 고압 가스 제조 공장에서 17일 후 저녁 6시에 소음 공해로 인해 피해...</td>\n",
       "      <td>17일 후 저녁 6시</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259178</th>\n",
       "      <td>여보세요? 도시 미관 개선 공사 현장에서 8월 27일 오전에 발생한 건설 근로자의 ...</td>\n",
       "      <td>8월 27일 오전</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131932</th>\n",
       "      <td>저희 초저온 냉각 장치 제조 공장에서 매운 냄새 문제로 불편을 겪고 있습니다. 3월...</td>\n",
       "      <td>3월 10일 밤 11시</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146867</th>\n",
       "      <td>안녕하세요. 철도 차량 제조 공장에서 2월 3일 오후 2시부터 3시에 발생한 강한 ...</td>\n",
       "      <td>2월 3일 오후 2시부터 3시</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121958</th>\n",
       "      <td>여보세요? 고속 인쇄 공장에서 10월 6일 오후 11시부터 12시에 발생한 화학물질...</td>\n",
       "      <td>10월 6일 오후 11시부터 12시</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text              keyword\n",
       "254099  안녕하세요. 초고층 타워 공사 현장에서 4월 1일 새벽 3시에 불완전한 작업 환경로...          4월 1일 새벽 3시\n",
       "53461   안녕하세요. 포스코에서 3월 12일 오후 5시부터 6시에 더운 날씨의 냄새 관련하여...    3월 12일 오후 5시부터 6시\n",
       "247984  안녕하세요. 최근 아파트 단지에서 3년 후 밤 10시에 발생한 수둇물 냄새 문제로 ...           3년 후 밤 10시\n",
       "209067  안녕하세요. 상가 근처에서 8월 24일 오전에 발생한 쓰레기 냄새으로 생활에 큰 불...            8월 24일 오전\n",
       "185997  안녕하세요. 바이오매스 에너지 목장에서 8월 22일에 발생한 새끼 동물 냄새으로 생...               8월 22일\n",
       "...                                                   ...                  ...\n",
       "119879  안녕하세요. 고압 가스 제조 공장에서 17일 후 저녁 6시에 소음 공해로 인해 피해...          17일 후 저녁 6시\n",
       "259178  여보세요? 도시 미관 개선 공사 현장에서 8월 27일 오전에 발생한 건설 근로자의 ...            8월 27일 오전\n",
       "131932  저희 초저온 냉각 장치 제조 공장에서 매운 냄새 문제로 불편을 겪고 있습니다. 3월...         3월 10일 밤 11시\n",
       "146867  안녕하세요. 철도 차량 제조 공장에서 2월 3일 오후 2시부터 3시에 발생한 강한 ...     2월 3일 오후 2시부터 3시\n",
       "121958  여보세요? 고속 인쇄 공장에서 10월 6일 오후 11시부터 12시에 발생한 화학물질...  10월 6일 오후 11시부터 12시\n",
       "\n",
       "[240000 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4월 1일 새벽 3시', '3월 12일 오후 5시부터 6시', '3년 후 밤 10시', '8월 24일 오전', '8월 22일']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_time_df.iloc[0:5]['keyword'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def safe_convert_to_list(value):\n",
    "#     try:\n",
    "#         # JSON 형식으로 변환 가능한지 확인\n",
    "#         return json.loads(value) if isinstance(value, str) else value\n",
    "#     except json.JSONDecodeError:\n",
    "#         # 변환이 불가능한 경우 그대로 반환\n",
    "#         return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "train_location_df = pd.read_csv('/home/yjtech/Desktop/LLM/Data/smell_keyword/train_location_df.csv')\n",
    "val_location_df = pd.read_csv('/home/yjtech/Desktop/LLM/Data/smell_keyword/val_location_df.csv')\n",
    "\n",
    "\n",
    "# 데이터셋 전체 변환\n",
    "# if train_location_df['keyword'].dtype == 'object':  # keyword가 문자열인지 확인\n",
    "#     train_location_df['keyword'] = train_location_df['keyword'].apply(safe_convert_to_list)  # 또는 json.loads\n",
    "\n",
    "# # val_df의 'keyword' 컬럼 문자열 -> 리스트 변환\n",
    "# if val_location_df['keyword'].dtype == 'object':  # keyword가 문자열인지 확인\n",
    "#     val_location_df['keyword'] = val_location_df['keyword'].apply(safe_convert_to_list)  # 또는 json.loads\n",
    "\n",
    "train_location_data_dict = train_location_df.to_dict(orient='list')\n",
    "location_train_dataset = Dataset.from_dict(train_location_data_dict)\n",
    "location_train_dataset\n",
    "print(len(location_train_dataset))\n",
    "\n",
    "val_location_data_dict = val_location_df.to_dict(orient='list')\n",
    "location_val_dataset = Dataset.from_dict(val_location_data_dict)\n",
    "print(len(location_val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요. 재활용 센터에서 4월 8일 오후 3시부터 4시에 녹조로 인한 썩은 냄새 관련하여 전화드렸습니다. 안녕하세요. 재활용 센터의 폐기물 분석팀입니다. 4월 8일 오후 3시부터 4시에 발생한 음식물 쓰레기 악취와 관련된 상담이신가요? 재활용 센터에서 4월 8일 오후 3시부터 4시에 발생한 음식물 쓰레기 악취로 인해 호흡기 문제로 생활에 어려움을 겪고 있습니다. 불편을 드려 대단히 죄송합니다. 문제를 해결하기 위해 재활용 센터을 점검하겠습니다. 그럼 신속히 해결해주세요. 확인해 보고 빠르게 처리하겠습니다. 고맙습니다. 감사합니다. 빠르게 처리하겠습니다.\n",
      "재활용 센터\n"
     ]
    }
   ],
   "source": [
    "print(location_val_dataset[0]['text'])\n",
    "print(location_val_dataset[0]['keyword'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "smell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480000\n",
      "120000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "train_smell_df = pd.read_csv('/home/yjtech/Desktop/LLM/Data/smell_keyword/train_smell_df.csv')\n",
    "val_smell_df = pd.read_csv('/home/yjtech/Desktop/LLM/Data/smell_keyword/val_smell_df.csv')\n",
    "\n",
    "\n",
    "# # 데이터셋 전체 변환\n",
    "# if train_smell_df['keyword'].dtype == 'object':  # keyword가 문자열인지 확인\n",
    "#     train_smell_df['keyword'] = train_smell_df['keyword'].apply(safe_convert_to_list)  # 또는 json.loads\n",
    "\n",
    "# # val_df의 'keyword' 컬럼 문자열 -> 리스트 변환\n",
    "# if val_smell_df['keyword'].dtype == 'object':  # keyword가 문자열인지 확인\n",
    "#     val_smell_df['keyword'] = val_smell_df['keyword'].apply(safe_convert_to_list)  # 또는 json.loads\n",
    "\n",
    "train_smell_data_dict = train_smell_df.to_dict(orient='list')\n",
    "smell_train_dataset = Dataset.from_dict(train_smell_data_dict)\n",
    "smell_train_dataset\n",
    "print(len(smell_train_dataset))\n",
    "\n",
    "val_smell_data_dict = val_smell_df.to_dict(orient='list')\n",
    "smell_val_dataset = Dataset.from_dict(val_smell_data_dict)\n",
    "print(len(smell_val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4941</th>\n",
       "      <td>안녕하세요. 공동 주택 쓰레기장에서 4월 24일 오후 5시부터 6시에 썩은 물질 냄...</td>\n",
       "      <td>4월 24일 오후 5시부터 6시</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51775</th>\n",
       "      <td>포항 북부시장에서 1월 22일 밤 10시에 겨울 아침의 냄새이 불규칙하게 나고 있어...</td>\n",
       "      <td>1월 22일 밤 10시</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115253</th>\n",
       "      <td>여보세요? 고속 인쇄 공장에서 20일 후 자정에 화학 가스 누출로 인해 문의 드립니...</td>\n",
       "      <td>20일 후 자정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299321</th>\n",
       "      <td>안녕하세요. 군사 기지 시설 공사 현장에서 11월 2일 오후 9시부터 10시에 방부...</td>\n",
       "      <td>11월 2일 오후 9시부터 10시</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173570</th>\n",
       "      <td>여보세요? 밀집형 가축 농장에서 5월 11일 오후에 발생한 사료 관리 문제로 연락드...</td>\n",
       "      <td>5월 11일 오후</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75094</th>\n",
       "      <td>안녕하세요. 최근 호미곶 바람의 언덕에서 4월 28일 새벽 1시에 발생한 산성 냄새...</td>\n",
       "      <td>4월 28일 새벽 1시</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171847</th>\n",
       "      <td>안녕하세요. 수산물 가공 축사에서 1년 후 오후 7시부터 8시에 발생한 퇴비 냄새으...</td>\n",
       "      <td>1년 후 오후 7시부터 8시</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138313</th>\n",
       "      <td>여보세요? 비료 공장에서 9월 12일 밤 9시에 기계 고장로 인해 문의 드립니다. ...</td>\n",
       "      <td>9월 12일 밤 9시</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271268</th>\n",
       "      <td>여보세요? 도시 외곽 고속도로 건설 현장에서 12월 20일 새벽 2시에 발생한 지속...</td>\n",
       "      <td>12월 20일 새벽 2시</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72612</th>\n",
       "      <td>저희 포항 영일대 광장에서 밤 공기 냄새 문제로 불편을 겪고 있습니다. 6월 10일...</td>\n",
       "      <td>6월 10일 오전 9시부터 10시</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text             keyword\n",
       "4941    안녕하세요. 공동 주택 쓰레기장에서 4월 24일 오후 5시부터 6시에 썩은 물질 냄...   4월 24일 오후 5시부터 6시\n",
       "51775   포항 북부시장에서 1월 22일 밤 10시에 겨울 아침의 냄새이 불규칙하게 나고 있어...        1월 22일 밤 10시\n",
       "115253  여보세요? 고속 인쇄 공장에서 20일 후 자정에 화학 가스 누출로 인해 문의 드립니...            20일 후 자정\n",
       "299321  안녕하세요. 군사 기지 시설 공사 현장에서 11월 2일 오후 9시부터 10시에 방부...  11월 2일 오후 9시부터 10시\n",
       "173570  여보세요? 밀집형 가축 농장에서 5월 11일 오후에 발생한 사료 관리 문제로 연락드...           5월 11일 오후\n",
       "...                                                   ...                 ...\n",
       "75094   안녕하세요. 최근 호미곶 바람의 언덕에서 4월 28일 새벽 1시에 발생한 산성 냄새...        4월 28일 새벽 1시\n",
       "171847  안녕하세요. 수산물 가공 축사에서 1년 후 오후 7시부터 8시에 발생한 퇴비 냄새으...     1년 후 오후 7시부터 8시\n",
       "138313  여보세요? 비료 공장에서 9월 12일 밤 9시에 기계 고장로 인해 문의 드립니다. ...         9월 12일 밤 9시\n",
       "271268  여보세요? 도시 외곽 고속도로 건설 현장에서 12월 20일 새벽 2시에 발생한 지속...       12월 20일 새벽 2시\n",
       "72612   저희 포항 영일대 광장에서 밤 공기 냄새 문제로 불편을 겪고 있습니다. 6월 10일...  6월 10일 오전 9시부터 10시\n",
       "\n",
       "[60000 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smell Types Distribution: Counter({'타는 냄새': 3300, '불쾌한 냄새': 3283, '가스 냄새': 3273, '악취': 2635, '산성 냄새': 2592, '기름 냄새': 2291, '화학 냄새': 1913, '과일 썩은 냄새': 1785, '퇴비 냄새': 1781, '쓰레기 냄새': 1770, '고약한 냄새': 1697, '탄 냄새': 1684, '자극적인 냄새': 1677, '음식물 냄새': 1643, '소독약 냄새': 1640, '찌든 냄새': 1597, '날카로운 냄새': 1546, '비린내': 1488, '오염된 물 냄새': 1456, '방부제 냄새': 1416, '고무 타는 냄새': 1376, '상쾌한 냄새': 1100, '장마철 냄새': 1065, '얼음이 녹은 냄새': 1061, '바람에 실린 꽃 향기': 1050, '습한 냄새': 1049, '비 내리는 도시 냄새': 1046, '부패한 냄새': 1039, '비 온 후 냄새': 1039, '포근한 바람 냄새': 1035, '침투성 냄새': 1034, '비 오는 날 냄새': 1032, '찌든 유기물 냄새': 1032, '더운 날씨의 냄새': 1030, '잿더미 냄새': 1029, '진한 해풍 냄새': 1022, '부패된 해산물 냄새': 1020, '냄새가 심한 재활용 쓰레기 냄새': 1019, '썩은 물질 냄새': 1017, '먼지 냄새': 1016, '겨울 아침의 냄새': 1016, '섞인 음식물 냄새': 1012, '습기 섞인 냄새': 1012, '부패된 고기 냄새': 1011, '강풍에 섞인 바다 냄새': 1011, '압축된 쓰레기 냄새': 1011, '해돋이 전 공기 냄새': 1011, '선선한 냄새': 1010, '염분이 섞인 냄새': 1010, '산불 연기 냄새': 1009, '지속적으로 나는 냄새': 1009, '건조한 먼지 냄새': 1007, '오염된 냄새': 1006, '담배 꽁초 냄새': 1005, '햇볕에 말린 흙 냄새': 1001, '아스팔트 타는 냄새': 1001, '우박 냄새': 1001, '짙은 미세먼지': 996, '더러운 냄새': 996, '안개 속의 축축한 냄새': 994, '플라스틱 태운 냄새': 993, '강한 페인트 잔여물 냄새': 991, '맑은 날 공기 냄새': 991, '소나기 후 냄새': 991, '녹조로 인한 썩은 냄새': 990, '자연의 냄새': 990, '소각장 배출물 냄새': 990, '젖은 쓰레기 냄새': 988, '오래된 음료수 잔여물 냄새': 985, '메스꺼운 냄새': 984, '차가운 이슬 냄새': 982, '녹슨 금속 냄새': 980, '물비린내': 980, '폭발적 악취': 975, '구름 냄새': 972, '밤 공기 냄새': 970, '축축한 낙엽 냄새': 969, '화학 용제 냄새': 969, '황사 냄새': 966, '맑고 차가운 산 공기 냄새': 962, '뿌연 공기': 960, '낡은 가구 냄새': 959, '폭우 후의 땅 냄새': 958, '서리 낀 풀 냄새': 953, '산골짜기 물 냄새': 944, '폭염 후의 공기 냄새': 936, '미세먼지 냄새': 933, '곰팡이가 핀 음식 냄새': 928, '스프레이 냄새': 905, '불타는 고기 냄새': 904, '축축한 흙 냄새': 903, '비린 고기 냄새': 890, '고기 썩은 냄새': 890, '젖은 카펫 냄새': 889, '쓰레기통 냄새': 887, '동물 배설물 냄새': 885, '화재 잔여물 냄새': 884, '강한 세제 냄새': 883, '사람 땀 냄새': 875, '하수구 냄새': 873, '아연 코팅 냄새': 872, '배수구 냄새': 870, '계란 썩은 냄새': 870, '수둇물 냄새': 870, '양념된 음식 냄새': 868, '동물 냄새': 867, '담배 냄새': 866, '오염된 배관 냄새': 865, '밀폐된 공간 냄새': 863, '오래된 냄새': 863, '조리 후 남은 냄새': 860, '고온 처리 후 냄새': 857, '낡은 나무 냄새': 857, '썩은 음식 냄새': 855, '곰팡이 냄새': 854, '가축 운송 트럭 냄새': 852, '화장실 냄새': 850, '솔벤트 냄새': 850, '산성 증기 냄새': 849, '이끼 냄새': 848, '비산 먼지 냄새': 846, '강한 냄새': 844, '공장 배출 가스 냄새': 844, '단백질 냄새': 843, '침전된 물질 냄새': 841, '음악 센터 냄새': 839, '농축된 암모니아 냄새': 839, '냉동고 냄새': 839, '석회 냄새': 838, '새우 배설물 냄새': 837, '시멘트 분말 냄새': 834, '시멘트 냄새': 834, '분해 중인 동물 냄새': 833, '시큼한 냄새': 832, '기계 오일 냄새': 827, '건식 재료 냄새': 827, '비료 제조 과정 냄새': 826, '녹슨 철 냄새': 825, '가축 장비에서 나는 냄새': 824, '절단된 금속 냄새': 821, '동물 사료 냄새': 821, '합성수지 냄새': 820, '기계 연료 냄새': 820, '썩은 풀 냄새': 818, '유해 화학물질 냄새': 818, '섬유 강화 플라스틱 냄새': 818, '건설 자재 냄새': 816, '폴리우레탄 냄새': 815, '양 사육장 냄새': 815, '목재 가공 냄새': 812, '조합 재료 냄새': 811, '비린 냄새': 811, '섬유 화학 냄새': 810, '축산물 가공 냄새': 810, '연소된 폐기물 냄새': 810, '찜찜한 냄새': 810, '발효된 퇴비 냄새': 809, '석탄 가루 냄새': 809, '방사된 연기 냄새': 809, '기타 건축 폐기물 냄새': 806, '가축 사료 냄새': 806, '주조 공정 냄새': 806, '고기 굽는 냄새': 806, '석회석 냄새': 805, '가스 누출 냄새': 802, '아스팔트 냄새': 802, '파쇄된 돌 냄새': 799, '열 처리 냄새': 798, '폐수 처리장 냄새': 797, '방수 코팅 냄새': 794, '돼지 똥 냄새': 793, '축산 폐기물 냄새': 792, '연소된 건초 냄새': 790, '건초 저장고 냄새': 790, '금속 냄새': 789, '동물 사체 냄새': 789, '우사 내부 냄새': 788, '플라스틱 용융 냄새': 787, '유리 및 플라스틱 재료 냄새': 787, '젖은 동물 냄새': 786, '동물의 땀 냄새': 786, '새끼 동물 냄새': 786, '육류 냄새': 783, '유기물 부패 냄새': 782, '동물 방목지 냄새': 782, '부패된 알 냄새': 781, '소 똥 냄새': 781, '부패된 잔해 냄새': 780, '붕괴된 건축물 냄새': 779, '부패된 사료 냄새': 779, '폐기된 자재 냄새': 777, '오염된 하수 냄새': 776, '방열재 냄새': 775, '분뇨 냄새': 773, '접착제 냄새': 773, '화학 물질 냄새': 772, '젖소 목장 냄새': 772, '도료 혼합 냄새': 771, '페인트 냄새': 769, '청소 화학 냄새': 769, '천장 단열재 냄새': 766, '연마 공정 냄새': 765, '발효된 냄새': 764, '배관 설치 냄새': 760, '축사 배기구 냄새': 756, '젖은 흙 냄새': 755, '소화불량 냄새': 749, '석유 냄새': 746, '저장된 배설물 냄새': 745, '파라핀 냄새': 681, '소각된 폐기물 냄새': 668, '매운 냄새': 659, '고체 연료 연소 냄새': 656, '플라스틱 녹는 냄새': 656, '지속적인 냄새': 655, '찌르는 냄새': 646, '고압 공정 냄새': 645, '산화된 금속 냄새': 643, '화학약품 냄새': 641, '합성 가스 냄새': 640, '강렬한 탄화 냄새': 637, '오존 냄새': 633, '촉매 작용 중 발생한 냄새': 630, '아크 용접 냄새': 629, '유기물 냄새': 628, '공기 중의 찌든 냄새': 626, '기체 혼합 냄새': 625, '공업용 윤활유 냄새': 624, '질소 화합물 냄새': 624, '독성 화학물 냄새': 622, '부패 냄새': 621, '공업용 에폭시 냄새': 619, '미묘한 가스 냄새': 619, '중화제 냄새': 618, '연소 냄새': 613, '고체 화학물 분해 냄새': 612, '유황 냄새': 610, '오염된 공기 냄새': 609, '분말 금속 냄새': 607, '폐수 증발 냄새': 606, '연마재 냄새': 605, '화학 반응 부산물 냄새': 605, '강한 접착제 냄새': 603, '부식성 기체 냄새': 602, '페인트 희석제 냄새': 601, '폴리머 처리 냄새': 601, '고온에서 가열된 화학물 냄새': 601, '미세 먼지 냄새': 597, '금속성 냄새': 596, '화학 반응 중 발생한 냄새': 595, '전기적 소각 냄새': 594, '폭발적인 악취': 593, '혼합된 화학 냄새': 589, '고온 증기 냄새': 587, '고온 금속 냄새': 585, '염료 냄새': 583, '분말 화학 냄새': 582, '소성된 플라스틱 냄새': 581, '연료 누출 냄새': 578, '매캐한 냄새': 574, '부식 냄새': 557})\n",
      "Smell Types Distribution: Counter({'가스 냄새': 826, '불쾌한 냄새': 805, '타는 냄새': 783, '악취': 660, '산성 냄새': 650, '기름 냄새': 595, '화학 냄새': 502, '과일 썩은 냄새': 491, '음식물 냄새': 468, '퇴비 냄새': 467, '자극적인 냄새': 440, '쓰레기 냄새': 431, '고약한 냄새': 420, '탄 냄새': 417, '찌든 냄새': 416, '소독약 냄새': 401, '오염된 물 냄새': 399, '방부제 냄새': 394, '날카로운 냄새': 385, '비린내': 361, '고무 타는 냄새': 339, '젖은 쓰레기 냄새': 287, '비 온 후 냄새': 274, '부패된 해산물 냄새': 274, '더러운 냄새': 271, '우박 냄새': 268, '녹조로 인한 썩은 냄새': 268, '차가운 이슬 냄새': 268, '부패된 고기 냄새': 267, '진한 해풍 냄새': 264, '햇볕에 말린 흙 냄새': 262, '폭우 후의 땅 냄새': 262, '오래된 음료수 잔여물 냄새': 262, '침투성 냄새': 261, '물비린내': 261, '압축된 쓰레기 냄새': 260, '메스꺼운 냄새': 260, '염분이 섞인 냄새': 259, '산불 연기 냄새': 259, '산골짜기 물 냄새': 258, '냄새가 심한 재활용 쓰레기 냄새': 258, '담배 꽁초 냄새': 258, '강한 페인트 잔여물 냄새': 258, '섞인 음식물 냄새': 257, '부패한 냄새': 254, '맑고 차가운 산 공기 냄새': 254, '더운 날씨의 냄새': 253, '지속적으로 나는 냄새': 253, '자연의 냄새': 253, '뿌연 공기': 253, '얼음이 녹은 냄새': 252, '축축한 낙엽 냄새': 250, '장마철 냄새': 249, '아스팔트 타는 냄새': 249, '소각장 배출물 냄새': 248, '먼지 냄새': 248, '황사 냄새': 247, '습기 섞인 냄새': 246, '강풍에 섞인 바다 냄새': 246, '미세먼지 냄새': 246, '비 오는 날 냄새': 246, '상쾌한 냄새': 245, '폭염 후의 공기 냄새': 244, '동물 냄새': 243, '비 내리는 도시 냄새': 243, '맑은 날 공기 냄새': 243, '계란 썩은 냄새': 242, '바람에 실린 꽃 향기': 242, '습한 냄새': 242, '겨울 아침의 냄새': 242, '축축한 흙 냄새': 241, '찌든 유기물 냄새': 241, '포근한 바람 냄새': 241, '금속 냄새': 239, '건조한 먼지 냄새': 239, '서리 낀 풀 냄새': 238, '고온 처리 후 냄새': 237, '화학 용제 냄새': 236, '짙은 미세먼지': 236, '구름 냄새': 234, '오래된 냄새': 234, '낡은 가구 냄새': 233, '안개 속의 축축한 냄새': 233, '폭발적 악취': 233, '썩은 물질 냄새': 233, '선선한 냄새': 232, '소나기 후 냄새': 232, '찜찜한 냄새': 231, '밤 공기 냄새': 230, '사람 땀 냄새': 230, '축산물 가공 냄새': 230, '낡은 나무 냄새': 230, '플라스틱 태운 냄새': 229, '화장실 냄새': 229, '건설 자재 냄새': 228, '고기 썩은 냄새': 228, '이끼 냄새': 227, '오염된 냄새': 226, '해돋이 전 공기 냄새': 226, '밀폐된 공간 냄새': 226, '녹슨 금속 냄새': 223, '잿더미 냄새': 223, '강한 세제 냄새': 223, '천장 단열재 냄새': 222, '썩은 음식 냄새': 222, '섬유 화학 냄새': 222, '오염된 배관 냄새': 221, '접착제 냄새': 220, '녹슨 철 냄새': 220, '합성수지 냄새': 220, '배수구 냄새': 218, '부패된 사료 냄새': 218, '페인트 냄새': 218, '공장 배출 가스 냄새': 217, '폐기된 자재 냄새': 217, '방수 코팅 냄새': 217, '곰팡이 냄새': 216, '연소된 건초 냄새': 216, '폐수 처리장 냄새': 216, '가축 사료 냄새': 216, '연소된 폐기물 냄새': 215, '기타 건축 폐기물 냄새': 213, '화재 잔여물 냄새': 213, '하수구 냄새': 212, '건초 저장고 냄새': 212, '양 사육장 냄새': 212, '가축 운송 트럭 냄새': 211, '스프레이 냄새': 210, '동물 배설물 냄새': 210, '새끼 동물 냄새': 210, '동물의 땀 냄새': 209, '비린 고기 냄새': 208, '쓰레기통 냄새': 208, '농축된 암모니아 냄새': 208, '강한 냄새': 208, '분해 중인 동물 냄새': 207, '동물 사료 냄새': 207, '돼지 똥 냄새': 207, '곰팡이가 핀 음식 냄새': 207, '수둇물 냄새': 206, '가축 장비에서 나는 냄새': 206, '조리 후 남은 냄새': 206, '담배 냄새': 206, '젖은 카펫 냄새': 205, '절단된 금속 냄새': 204, '새우 배설물 냄새': 204, '냉동고 냄새': 204, '젖은 동물 냄새': 204, '아스팔트 냄새': 203, '플라스틱 용융 냄새': 203, '붕괴된 건축물 냄새': 203, '음악 센터 냄새': 203, '발효된 퇴비 냄새': 202, '열 처리 냄새': 202, '양념된 음식 냄새': 202, '썩은 풀 냄새': 202, '석탄 가루 냄새': 201, '섬유 강화 플라스틱 냄새': 201, '축산 폐기물 냄새': 201, '비료 제조 과정 냄새': 201, '석회석 냄새': 201, '저장된 배설물 냄새': 200, '청소 화학 냄새': 200, '불타는 고기 냄새': 199, '육류 냄새': 199, '젖은 흙 냄새': 199, '가스 누출 냄새': 198, '분뇨 냄새': 198, '폴리우레탄 냄새': 198, '침전된 물질 냄새': 197, '소화불량 냄새': 197, '젖소 목장 냄새': 197, '시큼한 냄새': 197, '목재 가공 냄새': 197, '축사 배기구 냄새': 196, '우사 내부 냄새': 195, '단백질 냄새': 195, '동물 방목지 냄새': 194, '비린 냄새': 193, '솔벤트 냄새': 193, '방열재 냄새': 193, '방사된 연기 냄새': 192, '석회 냄새': 192, '도료 혼합 냄새': 191, '동물 사체 냄새': 191, '아연 코팅 냄새': 190, '부패된 알 냄새': 189, '고기 굽는 냄새': 189, '연마 공정 냄새': 188, '조합 재료 냄새': 188, '기계 연료 냄새': 187, '소 똥 냄새': 186, '건식 재료 냄새': 186, '파쇄된 돌 냄새': 186, '화학 물질 냄새': 185, '발효된 냄새': 185, '배관 설치 냄새': 185, '산성 증기 냄새': 184, '유해 화학물질 냄새': 184, '유리 및 플라스틱 재료 냄새': 184, '비산 먼지 냄새': 183, '기계 오일 냄새': 182, '부패 냄새': 182, '시멘트 냄새': 182, '오염된 하수 냄새': 181, '시멘트 분말 냄새': 178, '석유 냄새': 178, '유기물 부패 냄새': 178, '주조 공정 냄새': 172, '부패된 잔해 냄새': 172, '공기 중의 찌든 냄새': 170, '독성 화학물 냄새': 168, '폴리머 처리 냄새': 168, '소성된 플라스틱 냄새': 167, '미세 먼지 냄새': 166, '혼합된 화학 냄새': 165, '염료 냄새': 165, '고온에서 가열된 화학물 냄새': 163, '연료 누출 냄새': 162, '오염된 공기 냄새': 162, '매캐한 냄새': 161, '강한 접착제 냄새': 161, '찌르는 냄새': 160, '미묘한 가스 냄새': 159, '공업용 에폭시 냄새': 158, '공업용 윤활유 냄새': 158, '화학약품 냄새': 157, '고체 화학물 분해 냄새': 157, '고압 공정 냄새': 156, '페인트 희석제 냄새': 156, '분말 화학 냄새': 156, '화학 반응 부산물 냄새': 156, '분말 금속 냄새': 155, '강렬한 탄화 냄새': 155, '파라핀 냄새': 154, '질소 화합물 냄새': 153, '금속성 냄새': 152, '기체 혼합 냄새': 151, '지속적인 냄새': 151, '합성 가스 냄새': 151, '전기적 소각 냄새': 151, '폐수 증발 냄새': 151, '산화된 금속 냄새': 150, '고체 연료 연소 냄새': 150, '연마재 냄새': 146, '중화제 냄새': 145, '플라스틱 녹는 냄새': 145, '매운 냄새': 145, '폭발적인 악취': 144, '고온 증기 냄새': 143, '아크 용접 냄새': 142, '오존 냄새': 142, '소각된 폐기물 냄새': 141, '유황 냄새': 140, '연소 냄새': 140, '부식 냄새': 139, '고온 금속 냄새': 137, '유기물 냄새': 135, '촉매 작용 중 발생한 냄새': 131, '부식성 기체 냄새': 128, '화학 반응 중 발생한 냄새': 114})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "smell_type_counts1 = Counter([data['keyword'] for data in smell_train_dataset])\n",
    "smell_type_counts2 = Counter([data['keyword'] for data in smell_val_dataset])\n",
    "\n",
    "print(\"Smell Types Distribution:\", smell_type_counts1)\n",
    "print(\"Smell Types Distribution:\", smell_type_counts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261\n"
     ]
    }
   ],
   "source": [
    "print(len(smell_type_counts1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "train_time_df = pd.read_csv('/home/yjtech/Desktop/LLM/Data/smell_keyword/train_time_df.csv')\n",
    "val_time_df = pd.read_csv('/home/yjtech/Desktop/LLM/Data/smell_keyword/val_time_df.csv')\n",
    "\n",
    "\n",
    "# # 데이터셋 전체 변환\n",
    "# if train_time_df['keyword'].dtype == 'object':  # keyword가 문자열인지 확인\n",
    "#     train_time_df['keyword'] = train_time_df['keyword'].apply(safe_convert_to_list)  # 또는 json.loads\n",
    "\n",
    "# # val_df의 'keyword' 컬럼 문자열 -> 리스트 변환\n",
    "# if val_time_df['keyword'].dtype == 'object':  # keyword가 문자열인지 확인\n",
    "#     val_time_df['keyword'] = val_time_df['keyword'].apply(safe_convert_to_list)  # 또는 json.loads\n",
    "\n",
    "train_time_data_dict = train_time_df.to_dict(orient='list')\n",
    "time_train_dataset = Dataset.from_dict(train_time_data_dict)\n",
    "time_train_dataset\n",
    "print(len(time_train_dataset))\n",
    "\n",
    "val_time_data_dict = val_time_df.to_dict(orient='list')\n",
    "time_val_dataset = Dataset.from_dict(val_time_data_dict)\n",
    "print(len(time_val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 18:44:06.470816: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-03 18:44:06.495683: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 18:44:06.865649: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from typing import Dict\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AdamW,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from mecab import MeCab\n",
    "\n",
    "class CustomKeyBERTTrainer:\n",
    "    def __init__(self, model_name: str, save_dir: str, **kwargs):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        if self.device == \"cuda\":\n",
    "            print(f\"GPU Model: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=kwargs.get(\"learning_rate\", 2e-5))\n",
    "        self.max_length = kwargs.get(\"max_length\", 128)\n",
    "        self.training_args = kwargs\n",
    "        self.save_dir = save_dir\n",
    "        self.mecab = MeCab()  # MeCab 초기화\n",
    "        self.best_model_path = os.path.join(self.save_dir, \"pytorch_model.bin\")\n",
    "        self.tokenizer_path = self.save_dir\n",
    "        \n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'epoch_times': [],\n",
    "            'best_epoch': 0\n",
    "        }\n",
    "\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        \n",
    "    def _mecab_tokenize(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        MeCab을 사용하여 입력 텍스트에서 명사, 형용사, 부사를 추출\n",
    "        \"\"\"\n",
    "        pos_tags = self.mecab.pos(text)  # 품사 태그 추출\n",
    "        # 원하는 품사 필터링: 명사(NNG, NNP), 형용사(VA), 부사(MAG, MAJ)\n",
    "        keywords = [\n",
    "            word for word, pos in pos_tags if pos in (\"NNG\", \"NNP\", \"VA\", \"MAG\", \"MAJ\")\n",
    "        ]\n",
    "        return \" \".join(keywords)  # 추출한 단어를 공백으로 연결하여 반환\n",
    "    \n",
    "    def preprocess_data(self, examples: Dict) -> Dict:\n",
    "        # MeCab 형태소 분석과 품사 필터링 적용\n",
    "        examples[\"text\"] = [self._mecab_tokenize(text) for text in examples[\"text\"]]\n",
    "        \n",
    "        # 기존 코드 유지\n",
    "        inputs = [f\"키워드 추출: {text}\" for text in examples[\"text\"]]\n",
    "        model_inputs = self.tokenizer(\n",
    "            inputs,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=None  # 텐서 변환을 DataCollator에 맡김\n",
    "        )\n",
    "\n",
    "        # 레이블(키워드) 처리\n",
    "        # 키워드가 리스트가 아니라 단일 문자열일 경우에도 처리 가능하도록 수정\n",
    "        labels = []\n",
    "        for keyword in examples[\"keyword\"]:\n",
    "            if isinstance(keyword, list):  # 리스트일 경우\n",
    "                labels.append(\", \".join(keyword) if keyword else \"\")\n",
    "            else:  # 단일 문자열일 경우\n",
    "                labels.append(keyword if keyword else \"\")\n",
    "        \n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            tokenized_labels = self.tokenizer(\n",
    "                labels,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=None  # 텐서 변환을 DataCollator에 맡김\n",
    "            )\n",
    "\n",
    "        # -100으로 패딩 토큰을 마스킹\n",
    "        labels = tokenized_labels[\"input_ids\"]\n",
    "        for i in range(len(labels)):\n",
    "            for j in range(len(labels[i])):\n",
    "                if labels[i][j] == self.tokenizer.pad_token_id:\n",
    "                    labels[i][j] = -100\n",
    "\n",
    "        model_inputs[\"labels\"] = labels\n",
    "        return model_inputs\n",
    "\n",
    "        \n",
    "    def save_model_and_tokenizer(self, epoch=None, is_best=False):\n",
    "        \"\"\"\n",
    "        최고 성능 모델만 저장하고 이전 모델을 삭제\n",
    "        \"\"\"\n",
    "        if is_best:\n",
    "            # 이전 최고 모델 디렉토리 삭제\n",
    "            if os.path.exists(self.best_model_path):\n",
    "                print(f\"Deleting previous best model at {self.best_model_path}\")\n",
    "                os.system(f\"rm -rf {self.best_model_path}\")\n",
    "            \n",
    "            # 새로운 최고 모델 저장\n",
    "            save_path = os.path.join(self.save_dir, f\"best_model_epoch_{epoch}\")\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            self.model.save_pretrained(save_path)\n",
    "            self.tokenizer.save_pretrained(save_path)\n",
    "            torch.save(self.history, os.path.join(save_path, 'training_history.pt'))\n",
    "            print(f\"New best model saved at {save_path}\")\n",
    "\n",
    "            # 최고 모델 경로 업데이트\n",
    "            self.best_model_path = save_path\n",
    "\n",
    "    def calculate_metrics(self, predictions, labels):\n",
    "        predictions = torch.argmax(predictions, dim=-1)\n",
    "        correct = (predictions == labels).masked_fill(labels == -100, 0)\n",
    "        accuracy = correct.sum().item() / (labels != -100).sum().item()\n",
    "        return accuracy\n",
    "\n",
    "    def train(self, train_dataset, valid_dataset=None):\n",
    "        start_time = time.time()\n",
    "        print(f\"\\nStarting training at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"Training parameters:\")\n",
    "        print(f\"- Batch size: {self.training_args['batch_size']}\")\n",
    "        print(f\"- Learning rate: {self.training_args.get('learning_rate', '2e-5')}\")\n",
    "        print(f\"- Max length: {self.max_length}\")\n",
    "        print(f\"- Number of epochs: {self.training_args['num_epochs']}\")\n",
    "        print(f\"- Training samples: {len(train_dataset)}\")\n",
    "        if valid_dataset:\n",
    "            print(f\"- Validation samples: {len(valid_dataset)}\")\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "        # 데이터셋 전처리\n",
    "        print(\"Preprocessing training data...\")\n",
    "        train_dataset = train_dataset.map(\n",
    "            self.preprocess_data,\n",
    "            batched=True,\n",
    "            remove_columns=train_dataset.column_names,\n",
    "            desc=\"Processing training data\"\n",
    "        )\n",
    "\n",
    "        if valid_dataset is not None:\n",
    "            print(\"Preprocessing validation data...\")\n",
    "            valid_dataset = valid_dataset.map(\n",
    "                self.preprocess_data,\n",
    "                batched=True,\n",
    "                remove_columns=valid_dataset.column_names,\n",
    "                desc=\"Processing validation data\"\n",
    "            )\n",
    "\n",
    "        # DataCollator 설정\n",
    "        data_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer=self.tokenizer,\n",
    "            model=self.model,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # DataLoader 설정 (num_workers=0으로 변경하여 멀티프로세싱 관련 오류 방지)\n",
    "        train_dataloader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.training_args[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            collate_fn=data_collator,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        if valid_dataset is not None:\n",
    "            valid_dataloader = torch.utils.data.DataLoader(\n",
    "                valid_dataset,\n",
    "                batch_size=self.training_args[\"batch_size\"],\n",
    "                shuffle=False,\n",
    "                collate_fn=data_collator,\n",
    "                num_workers=0,\n",
    "                pin_memory=True\n",
    "            )\n",
    "        best_val_loss = float('inf')\n",
    "        early_stopping_counter = 0\n",
    "        early_stopping_patience = self.training_args.get('patience', 3)\n",
    "\n",
    "        for epoch in range(self.training_args[\"num_epochs\"]):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            self.model.train()\n",
    "            epoch_loss = 0\n",
    "            epoch_accuracy = 0\n",
    "            train_steps = 0\n",
    "            \n",
    "            progress_bar = tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}\")\n",
    "            batch_losses = []\n",
    "            batch_accuracies = []\n",
    "            \n",
    "            for batch_idx, batch in enumerate(progress_bar):\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels,\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss # 손실 계산\n",
    "                accuracy = self.calculate_metrics(outputs.logits, labels)\n",
    "                \n",
    "                loss.backward()# 손실 역전파 \n",
    "                self.optimizer.step() # 가중치 업데이트\n",
    "                self.optimizer.zero_grad() # 그래디언트 초기화\n",
    "\n",
    "                batch_losses.append(loss.item())\n",
    "                batch_accuracies.append(accuracy)\n",
    "                \n",
    "                current_loss = np.mean(batch_losses[-100:])\n",
    "                current_accuracy = np.mean(batch_accuracies[-100:])\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f'{current_loss:.4f}',\n",
    "                    'accuracy': f'{current_accuracy:.4f}',\n",
    "                    'batch': f'{batch_idx + 1}/{len(train_dataloader)}'\n",
    "                })\n",
    "\n",
    "            avg_train_loss = np.mean(batch_losses)\n",
    "            avg_train_accuracy = np.mean(batch_accuracies)\n",
    "\n",
    "            if valid_dataset is not None:\n",
    "                self.model.eval()\n",
    "                val_losses = []\n",
    "                val_accuracies = []\n",
    "\n",
    "                print(\"\\nRunning validation...\")\n",
    "                with torch.no_grad():\n",
    "                    for batch in tqdm(valid_dataloader, desc=\"Validating\"):\n",
    "                        input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                        attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                        labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "                        outputs = self.model(\n",
    "                            input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            labels=labels,\n",
    "                        )\n",
    "                        \n",
    "                        loss = outputs.loss\n",
    "                        accuracy = self.calculate_metrics(outputs.logits, labels)\n",
    "                        \n",
    "                        val_losses.append(loss.item())\n",
    "                        val_accuracies.append(accuracy)\n",
    "\n",
    "                avg_val_loss = np.mean(val_losses)\n",
    "                avg_val_accuracy = np.mean(val_accuracies)\n",
    "\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    early_stopping_counter = 0\n",
    "                    self.history['best_epoch'] = epoch + 1\n",
    "                    print(f\"\\nNew best validation loss: {best_val_loss:.4f}\")\n",
    "                    self.save_model_and_tokenizer(epoch + 1, is_best=True)  # 최고 모델만 저장\n",
    "                else:\n",
    "                    early_stopping_counter += 1\n",
    "\n",
    "\n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "            self.history['epoch_times'].append(epoch_time)\n",
    "            self.history['train_loss'].append(avg_train_loss)\n",
    "            if valid_dataset is not None:\n",
    "                self.history['val_loss'].append(avg_val_loss)\n",
    "\n",
    "            # Print epoch summary\n",
    "            print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "            print(f\"Time taken: {epoch_time:.2f} seconds\")\n",
    "            print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "            print(f\"Training accuracy: {avg_train_accuracy:.4f}\")\n",
    "            if valid_dataset is not None:\n",
    "                print(f\"Validation loss: {avg_val_loss:.4f}\")\n",
    "                print(f\"Validation accuracy: {avg_val_accuracy:.4f}\")\n",
    "                print(f\"Best validation loss so far: {best_val_loss:.4f}\")\n",
    "                print(f\"Early stopping counter: {early_stopping_counter}/{early_stopping_patience}\")\n",
    "\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print(\"\\nEarly stopping triggered.\")\n",
    "                break\n",
    "    def predict(self, text: str) -> str:\n",
    "        \"\"\"모델 추론\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            f\"키워드 추출: {self._normalize_text(text)}\",\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "        ).to(self.device)\n",
    "\n",
    "        outputs = self.model.generate(\n",
    "            inputs[\"input_ids\"], max_length=self.max_length, num_beams=5\n",
    "        )\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def _normalize_text(self, text: str) -> str:\n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Model: NVIDIA GeForce RTX 4080\n",
      "Available GPU memory: 15.59 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yjtech2/Desktop/yurim/anaconda3/envs/venv-llm/lib/python3.8/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Parameter 'function'=<bound method CustomKeyBERTTrainer.preprocess_data of <__main__.CustomKeyBERTTrainer object at 0x7f5334b15d00>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training at: 2024-12-03 18:44:09\n",
      "Training parameters:\n",
      "- Batch size: 8\n",
      "- Learning rate: 0.0001\n",
      "- Max length: 128\n",
      "- Number of epochs: 15\n",
      "- Training samples: 240000\n",
      "- Validation samples: 60000\n",
      "\n",
      "==================================================\n",
      "\n",
      "Preprocessing training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff34e9dc23c4c8195dbdb037da37254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing training data:   0%|          | 0/240000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yjtech2/Desktop/yurim/anaconda3/envs/venv-llm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:4114: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing validation data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7192394329f4ff9b58be17da0a5a88d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing validation data:   0%|          | 0/60000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 30000/30000 [27:22<00:00, 18.26it/s, loss=0.0107, accuracy=0.9976, batch=30000/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 7500/7500 [02:00<00:00, 62.44it/s]\n",
      "/home/yjtech2/Desktop/yurim/anaconda3/envs/venv-llm/lib/python3.8/site-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best validation loss: 0.0020\n",
      "New best model saved at ./location_models/best_model_epoch_1\n",
      "\n",
      "Epoch 1 Summary:\n",
      "Time taken: 1763.68 seconds\n",
      "Average training loss: 0.0199\n",
      "Training accuracy: 0.9951\n",
      "Validation loss: 0.0020\n",
      "Validation accuracy: 0.9995\n",
      "Best validation loss so far: 0.0020\n",
      "Early stopping counter: 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 30000/30000 [27:22<00:00, 18.26it/s, loss=0.0023, accuracy=0.9992, batch=30000/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 7500/7500 [02:00<00:00, 62.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best validation loss: 0.0005\n",
      "Deleting previous best model at ./location_models/best_model_epoch_1\n",
      "New best model saved at ./location_models/best_model_epoch_2\n",
      "\n",
      "Epoch 2 Summary:\n",
      "Time taken: 1763.47 seconds\n",
      "Average training loss: 0.0064\n",
      "Training accuracy: 0.9984\n",
      "Validation loss: 0.0005\n",
      "Validation accuracy: 0.9998\n",
      "Best validation loss so far: 0.0005\n",
      "Early stopping counter: 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 30000/30000 [27:22<00:00, 18.26it/s, loss=0.0019, accuracy=0.9994, batch=30000/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 7500/7500 [02:00<00:00, 62.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "Time taken: 1762.64 seconds\n",
      "Average training loss: 0.0048\n",
      "Training accuracy: 0.9988\n",
      "Validation loss: 0.0005\n",
      "Validation accuracy: 0.9998\n",
      "Best validation loss so far: 0.0005\n",
      "Early stopping counter: 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 30000/30000 [27:22<00:00, 18.27it/s, loss=0.0105, accuracy=0.9976, batch=30000/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 7500/7500 [02:00<00:00, 62.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      "Time taken: 1762.38 seconds\n",
      "Average training loss: 0.0041\n",
      "Training accuracy: 0.9990\n",
      "Validation loss: 0.0015\n",
      "Validation accuracy: 0.9996\n",
      "Best validation loss so far: 0.0005\n",
      "Early stopping counter: 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 30000/30000 [27:21<00:00, 18.28it/s, loss=0.0023, accuracy=0.9991, batch=30000/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 7500/7500 [01:59<00:00, 62.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best validation loss: 0.0003\n",
      "Deleting previous best model at ./location_models/best_model_epoch_2\n",
      "New best model saved at ./location_models/best_model_epoch_5\n",
      "\n",
      "Epoch 5 Summary:\n",
      "Time taken: 1762.29 seconds\n",
      "Average training loss: 0.0034\n",
      "Training accuracy: 0.9992\n",
      "Validation loss: 0.0003\n",
      "Validation accuracy: 0.9998\n",
      "Best validation loss so far: 0.0003\n",
      "Early stopping counter: 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 30000/30000 [27:21<00:00, 18.28it/s, loss=0.0016, accuracy=0.9995, batch=30000/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 7500/7500 [01:59<00:00, 62.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Summary:\n",
      "Time taken: 1761.46 seconds\n",
      "Average training loss: 0.0031\n",
      "Training accuracy: 0.9992\n",
      "Validation loss: 0.0013\n",
      "Validation accuracy: 0.9996\n",
      "Best validation loss so far: 0.0003\n",
      "Early stopping counter: 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 30000/30000 [27:22<00:00, 18.27it/s, loss=0.0014, accuracy=0.9995, batch=30000/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 7500/7500 [01:59<00:00, 62.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best validation loss: 0.0003\n",
      "Deleting previous best model at ./location_models/best_model_epoch_5\n",
      "New best model saved at ./location_models/best_model_epoch_7\n",
      "\n",
      "Epoch 7 Summary:\n",
      "Time taken: 1762.81 seconds\n",
      "Average training loss: 0.0027\n",
      "Training accuracy: 0.9993\n",
      "Validation loss: 0.0003\n",
      "Validation accuracy: 0.9998\n",
      "Best validation loss so far: 0.0003\n",
      "Early stopping counter: 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 30000/30000 [27:21<00:00, 18.28it/s, loss=0.0007, accuracy=0.9997, batch=30000/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 7500/7500 [01:59<00:00, 62.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Summary:\n",
      "Time taken: 1761.40 seconds\n",
      "Average training loss: 0.0025\n",
      "Training accuracy: 0.9993\n",
      "Validation loss: 0.0003\n",
      "Validation accuracy: 0.9998\n",
      "Best validation loss so far: 0.0003\n",
      "Early stopping counter: 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 30000/30000 [27:21<00:00, 18.28it/s, loss=0.0059, accuracy=0.9989, batch=30000/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 7500/7500 [01:59<00:00, 62.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Summary:\n",
      "Time taken: 1761.47 seconds\n",
      "Average training loss: 0.0024\n",
      "Training accuracy: 0.9994\n",
      "Validation loss: 0.0012\n",
      "Validation accuracy: 0.9997\n",
      "Best validation loss so far: 0.0003\n",
      "Early stopping counter: 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 30000/30000 [27:21<00:00, 18.28it/s, loss=0.0068, accuracy=0.9986, batch=30000/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 7500/7500 [02:00<00:00, 62.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 Summary:\n",
      "Time taken: 1761.40 seconds\n",
      "Average training loss: 0.0022\n",
      "Training accuracy: 0.9994\n",
      "Validation loss: 0.0006\n",
      "Validation accuracy: 0.9997\n",
      "Best validation loss so far: 0.0003\n",
      "Early stopping counter: 3/3\n",
      "\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # 멀티프로세싱 경고 방지\n",
    "\n",
    "shutil.rmtree(\"./location_models\", ignore_errors=True)\n",
    "\n",
    "trainer = CustomKeyBERTTrainer(\n",
    "    model_name=\"facebook/bart-base\", # t5-base     skt/kobart-base-v2        facebook/bart-base\n",
    "    save_dir=\"./location_models\",\n",
    "    max_length=128,\n",
    "    learning_rate=1e-4,\n",
    "    batch_size=8,\n",
    "    num_epochs=15,\n",
    "    gradient_accumulation_steps=8,\n",
    "    patience=3 # 몇 에폭마다 체크포인트 저장할지\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()  # GPU 메모리 초기화\n",
    "    trainer.train(location_train_dataset, location_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### smell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from typing import Dict\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AdamW,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from mecab import MeCab\n",
    "\n",
    "class CustomKeyBERTTrainer:\n",
    "    def __init__(self, model_name: str, save_dir: str, **kwargs):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        if self.device == \"cuda\":\n",
    "            print(f\"GPU Model: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=kwargs.get(\"learning_rate\", 2e-5))\n",
    "        self.max_length = kwargs.get(\"max_length\", 128)\n",
    "        self.training_args = kwargs\n",
    "        self.save_dir = save_dir\n",
    "        self.mecab = MeCab()  # MeCab 초기화\n",
    "        self.best_model_path = os.path.join(self.save_dir, \"pytorch_model.bin\")\n",
    "        self.tokenizer_path = self.save_dir\n",
    "        \n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'epoch_times': [],\n",
    "            'best_epoch': 0\n",
    "        }\n",
    "\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        \n",
    "    def _mecab_tokenize(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        MeCab을 사용하여 입력 텍스트에서 명사, 형용사, 부사를 추출\n",
    "        \"\"\"\n",
    "        pos_tags = self.mecab.pos(text)  # 품사 태그 추출\n",
    "        # 원하는 품사 필터링: 명사(NNG, NNP), 형용사(VA), 부사(MAG, MAJ)\n",
    "        keywords = [\n",
    "            word for word, pos in pos_tags if pos in (\"NNG\", \"NNP\", \"VA\", \"MAG\", \"MAJ\")\n",
    "        ]\n",
    "        return \" \".join(keywords)  # 추출한 단어를 공백으로 연결하여 반환\n",
    "    \n",
    "    def preprocess_data(self, examples: Dict) -> Dict:\n",
    "        # MeCab 형태소 분석과 품사 필터링 적용\n",
    "        examples[\"text\"] = [self._mecab_tokenize(text) for text in examples[\"text\"]]\n",
    "        \n",
    "        # 기존 코드 유지\n",
    "        inputs = [f\"키워드 추출: {text}\" for text in examples[\"text\"]]\n",
    "        model_inputs = self.tokenizer(\n",
    "            inputs,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=None  # 텐서 변환을 DataCollator에 맡김\n",
    "        )\n",
    "\n",
    "        # 레이블(키워드) 처리\n",
    "        # 키워드가 리스트가 아니라 단일 문자열일 경우에도 처리 가능하도록 수정\n",
    "        labels = []\n",
    "        for keyword in examples[\"keyword\"]:\n",
    "            if isinstance(keyword, list):  # 리스트일 경우\n",
    "                labels.append(\", \".join(keyword) if keyword else \"\")\n",
    "            else:  # 단일 문자열일 경우\n",
    "                labels.append(keyword if keyword else \"\")\n",
    "        \n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            tokenized_labels = self.tokenizer(\n",
    "                labels,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=None  # 텐서 변환을 DataCollator에 맡김\n",
    "            )\n",
    "\n",
    "        # -100으로 패딩 토큰을 마스킹\n",
    "        labels = tokenized_labels[\"input_ids\"]\n",
    "        for i in range(len(labels)):\n",
    "            for j in range(len(labels[i])):\n",
    "                if labels[i][j] == self.tokenizer.pad_token_id:\n",
    "                    labels[i][j] = -100\n",
    "\n",
    "        model_inputs[\"labels\"] = labels\n",
    "        return model_inputs\n",
    "\n",
    "        \n",
    "    def save_model_and_tokenizer(self, epoch=None, is_best=False):\n",
    "        \"\"\"\n",
    "        최고 성능 모델만 저장하고 이전 모델을 삭제\n",
    "        \"\"\"\n",
    "        if is_best:\n",
    "            # 이전 최고 모델 디렉토리 삭제\n",
    "            if os.path.exists(self.best_model_path):\n",
    "                print(f\"Deleting previous best model at {self.best_model_path}\")\n",
    "                os.system(f\"rm -rf {self.best_model_path}\")\n",
    "            \n",
    "            # 새로운 최고 모델 저장\n",
    "            save_path = os.path.join(self.save_dir, f\"best_model_epoch_{epoch}\")\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            self.model.save_pretrained(save_path)\n",
    "            self.tokenizer.save_pretrained(save_path)\n",
    "            torch.save(self.history, os.path.join(save_path, 'training_history.pt'))\n",
    "            print(f\"New best model saved at {save_path}\")\n",
    "\n",
    "            # 최고 모델 경로 업데이트\n",
    "            self.best_model_path = save_path\n",
    "\n",
    "    def calculate_metrics(self, predictions, labels):\n",
    "        predictions = torch.argmax(predictions, dim=-1)\n",
    "        correct = (predictions == labels).masked_fill(labels == -100, 0)\n",
    "        accuracy = correct.sum().item() / (labels != -100).sum().item()\n",
    "        return accuracy\n",
    "\n",
    "    def train(self, train_dataset, valid_dataset=None):\n",
    "        start_time = time.time()\n",
    "        print(f\"\\nStarting training at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"Training parameters:\")\n",
    "        print(f\"- Batch size: {self.training_args['batch_size']}\")\n",
    "        print(f\"- Learning rate: {self.training_args.get('learning_rate', '2e-5')}\")\n",
    "        print(f\"- Max length: {self.max_length}\")\n",
    "        print(f\"- Number of epochs: {self.training_args['num_epochs']}\")\n",
    "        print(f\"- Training samples: {len(train_dataset)}\")\n",
    "        if valid_dataset:\n",
    "            print(f\"- Validation samples: {len(valid_dataset)}\")\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "        # 데이터셋 전처리\n",
    "        print(\"Preprocessing training data...\")\n",
    "        train_dataset = train_dataset.map(\n",
    "            self.preprocess_data,\n",
    "            batched=True,\n",
    "            remove_columns=train_dataset.column_names,\n",
    "            desc=\"Processing training data\"\n",
    "        )\n",
    "\n",
    "        if valid_dataset is not None:\n",
    "            print(\"Preprocessing validation data...\")\n",
    "            valid_dataset = valid_dataset.map(\n",
    "                self.preprocess_data,\n",
    "                batched=True,\n",
    "                remove_columns=valid_dataset.column_names,\n",
    "                desc=\"Processing validation data\"\n",
    "            )\n",
    "\n",
    "        # DataCollator 설정\n",
    "        data_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer=self.tokenizer,\n",
    "            model=self.model,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # DataLoader 설정 (num_workers=0으로 변경하여 멀티프로세싱 관련 오류 방지)\n",
    "        train_dataloader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.training_args[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            collate_fn=data_collator,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        if valid_dataset is not None:\n",
    "            valid_dataloader = torch.utils.data.DataLoader(\n",
    "                valid_dataset,\n",
    "                batch_size=self.training_args[\"batch_size\"],\n",
    "                shuffle=False,\n",
    "                collate_fn=data_collator,\n",
    "                num_workers=0,\n",
    "                pin_memory=True\n",
    "            )\n",
    "        best_val_loss = float('inf')\n",
    "        early_stopping_counter = 0\n",
    "        early_stopping_patience = self.training_args.get('patience', 3)\n",
    "\n",
    "        for epoch in range(self.training_args[\"num_epochs\"]):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            self.model.train()\n",
    "            epoch_loss = 0\n",
    "            epoch_accuracy = 0\n",
    "            train_steps = 0\n",
    "            \n",
    "            progress_bar = tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}\")\n",
    "            batch_losses = []\n",
    "            batch_accuracies = []\n",
    "            \n",
    "            for batch_idx, batch in enumerate(progress_bar):\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels,\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss # 손실 계산\n",
    "                accuracy = self.calculate_metrics(outputs.logits, labels)\n",
    "                \n",
    "                loss.backward()# 손실 역전파 \n",
    "                self.optimizer.step() # 가중치 업데이트\n",
    "                self.optimizer.zero_grad() # 그래디언트 초기화\n",
    "\n",
    "                batch_losses.append(loss.item())\n",
    "                batch_accuracies.append(accuracy)\n",
    "                \n",
    "                current_loss = np.mean(batch_losses[-100:])\n",
    "                current_accuracy = np.mean(batch_accuracies[-100:])\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f'{current_loss:.4f}',\n",
    "                    'accuracy': f'{current_accuracy:.4f}',\n",
    "                    'batch': f'{batch_idx + 1}/{len(train_dataloader)}'\n",
    "                })\n",
    "\n",
    "            avg_train_loss = np.mean(batch_losses)\n",
    "            avg_train_accuracy = np.mean(batch_accuracies)\n",
    "\n",
    "            if valid_dataset is not None:\n",
    "                self.model.eval()\n",
    "                val_losses = []\n",
    "                val_accuracies = []\n",
    "\n",
    "                print(\"\\nRunning validation...\")\n",
    "                with torch.no_grad():\n",
    "                    for batch in tqdm(valid_dataloader, desc=\"Validating\"):\n",
    "                        input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                        attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                        labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "                        outputs = self.model(\n",
    "                            input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            labels=labels,\n",
    "                        )\n",
    "                        \n",
    "                        loss = outputs.loss\n",
    "                        accuracy = self.calculate_metrics(outputs.logits, labels)\n",
    "                        \n",
    "                        val_losses.append(loss.item())\n",
    "                        val_accuracies.append(accuracy)\n",
    "\n",
    "                avg_val_loss = np.mean(val_losses)\n",
    "                avg_val_accuracy = np.mean(val_accuracies)\n",
    "\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    early_stopping_counter = 0\n",
    "                    self.history['best_epoch'] = epoch + 1\n",
    "                    print(f\"\\nNew best validation loss: {best_val_loss:.4f}\")\n",
    "                    self.save_model_and_tokenizer(epoch + 1, is_best=True)  # 최고 모델만 저장\n",
    "                else:\n",
    "                    early_stopping_counter += 1\n",
    "\n",
    "\n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "            self.history['epoch_times'].append(epoch_time)\n",
    "            self.history['train_loss'].append(avg_train_loss)\n",
    "            if valid_dataset is not None:\n",
    "                self.history['val_loss'].append(avg_val_loss)\n",
    "\n",
    "            # Print epoch summary\n",
    "            print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "            print(f\"Time taken: {epoch_time:.2f} seconds\")\n",
    "            print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "            print(f\"Training accuracy: {avg_train_accuracy:.4f}\")\n",
    "            if valid_dataset is not None:\n",
    "                print(f\"Validation loss: {avg_val_loss:.4f}\")\n",
    "                print(f\"Validation accuracy: {avg_val_accuracy:.4f}\")\n",
    "                print(f\"Best validation loss so far: {best_val_loss:.4f}\")\n",
    "                print(f\"Early stopping counter: {early_stopping_counter}/{early_stopping_patience}\")\n",
    "\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print(\"\\nEarly stopping triggered.\")\n",
    "                break\n",
    "    def predict(self, text: str) -> str:\n",
    "        \"\"\"모델 추론\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            f\"키워드 추출: {self._normalize_text(text)}\",\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "        ).to(self.device)\n",
    "\n",
    "        outputs = self.model.generate(\n",
    "            inputs[\"input_ids\"], max_length=self.max_length, num_beams=5\n",
    "        )\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def _normalize_text(self, text: str) -> str:\n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Model: NVIDIA GeForce RTX 4080\n",
      "Available GPU memory: 15.59 GB\n",
      "\n",
      "Starting training at: 2024-12-03 23:41:34\n",
      "Training parameters:\n",
      "- Batch size: 8\n",
      "- Learning rate: 0.0001\n",
      "- Max length: 128\n",
      "- Number of epochs: 15\n",
      "- Training samples: 240000\n",
      "- Validation samples: 60000\n",
      "\n",
      "==================================================\n",
      "\n",
      "Preprocessing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yjtech2/Desktop/yurim/anaconda3/envs/venv-llm/lib/python3.8/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30004c6dcd9427abdaa08508f41909b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing training data:   0%|          | 0/240000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yjtech2/Desktop/yurim/anaconda3/envs/venv-llm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:4114: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing validation data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f239d47ce2428b82b715dd43738a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing validation data:   0%|          | 0/60000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 30000/30000 [27:24<00:00, 18.24it/s, loss=0.3019, accuracy=0.8849, batch=30000/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 7500/7500 [01:59<00:00, 62.68it/s]\n",
      "/home/yjtech2/Desktop/yurim/anaconda3/envs/venv-llm/lib/python3.8/site-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best validation loss: 0.2931\n",
      "New best model saved at ./smell_models/best_model_epoch_1\n",
      "\n",
      "Epoch 1 Summary:\n",
      "Time taken: 1764.73 seconds\n",
      "Average training loss: 0.3271\n",
      "Training accuracy: 0.8774\n",
      "Validation loss: 0.2931\n",
      "Validation accuracy: 0.8839\n",
      "Best validation loss so far: 0.2931\n",
      "Early stopping counter: 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 30000/30000 [27:24<00:00, 18.25it/s, loss=0.2909, accuracy=0.8855, batch=30000/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 7500/7500 [01:59<00:00, 62.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best validation loss: 0.2878\n",
      "Deleting previous best model at ./smell_models/best_model_epoch_1\n",
      "New best model saved at ./smell_models/best_model_epoch_2\n",
      "\n",
      "Epoch 2 Summary:\n",
      "Time taken: 1764.31 seconds\n",
      "Average training loss: 0.2939\n",
      "Training accuracy: 0.8847\n",
      "Validation loss: 0.2878\n",
      "Validation accuracy: 0.8867\n",
      "Best validation loss so far: 0.2878\n",
      "Early stopping counter: 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 30000/30000 [27:24<00:00, 18.25it/s, loss=0.2888, accuracy=0.8846, batch=30000/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 7500/7500 [01:59<00:00, 62.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best validation loss: 0.2854\n",
      "Deleting previous best model at ./smell_models/best_model_epoch_2\n",
      "New best model saved at ./smell_models/best_model_epoch_3\n",
      "\n",
      "Epoch 3 Summary:\n",
      "Time taken: 1764.46 seconds\n",
      "Average training loss: 0.2904\n",
      "Training accuracy: 0.8857\n",
      "Validation loss: 0.2854\n",
      "Validation accuracy: 0.8881\n",
      "Best validation loss so far: 0.2854\n",
      "Early stopping counter: 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 30000/30000 [27:24<00:00, 18.24it/s, loss=0.2868, accuracy=0.8879, batch=30000/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 7500/7500 [01:59<00:00, 62.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      "Time taken: 1764.28 seconds\n",
      "Average training loss: 0.2890\n",
      "Training accuracy: 0.8861\n",
      "Validation loss: 0.2934\n",
      "Validation accuracy: 0.8850\n",
      "Best validation loss so far: 0.2854\n",
      "Early stopping counter: 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 30000/30000 [27:24<00:00, 18.24it/s, loss=0.2882, accuracy=0.8882, batch=30000/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 7500/7500 [01:59<00:00, 62.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best validation loss: 0.2838\n",
      "Deleting previous best model at ./smell_models/best_model_epoch_3\n",
      "New best model saved at ./smell_models/best_model_epoch_5\n",
      "\n",
      "Epoch 5 Summary:\n",
      "Time taken: 1764.74 seconds\n",
      "Average training loss: 0.2883\n",
      "Training accuracy: 0.8864\n",
      "Validation loss: 0.2838\n",
      "Validation accuracy: 0.8867\n",
      "Best validation loss so far: 0.2838\n",
      "Early stopping counter: 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 30000/30000 [27:23<00:00, 18.25it/s, loss=0.2845, accuracy=0.8873, batch=30000/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 7500/7500 [01:59<00:00, 62.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best validation loss: 0.2830\n",
      "Deleting previous best model at ./smell_models/best_model_epoch_5\n",
      "New best model saved at ./smell_models/best_model_epoch_6\n",
      "\n",
      "Epoch 6 Summary:\n",
      "Time taken: 1763.75 seconds\n",
      "Average training loss: 0.2873\n",
      "Training accuracy: 0.8866\n",
      "Validation loss: 0.2830\n",
      "Validation accuracy: 0.8878\n",
      "Best validation loss so far: 0.2830\n",
      "Early stopping counter: 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 30000/30000 [27:23<00:00, 18.25it/s, loss=0.2834, accuracy=0.8853, batch=30000/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 7500/7500 [01:59<00:00, 62.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best validation loss: 0.2828\n",
      "Deleting previous best model at ./smell_models/best_model_epoch_6\n",
      "New best model saved at ./smell_models/best_model_epoch_7\n",
      "\n",
      "Epoch 7 Summary:\n",
      "Time taken: 1764.11 seconds\n",
      "Average training loss: 0.2869\n",
      "Training accuracy: 0.8868\n",
      "Validation loss: 0.2828\n",
      "Validation accuracy: 0.8880\n",
      "Best validation loss so far: 0.2828\n",
      "Early stopping counter: 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 30000/30000 [27:22<00:00, 18.26it/s, loss=0.2845, accuracy=0.8842, batch=30000/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 7500/7500 [01:59<00:00, 62.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Summary:\n",
      "Time taken: 1762.29 seconds\n",
      "Average training loss: 0.2863\n",
      "Training accuracy: 0.8870\n",
      "Validation loss: 0.2828\n",
      "Validation accuracy: 0.8878\n",
      "Best validation loss so far: 0.2828\n",
      "Early stopping counter: 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 30000/30000 [27:23<00:00, 18.25it/s, loss=0.2795, accuracy=0.8890, batch=30000/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 7500/7500 [01:59<00:00, 62.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best validation loss: 0.2826\n",
      "Deleting previous best model at ./smell_models/best_model_epoch_7\n",
      "New best model saved at ./smell_models/best_model_epoch_9\n",
      "\n",
      "Epoch 9 Summary:\n",
      "Time taken: 1763.86 seconds\n",
      "Average training loss: 0.2859\n",
      "Training accuracy: 0.8870\n",
      "Validation loss: 0.2826\n",
      "Validation accuracy: 0.8883\n",
      "Best validation loss so far: 0.2826\n",
      "Early stopping counter: 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 30000/30000 [27:23<00:00, 18.25it/s, loss=0.2840, accuracy=0.8915, batch=30000/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 7500/7500 [01:59<00:00, 62.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 Summary:\n",
      "Time taken: 1763.06 seconds\n",
      "Average training loss: 0.2856\n",
      "Training accuracy: 0.8871\n",
      "Validation loss: 0.2829\n",
      "Validation accuracy: 0.8877\n",
      "Best validation loss so far: 0.2826\n",
      "Early stopping counter: 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11: 100%|██████████| 30000/30000 [27:23<00:00, 18.25it/s, loss=0.2815, accuracy=0.8859, batch=30000/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 7500/7500 [01:59<00:00, 62.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best validation loss: 0.2822\n",
      "Deleting previous best model at ./smell_models/best_model_epoch_9\n",
      "New best model saved at ./smell_models/best_model_epoch_11\n",
      "\n",
      "Epoch 11 Summary:\n",
      "Time taken: 1763.59 seconds\n",
      "Average training loss: 0.2853\n",
      "Training accuracy: 0.8872\n",
      "Validation loss: 0.2822\n",
      "Validation accuracy: 0.8879\n",
      "Best validation loss so far: 0.2822\n",
      "Early stopping counter: 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12: 100%|██████████| 30000/30000 [27:22<00:00, 18.26it/s, loss=0.2845, accuracy=0.8858, batch=30000/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 7500/7500 [01:59<00:00, 62.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12 Summary:\n",
      "Time taken: 1762.32 seconds\n",
      "Average training loss: 0.2851\n",
      "Training accuracy: 0.8873\n",
      "Validation loss: 0.2824\n",
      "Validation accuracy: 0.8871\n",
      "Best validation loss so far: 0.2822\n",
      "Early stopping counter: 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13: 100%|██████████| 30000/30000 [27:22<00:00, 18.26it/s, loss=0.2844, accuracy=0.8889, batch=30000/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 7500/7500 [01:59<00:00, 62.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13 Summary:\n",
      "Time taken: 1762.22 seconds\n",
      "Average training loss: 0.2851\n",
      "Training accuracy: 0.8873\n",
      "Validation loss: 0.2833\n",
      "Validation accuracy: 0.8879\n",
      "Best validation loss so far: 0.2822\n",
      "Early stopping counter: 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14: 100%|██████████| 30000/30000 [27:23<00:00, 18.26it/s, loss=0.2874, accuracy=0.8860, batch=30000/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 7500/7500 [01:59<00:00, 62.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best validation loss: 0.2821\n",
      "Deleting previous best model at ./smell_models/best_model_epoch_11\n",
      "New best model saved at ./smell_models/best_model_epoch_14\n",
      "\n",
      "Epoch 14 Summary:\n",
      "Time taken: 1763.48 seconds\n",
      "Average training loss: 0.2848\n",
      "Training accuracy: 0.8874\n",
      "Validation loss: 0.2821\n",
      "Validation accuracy: 0.8870\n",
      "Best validation loss so far: 0.2821\n",
      "Early stopping counter: 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15: 100%|██████████| 30000/30000 [27:23<00:00, 18.26it/s, loss=0.2839, accuracy=0.8886, batch=30000/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 7500/7500 [01:59<00:00, 62.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15 Summary:\n",
      "Time taken: 1762.43 seconds\n",
      "Average training loss: 0.2849\n",
      "Training accuracy: 0.8874\n",
      "Validation loss: 0.2826\n",
      "Validation accuracy: 0.8876\n",
      "Best validation loss so far: 0.2821\n",
      "Early stopping counter: 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # 멀티프로세싱 경고 방지\n",
    "\n",
    "shutil.rmtree(\"./smell_models\", ignore_errors=True)\n",
    "\n",
    "trainer = CustomKeyBERTTrainer(\n",
    "    model_name=\"facebook/bart-base\", # t5-base     skt/kobart-base-v2\n",
    "    save_dir=\"./smell_models\",\n",
    "    max_length=128,\n",
    "    learning_rate=1e-4,\n",
    "    batch_size=8,\n",
    "    num_epochs=15,\n",
    "    gradient_accumulation_steps=8,\n",
    "    patience=3 # 몇 에폭마다 체크포인트 저장할지\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()  # GPU 메모리 초기화\n",
    "    trainer.train(smell_train_dataset, smell_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "# from typing import Dict\n",
    "# import time\n",
    "# from datetime import datetime\n",
    "# import numpy as np\n",
    "# from transformers import (\n",
    "#     AutoTokenizer,\n",
    "#     AutoModelForSeq2SeqLM,\n",
    "#     AdamW,\n",
    "#     DataCollatorForSeq2Seq,\n",
    "# )\n",
    "\n",
    "# from transformers import get_linear_schedule_with_warmup\n",
    "# from torch.cuda.amp import GradScaler, autocast\n",
    "# from mecab import MeCab\n",
    "\n",
    "# class CustomKeyBERTTrainer:\n",
    "#     def __init__(self, model_name: str, save_dir: str, **kwargs):\n",
    "#         self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#         print(f\"Using device: {self.device}\")\n",
    "#         if self.device == \"cuda\":\n",
    "#             print(f\"GPU Model: {torch.cuda.get_device_name(0)}\")\n",
    "#             print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        \n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#         self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
    "#         self.optimizer = AdamW(self.model.parameters(), lr=kwargs.get(\"learning_rate\", 2e-5))\n",
    "#         self.max_length = kwargs.get(\"max_length\", 128)\n",
    "#         self.training_args = kwargs\n",
    "#         self.save_dir = save_dir\n",
    "#         self.mecab = MeCab()  # MeCab 초기화\n",
    "#         self.best_model_path = os.path.join(self.save_dir, \"pytorch_model.bin\")\n",
    "#         self.tokenizer_path = self.save_dir\n",
    "        \n",
    "#         self.history = {\n",
    "#             'train_loss': [],\n",
    "#             'val_loss': [],\n",
    "#             'epoch_times': [],\n",
    "#             'best_epoch': 0\n",
    "#         }\n",
    "\n",
    "#         os.makedirs(self.save_dir, exist_ok=True)\n",
    "        \n",
    "#     def _mecab_tokenize(self, text: str) -> str:\n",
    "#         \"\"\"\n",
    "#         MeCab을 사용하여 입력 텍스트에서 명사, 형용사, 부사를 추출\n",
    "#         \"\"\"\n",
    "#         pos_tags = self.mecab.pos(text)  # 품사 태그 추출\n",
    "#         # 원하는 품사 필터링: 명사(NNG, NNP), 형용사(VA), 부사(MAG, MAJ)\n",
    "#         keywords = [\n",
    "#             word for word, pos in pos_tags if pos in (\"NNG\", \"NNP\", \"VA\", \"MAG\", \"MAJ\")\n",
    "#         ]\n",
    "#         return \" \".join(keywords)  # 추출한 단어를 공백으로 연결하여 반환\n",
    "    \n",
    "#     def preprocess_data(self, examples: Dict) -> Dict:\n",
    "#         # MeCab 형태소 분석과 품사 필터링 적용\n",
    "#         examples[\"text\"] = [self._mecab_tokenize(text) for text in examples[\"text\"]]\n",
    "        \n",
    "#         # 기존 코드 유지\n",
    "#         inputs = [f\"키워드 추출: {text}\" for text in examples[\"text\"]]\n",
    "#         model_inputs = self.tokenizer(\n",
    "#             inputs,\n",
    "#             max_length=self.max_length,\n",
    "#             padding=\"max_length\",\n",
    "#             truncation=True,\n",
    "#             return_tensors=None  # 텐서 변환을 DataCollator에 맡김\n",
    "#         )\n",
    "\n",
    "#         # 레이블(키워드) 처리\n",
    "#         # 키워드가 리스트가 아니라 단일 문자열일 경우에도 처리 가능하도록 수정\n",
    "#         labels = []\n",
    "#         for keyword in examples[\"keyword\"]:\n",
    "#             if isinstance(keyword, list):  # 리스트일 경우\n",
    "#                 labels.append(\", \".join(keyword) if keyword else \"\")\n",
    "#             else:  # 단일 문자열일 경우\n",
    "#                 labels.append(keyword if keyword else \"\")\n",
    "        \n",
    "#         with self.tokenizer.as_target_tokenizer():\n",
    "#             tokenized_labels = self.tokenizer(\n",
    "#                 labels,\n",
    "#                 max_length=self.max_length,\n",
    "#                 padding=\"max_length\",\n",
    "#                 truncation=True,\n",
    "#                 return_tensors=None  # 텐서 변환을 DataCollator에 맡김\n",
    "#             )\n",
    "\n",
    "#         # -100으로 패딩 토큰을 마스킹\n",
    "#         labels = tokenized_labels[\"input_ids\"]\n",
    "#         for i in range(len(labels)):\n",
    "#             for j in range(len(labels[i])):\n",
    "#                 if labels[i][j] == self.tokenizer.pad_token_id:\n",
    "#                     labels[i][j] = -100\n",
    "\n",
    "#         model_inputs[\"labels\"] = labels\n",
    "#         return model_inputs\n",
    "\n",
    "        \n",
    "#     def save_model_and_tokenizer(self, epoch=None, is_best=False):\n",
    "#         \"\"\"\n",
    "#         최고 성능 모델만 저장하고 이전 모델을 삭제\n",
    "#         \"\"\"\n",
    "#         if is_best:\n",
    "#             # 이전 최고 모델 디렉토리 삭제\n",
    "#             if os.path.exists(self.best_model_path):\n",
    "#                 print(f\"Deleting previous best model at {self.best_model_path}\")\n",
    "#                 os.system(f\"rm -rf {self.best_model_path}\")\n",
    "            \n",
    "#             # 새로운 최고 모델 저장\n",
    "#             save_path = os.path.join(self.save_dir, f\"best_model_epoch_{epoch}\")\n",
    "#             os.makedirs(save_path, exist_ok=True)\n",
    "#             self.model.save_pretrained(save_path)\n",
    "#             self.tokenizer.save_pretrained(save_path)\n",
    "#             torch.save(self.history, os.path.join(save_path, 'training_history.pt'))\n",
    "#             print(f\"New best model saved at {save_path}\")\n",
    "\n",
    "#             # 최고 모델 경로 업데이트\n",
    "#             self.best_model_path = save_path\n",
    "\n",
    "#     def calculate_metrics(self, predictions, labels):\n",
    "#         predictions = torch.argmax(predictions, dim=-1)\n",
    "#         correct = (predictions == labels).masked_fill(labels == -100, 0)\n",
    "#         accuracy = correct.sum().item() / (labels != -100).sum().item()\n",
    "#         return accuracy\n",
    "\n",
    "#     def train(self, train_dataset, valid_dataset=None):\n",
    "#         start_time = time.time()\n",
    "#         print(f\"\\nStarting training at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "#         print(f\"Training parameters:\")\n",
    "#         print(f\"- Batch size: {self.training_args['batch_size']}\")\n",
    "#         print(f\"- Learning rate: {self.training_args.get('learning_rate', '2e-5')}\")\n",
    "#         print(f\"- Max length: {self.max_length}\")\n",
    "#         print(f\"- Number of epochs: {self.training_args['num_epochs']}\")\n",
    "#         print(f\"- Training samples: {len(train_dataset)}\")\n",
    "#         if valid_dataset:\n",
    "#             print(f\"- Validation samples: {len(valid_dataset)}\")\n",
    "#         print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "#         # 데이터셋 전처리\n",
    "#         print(\"Preprocessing training data...\")\n",
    "#         train_dataset = train_dataset.map(\n",
    "#             self.preprocess_data,\n",
    "#             batched=True,\n",
    "#             remove_columns=train_dataset.column_names,\n",
    "#             desc=\"Processing training data\"\n",
    "#         )\n",
    "\n",
    "#         if valid_dataset is not None:\n",
    "#             print(\"Preprocessing validation data...\")\n",
    "#             valid_dataset = valid_dataset.map(\n",
    "#                 self.preprocess_data,\n",
    "#                 batched=True,\n",
    "#                 remove_columns=valid_dataset.column_names,\n",
    "#                 desc=\"Processing validation data\"\n",
    "#             )\n",
    "\n",
    "#         # DataCollator 설정\n",
    "#         data_collator = DataCollatorForSeq2Seq(\n",
    "#             tokenizer=self.tokenizer,\n",
    "#             model=self.model,\n",
    "#             padding=True,\n",
    "#             return_tensors=\"pt\"\n",
    "#         )\n",
    "\n",
    "#         # DataLoader 설정 (num_workers=0으로 변경하여 멀티프로세싱 관련 오류 방지)\n",
    "#         train_dataloader = torch.utils.data.DataLoader(\n",
    "#             train_dataset,\n",
    "#             batch_size=self.training_args[\"batch_size\"],\n",
    "#             shuffle=True,\n",
    "#             collate_fn=data_collator,\n",
    "#             num_workers=0,\n",
    "#             pin_memory=True\n",
    "#         )\n",
    "\n",
    "#         if valid_dataset is not None:\n",
    "#             valid_dataloader = torch.utils.data.DataLoader(\n",
    "#                 valid_dataset,\n",
    "#                 batch_size=self.training_args[\"batch_size\"],\n",
    "#                 shuffle=False,\n",
    "#                 collate_fn=data_collator,\n",
    "#                 num_workers=0,\n",
    "#                 pin_memory=True\n",
    "#             )\n",
    "#         best_val_loss = float('inf')\n",
    "#         early_stopping_counter = 0\n",
    "#         early_stopping_patience = self.training_args.get('patience', 3)\n",
    "\n",
    "#         for epoch in range(self.training_args[\"num_epochs\"]):\n",
    "#             epoch_start_time = time.time()\n",
    "            \n",
    "#             self.model.train()\n",
    "#             epoch_loss = 0\n",
    "#             epoch_accuracy = 0\n",
    "#             train_steps = 0\n",
    "            \n",
    "#             progress_bar = tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}\")\n",
    "#             batch_losses = []\n",
    "#             batch_accuracies = []\n",
    "            \n",
    "#             for batch_idx, batch in enumerate(progress_bar):\n",
    "#                 input_ids = batch[\"input_ids\"].to(self.device)\n",
    "#                 attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "#                 labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "#                 outputs = self.model(\n",
    "#                     input_ids=input_ids,\n",
    "#                     attention_mask=attention_mask,\n",
    "#                     labels=labels,\n",
    "#                 )\n",
    "                \n",
    "#                 loss = outputs.loss # 손실 계산\n",
    "#                 accuracy = self.calculate_metrics(outputs.logits, labels)\n",
    "                \n",
    "#                 loss.backward()# 손실 역전파 \n",
    "#                 self.optimizer.step() # 가중치 업데이트\n",
    "#                 self.optimizer.zero_grad() # 그래디언트 초기화\n",
    "\n",
    "#                 batch_losses.append(loss.item())\n",
    "#                 batch_accuracies.append(accuracy)\n",
    "                \n",
    "#                 current_loss = np.mean(batch_losses[-100:])\n",
    "#                 current_accuracy = np.mean(batch_accuracies[-100:])\n",
    "#                 progress_bar.set_postfix({\n",
    "#                     'loss': f'{current_loss:.4f}',\n",
    "#                     'accuracy': f'{current_accuracy:.4f}',\n",
    "#                     'batch': f'{batch_idx + 1}/{len(train_dataloader)}'\n",
    "#                 })\n",
    "\n",
    "#             avg_train_loss = np.mean(batch_losses)\n",
    "#             avg_train_accuracy = np.mean(batch_accuracies)\n",
    "\n",
    "#             if valid_dataset is not None:\n",
    "#                 self.model.eval()\n",
    "#                 val_losses = []\n",
    "#                 val_accuracies = []\n",
    "\n",
    "#                 print(\"\\nRunning validation...\")\n",
    "#                 with torch.no_grad():\n",
    "#                     for batch in tqdm(valid_dataloader, desc=\"Validating\"):\n",
    "#                         input_ids = batch[\"input_ids\"].to(self.device)\n",
    "#                         attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "#                         labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "#                         outputs = self.model(\n",
    "#                             input_ids=input_ids,\n",
    "#                             attention_mask=attention_mask,\n",
    "#                             labels=labels,\n",
    "#                         )\n",
    "                        \n",
    "#                         loss = outputs.loss\n",
    "#                         accuracy = self.calculate_metrics(outputs.logits, labels)\n",
    "                        \n",
    "#                         val_losses.append(loss.item())\n",
    "#                         val_accuracies.append(accuracy)\n",
    "\n",
    "#                 avg_val_loss = np.mean(val_losses)\n",
    "#                 avg_val_accuracy = np.mean(val_accuracies)\n",
    "\n",
    "#                 if avg_val_loss < best_val_loss:\n",
    "#                     best_val_loss = avg_val_loss\n",
    "#                     early_stopping_counter = 0\n",
    "#                     self.history['best_epoch'] = epoch + 1\n",
    "#                     print(f\"\\nNew best validation loss: {best_val_loss:.4f}\")\n",
    "#                     self.save_model_and_tokenizer(epoch + 1, is_best=True)  # 최고 모델만 저장\n",
    "#                 else:\n",
    "#                     early_stopping_counter += 1\n",
    "\n",
    "\n",
    "#             epoch_time = time.time() - epoch_start_time\n",
    "#             self.history['epoch_times'].append(epoch_time)\n",
    "#             self.history['train_loss'].append(avg_train_loss)\n",
    "#             if valid_dataset is not None:\n",
    "#                 self.history['val_loss'].append(avg_val_loss)\n",
    "\n",
    "#             # Print epoch summary\n",
    "#             print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "#             print(f\"Time taken: {epoch_time:.2f} seconds\")\n",
    "#             print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "#             print(f\"Training accuracy: {avg_train_accuracy:.4f}\")\n",
    "#             if valid_dataset is not None:\n",
    "#                 print(f\"Validation loss: {avg_val_loss:.4f}\")\n",
    "#                 print(f\"Validation accuracy: {avg_val_accuracy:.4f}\")\n",
    "#                 print(f\"Best validation loss so far: {best_val_loss:.4f}\")\n",
    "#                 print(f\"Early stopping counter: {early_stopping_counter}/{early_stopping_patience}\")\n",
    "\n",
    "#             if early_stopping_counter >= early_stopping_patience:\n",
    "#                 print(\"\\nEarly stopping triggered.\")\n",
    "#                 break\n",
    "#     def predict(self, text: str) -> str:\n",
    "#         \"\"\"모델 추론\"\"\"\n",
    "#         inputs = self.tokenizer(\n",
    "#             f\"키워드 추출: {self._normalize_text(text)}\",\n",
    "#             return_tensors=\"pt\",\n",
    "#             max_length=self.max_length,\n",
    "#             truncation=True,\n",
    "#         ).to(self.device)\n",
    "\n",
    "#         outputs = self.model.generate(\n",
    "#             inputs[\"input_ids\"], max_length=self.max_length, num_beams=5\n",
    "#         )\n",
    "#         return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "#     def _normalize_text(self, text: str) -> str:\n",
    "#         return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # 멀티프로세싱 경고 방지\n",
    "\n",
    "# shutil.rmtree(\"./time_models\", ignore_errors=True)\n",
    "\n",
    "# trainer = CustomKeyBERTTrainer(\n",
    "#     model_name=\"facebook/bart-base\", # t5-base     skt/kobart-base-v2\n",
    "#     save_dir=\"./time_models\",\n",
    "#     max_length=128,\n",
    "#     learning_rate=1e-4,\n",
    "#     batch_size=8,\n",
    "#     num_epochs=15,\n",
    "#     gradient_accumulation_steps=8,\n",
    "#     patience=3 # 몇 에폭마다 체크포인트 저장할지\n",
    "# )\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     torch.cuda.empty_cache()  # GPU 메모리 초기화\n",
    "#     trainer.train(time_train_dataset, time_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_dates_and_times(text):\n",
    "    \"\"\"\n",
    "    문장에서 다양한 날짜와 시간 형식을 추출하는 함수\n",
    "    \"\"\"\n",
    "    # 날짜 및 시간 패턴 정의\n",
    "    patterns = [\n",
    "        # 날짜 패턴\n",
    "        r\"\\d{1,2}월 \\d{1,2}일\",            # 11월 3일\n",
    "        r\"\\d{1,2}월\",                      # 11월\n",
    "        r\"\\d{1,2}일\",                      # 3일\n",
    "        r\"\\d{1,2}년 \\d{1,2}월 \\d{1,2}일\", # 2023년 11월 3일\n",
    "        r\"\\d{1,2}년\",                      # 2023년\n",
    "        # 상대적 날짜 패턴\n",
    "        r\"오늘\", r\"어제\", r\"내일\", r\"모레\", r\"글피\",  # 상대 날짜\n",
    "        r\"\\d+일 전\", r\"\\d+일 후\",             # 3일 전, 3일 후\n",
    "        r\"\\d+개월 전\", r\"\\d+개월 후\",         # 3개월 전, 3개월 후\n",
    "        r\"\\d+년 전\", r\"\\d+년 후\",             # 3년 전, 3년 후\n",
    "\n",
    "        # 시간 패턴\n",
    "        r\"\\d{1,2}시\",                      # 3시\n",
    "        r\"\\d{1,2}시 \\d{1,2}분\",            # 3시 15분\n",
    "        r\"\\d{1,2}:\\d{2}\",                  # 03:15\n",
    "        r\"오전 \\d{1,2}시\",                 # 오전 9시\n",
    "        r\"오후 \\d{1,2}시\",                 # 오후 3시\n",
    "        r\"새벽 \\d{1,2}시\",                 # 새벽 2시\n",
    "        r\"정오\", r\"자정\",                  # 정오, 자정\n",
    "\n",
    "        # 복합 날짜와 시간\n",
    "        r\"\\d{1,2}월 \\d{1,2}일 \\d{1,2}시\",     # 11월 3일 3시\n",
    "        r\"\\d{1,2}월 \\d{1,2}일 오전 \\d{1,2}시\", # 11월 3일 오전 9시\n",
    "        r\"\\d{1,2}월 \\d{1,2}일 오후 \\d{1,2}시\"  # 11월 3일 오후 3시\n",
    "    ]\n",
    "\n",
    "    # 패턴 컴파일 및 검색\n",
    "    combined_pattern = \"|\".join(patterns)\n",
    "    matches = re.findall(combined_pattern, text)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화된 텍스트: ['í', 'Ĥ', '¤', 'ì', 'Ľ', 'Į', 'ë', 'ĵ', 'ľ', 'Ġì', '¶', 'Ķ', 'ì', '¶', 'ľ', ':', 'Ġì', 'ł', 'Ģ', 'í', 'Ŀ', '¬', 'Ġ', 'ê', '°', 'Ģ', 'ì', '¶', 'ķ', 'Ġë', '²', 'Ī', 'ì', 'ĭ', 'Ŀ', 'Ġì', 'Ħ', '¼', 'í', 'Ħ', '°', 'ì', 'Ĺ', 'Ĳ', 'ì', 'Ħ', 'ľ', 'Ġ2', 'ì', 'Ľ', 'Ķ', 'Ġ4', 'ìĿ', '¼', 'Ġë', '°', '¤', 'Ġ10', 'ì', 'ĭ', 'ľ', 'ì', 'Ĺ', 'Ĳ', 'Ġì', 'ŀ', 'ĳ', 'ë', '¬¼', 'Ġì', 'ĺ', '¤', 'ì', 'Ĺ', '¼', 'ë', '¡', 'ľ', 'Ġì', 'Ŀ', '¸', 'íķ', '´', 'Ġë', '¶', 'Ī', 'í', 'İ', '¸', 'ìĿ', 'Ħ', 'Ġ', 'ê', '²', 'ª', 'ê', '³', 'ł', 'Ġì', 'ŀ', 'Ī', 'ì', 'ĸ', '´', 'Ġì', 'Ĺ', '°', 'ë', 'Ŀ', '½', 'ë', 'ĵ', 'ľ', 'ë', '¦', '½', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'Ġ', 'ê', '°', 'Ĳ', 'ì', 'Ĥ¬', 'íķ', '©', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'Ġì', '¶', 'ķ', 'ì', 'Ĥ', '°', 'ì', 'Ĺ', 'ħ', 'Ġë', '¶', 'Ģ', 'ì', 'Ħ', 'ľ', 'ì', 'ŀ', 'ħ', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'Ġ', 'ê', '°', 'Ģ', 'ì', '¶', 'ķ', 'Ġë', '²', 'Ī', 'ì', 'ĭ', 'Ŀ', 'Ġì', 'Ħ', '¼', 'í', 'Ħ', '°', 'ì', 'Ĺ', 'Ĳ', 'ì', 'Ħ', 'ľ', 'Ġë', '°', 'ľ', 'ì', 'ĥ', 'Ŀ', 'íķ', 'ľ', 'Ġì', '¶', 'ķ', 'ì', 'Ĥ', '°', 'Ġ', 'í', 'ı', 'Ĳ', 'ê', '¸', '°', 'ë', '¬¼', 'Ġë', 'ĥ', 'Ħ', 'ì', 'ĥ', 'Ī', 'Ġë', '¬', '¸', 'ì', 'ł', 'ľ', 'ì', 'Ļ', 'Ģ', 'Ġ', 'ê', '´', 'Ģ', 'ë', 'ł', '¨', 'íķ', '´', 'Ġì', 'ĥ', 'ģ', 'ëĭ', '´', 'Ġë', 'ĵ', 'ľ', 'ë', '¦', '¬', 'ê', '²', 'ł', 'ì', 'Ĭ', 'µ', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'Ġì', 'ł', 'Ģ', 'í', 'Ŀ', '¬', 'Ġ', 'ê', '°', 'Ģ', 'ì', '¶', 'ķ', 'Ġë', '²', 'Ī', 'ì', 'ĭ', 'Ŀ', 'Ġì', 'Ħ', '¼', 'í', 'Ħ', '°', 'ì', 'Ĺ', 'Ĳ', 'ì', 'Ħ', 'ľ', 'Ġ2', 'ì', 'Ľ', 'Ķ', 'Ġ4', 'ìĿ', '¼', 'Ġë', '°', '¤', 'Ġ10', 'ì', 'ĭ', 'ľ', 'ì', 'Ĺ', 'Ĳ', 'Ġë', '°', 'ľ', 'ì', 'ĥ', 'Ŀ', 'íķ', 'ľ', 'Ġì', 'ŀ', 'ĳ', 'ë', '¬¼', 'Ġì', 'ĺ', '¤', 'ì', 'Ĺ', '¼', 'ë', '¡', 'ľ', 'Ġì', 'Ŀ', '¸', 'íķ', '´', 'Ġë', 'Ī', 'Ī', 'Ġì', 'ŀ', 'Ĳ', 'ê', '·', '¹', 'ê', '°', 'Ģ', 'Ġë', '°', 'ľ', 'ì', 'ĥ', 'Ŀ', 'Ġì', '¤', 'ĳ', 'ì', 'ŀ', 'ħ', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'Ġ', 'í', 'ĺ', 'Ħ', 'ì', 'ŀ', '¬', 'Ġ', 'ê', '°', 'Ģ', 'ì', '¶', 'ķ', 'Ġë', '²', 'Ī', 'ì', 'ĭ', 'Ŀ', 'Ġì', 'Ħ', '¼', 'í', 'Ħ', '°', 'ì', 'Ĺ', 'Ĳ', 'ì', 'Ħ', 'ľ', 'Ġë', '°', 'ľ', 'ì', 'ĥ', 'Ŀ', 'íķ', 'ľ', 'Ġì', 'ŀ', 'ĳ', 'ë', '¬¼', 'Ġì', 'ĺ', '¤', 'ì', 'Ĺ', '¼', 'Ġë', '¬', '¸', 'ì', 'ł', 'ľ', 'ì', 'Ĺ', 'Ĳ', 'Ġë', 'Į', 'Ģ', 'íķ', '´', 'Ġì', '¡', '°', 'ì', '¹', 'ĺ', 'Ġì', '¤', 'ĳ', 'ì', 'ŀ', 'ħ', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'Ġ', 'Ġ', 'Ġ', 'Ġ2', 'ì', 'Ľ', 'Ķ', 'Ġ4', 'ìĿ', '¼', 'Ġë', '°', '¤', 'Ġ10', 'ì', 'ĭ', 'ľ', 'ì', 'Ĺ', 'Ĳ', 'Ġì', 'Ļ', 'Ħ', 'ë', '£', 'Į', 'ë', 'Ĳ', 'ł', 'Ġì', 'ĺ', 'Ī', 'ì', 'ł', 'ķ', 'ì', 'ŀ', 'ħ', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'Ġ', 'ê', '·', '¸', 'ë', 'Ł', '¼', 'Ġë', '¬', '¸', 'ì', 'ł', 'ľ', 'ë', '¥', '¼', 'Ġë', '¹', '¨', 'ë', '¦', '¬', 'Ġ', 'íķ', '´', 'ê', '²', '°', 'íķ', '´', 'Ġì', '£', '¼', 'ì', 'Ħ', '¸', 'ì', 'ļ', 'Ķ', '.', 'Ġ', 'í', 'Ļ', 'ķ', 'ìĿ', '¸', 'Ġ', 'í', 'Ľ', 'Ħ', 'Ġì', '²', 'ĺ', 'ë', '¦', '¬', 'íķ', 'ĺ', 'ë', 'ı', 'Ħ', 'ë', '¡', 'Ŀ', 'Ġ', 'íķ', 'ĺ', 'ê', '²', 'ł', 'ì', 'Ĭ', 'µ', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'Ġì', '¢', 'ĭ', 'ìĿ', 'Ģ', 'Ġ', 'íķ', 'ĺ', 'ë', '£', '¨', 'Ġë', 'Ĳ', 'ĺ', 'ì', 'Ħ', '¸', 'ì', 'ļ', 'Ķ', '.', 'Ġë', 'Ħ', '¤', ',', 'Ġì', '²', 'ĺ', 'ë', '¦', '¬', 'Ġ', 'í', 'Ľ', 'Ħ', 'Ġë', 'ĭ', '¤', 'ì', 'ĭ', 'ľ', 'Ġì', 'Ĺ', '°', 'ë', 'Ŀ', '½', 'ë', 'ĵ', 'ľ', 'ë', '¦', '¬', 'ê', '²', 'ł', 'ì', 'Ĭ', 'µ', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'Ġ', 'ê', '°', 'Ĳ', 'ì', 'Ĥ¬', 'íķ', '©', 'ëĭ', 'Ī', 'ëĭ', '¤', '.']\n",
      "원래 문장:  저희 가축 번식 센터에서 2월 4일 밤 10시에 작물 오염로 인해 불편을 겪고 있어 연락드립니다. 감사합니다. 축산업 부서입니다. 가축 번식 센터에서 발생한 축산 폐기물 냄새 문제와 관련해 상담 드리겠습니다. 저희 가축 번식 센터에서 2월 4일 밤 10시에 발생한 작물 오염로 인해 눈 자극가 발생 중입니다. 현재 가축 번식 센터에서 발생한 작물 오염 문제에 대해 조치 중입니다.    2월 4일 밤 10시에 완료될 예정입니다. 그럼 문제를 빨리 해결해 주세요. 확인 후 처리하도록 하겠습니다. 좋은 하루 되세요. 네, 처리 후 다시 연락드리겠습니다. 감사합니다.\n",
      "location키워드 추출: 저희 가축 번식 센터에\n",
      "\n",
      "토큰화된 텍스트: ['í', 'Ĥ', '¤', 'ì', 'Ľ', 'Į', 'ë', 'ĵ', 'ľ', 'Ġì', '¶', 'Ķ', 'ì', '¶', 'ľ', ':', 'Ġì', 'ł', 'Ģ', 'í', 'Ŀ', '¬', 'Ġ', 'ê', '°', 'Ģ', 'ì', '¶', 'ķ', 'Ġë', '²', 'Ī', 'ì', 'ĭ', 'Ŀ', 'Ġì', 'Ħ', '¼', 'í', 'Ħ', '°', 'ì', 'Ĺ', 'Ĳ', 'ì', 'Ħ', 'ľ', 'Ġ2', 'ì', 'Ľ', 'Ķ', 'Ġ4', 'ìĿ', '¼', 'Ġë', '°', '¤', 'Ġ10', 'ì', 'ĭ', 'ľ', 'ì', 'Ĺ', 'Ĳ', 'Ġì', 'ŀ', 'ĳ', 'ë', '¬¼', 'Ġì', 'ĺ', '¤', 'ì', 'Ĺ', '¼', 'ë', '¡', 'ľ', 'Ġì', 'Ŀ', '¸', 'íķ', '´', 'Ġë', '¶', 'Ī', 'í', 'İ', '¸', 'ìĿ', 'Ħ', 'Ġ', 'ê', '²', 'ª', 'ê', '³', 'ł', 'Ġì', 'ŀ', 'Ī', 'ì', 'ĸ', '´', 'Ġì', 'Ĺ', '°', 'ë', 'Ŀ', '½', 'ë', 'ĵ', 'ľ', 'ë', '¦', '½', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'Ġ', 'ê', '°', 'Ĳ', 'ì', 'Ĥ¬', 'íķ', '©', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'Ġì', '¶', 'ķ', 'ì', 'Ĥ', '°', 'ì', 'Ĺ', 'ħ', 'Ġë', '¶', 'Ģ', 'ì', 'Ħ', 'ľ', 'ì', 'ŀ', 'ħ', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'Ġ', 'ê', '°', 'Ģ', 'ì', '¶', 'ķ', 'Ġë', '²', 'Ī', 'ì', 'ĭ', 'Ŀ', 'Ġì', 'Ħ', '¼', 'í', 'Ħ', '°', 'ì', 'Ĺ', 'Ĳ', 'ì', 'Ħ', 'ľ', 'Ġë', '°', 'ľ', 'ì', 'ĥ', 'Ŀ', 'íķ', 'ľ', 'Ġì', '¶', 'ķ', 'ì', 'Ĥ', '°', 'Ġ', 'í', 'ı', 'Ĳ', 'ê', '¸', '°', 'ë', '¬¼', 'Ġë', 'ĥ', 'Ħ', 'ì', 'ĥ', 'Ī', 'Ġë', '¬', '¸', 'ì', 'ł', 'ľ', 'ì', 'Ļ', 'Ģ', 'Ġ', 'ê', '´', 'Ģ', 'ë', 'ł', '¨', 'íķ', '´', 'Ġì', 'ĥ', 'ģ', 'ëĭ', '´', 'Ġë', 'ĵ', 'ľ', 'ë', '¦', '¬', 'ê', '²', 'ł', 'ì', 'Ĭ', 'µ', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'Ġì', 'ł', 'Ģ', 'í', 'Ŀ', '¬', 'Ġ', 'ê', '°', 'Ģ', 'ì', '¶', 'ķ', 'Ġë', '²', 'Ī', 'ì', 'ĭ', 'Ŀ', 'Ġì', 'Ħ', '¼', 'í', 'Ħ', '°', 'ì', 'Ĺ', 'Ĳ', 'ì', 'Ħ', 'ľ', 'Ġ2', 'ì', 'Ľ', 'Ķ', 'Ġ4', 'ìĿ', '¼', 'Ġë', '°', '¤', 'Ġ10', 'ì', 'ĭ', 'ľ', 'ì', 'Ĺ', 'Ĳ', 'Ġë', '°', 'ľ', 'ì', 'ĥ', 'Ŀ', 'íķ', 'ľ', 'Ġì', 'ŀ', 'ĳ', 'ë', '¬¼', 'Ġì', 'ĺ', '¤', 'ì', 'Ĺ', '¼', 'ë', '¡', 'ľ', 'Ġì', 'Ŀ', '¸', 'íķ', '´', 'Ġë', 'Ī', 'Ī', 'Ġì', 'ŀ', 'Ĳ', 'ê', '·', '¹', 'ê', '°', 'Ģ', 'Ġë', '°', 'ľ', 'ì', 'ĥ', 'Ŀ', 'Ġì', '¤', 'ĳ', 'ì', 'ŀ', 'ħ', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'Ġ', 'í', 'ĺ', 'Ħ', 'ì', 'ŀ', '¬', 'Ġ', 'ê', '°', 'Ģ', 'ì', '¶', 'ķ', 'Ġë', '²', 'Ī', 'ì', 'ĭ', 'Ŀ', 'Ġì', 'Ħ', '¼', 'í', 'Ħ', '°', 'ì', 'Ĺ', 'Ĳ', 'ì', 'Ħ', 'ľ', 'Ġë', '°', 'ľ', 'ì', 'ĥ', 'Ŀ', 'íķ', 'ľ', 'Ġì', 'ŀ', 'ĳ', 'ë', '¬¼', 'Ġì', 'ĺ', '¤', 'ì', 'Ĺ', '¼', 'Ġë', '¬', '¸', 'ì', 'ł', 'ľ', 'ì', 'Ĺ', 'Ĳ', 'Ġë', 'Į', 'Ģ', 'íķ', '´', 'Ġì', '¡', '°', 'ì', '¹', 'ĺ', 'Ġì', '¤', 'ĳ', 'ì', 'ŀ', 'ħ', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'Ġ', 'Ġ', 'Ġ', 'Ġ2', 'ì', 'Ľ', 'Ķ', 'Ġ4', 'ìĿ', '¼', 'Ġë', '°', '¤', 'Ġ10', 'ì', 'ĭ', 'ľ', 'ì', 'Ĺ', 'Ĳ', 'Ġì', 'Ļ', 'Ħ', 'ë', '£', 'Į', 'ë', 'Ĳ', 'ł', 'Ġì', 'ĺ', 'Ī', 'ì', 'ł', 'ķ', 'ì', 'ŀ', 'ħ', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'Ġ', 'ê', '·', '¸', 'ë', 'Ł', '¼', 'Ġë', '¬', '¸', 'ì', 'ł', 'ľ', 'ë', '¥', '¼', 'Ġë', '¹', '¨', 'ë', '¦', '¬', 'Ġ', 'íķ', '´', 'ê', '²', '°', 'íķ', '´', 'Ġì', '£', '¼', 'ì', 'Ħ', '¸', 'ì', 'ļ', 'Ķ', '.', 'Ġ', 'í', 'Ļ', 'ķ', 'ìĿ', '¸', 'Ġ', 'í', 'Ľ', 'Ħ', 'Ġì', '²', 'ĺ', 'ë', '¦', '¬', 'íķ', 'ĺ', 'ë', 'ı', 'Ħ', 'ë', '¡', 'Ŀ', 'Ġ', 'íķ', 'ĺ', 'ê', '²', 'ł', 'ì', 'Ĭ', 'µ', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'Ġì', '¢', 'ĭ', 'ìĿ', 'Ģ', 'Ġ', 'íķ', 'ĺ', 'ë', '£', '¨', 'Ġë', 'Ĳ', 'ĺ', 'ì', 'Ħ', '¸', 'ì', 'ļ', 'Ķ', '.', 'Ġë', 'Ħ', '¤', ',', 'Ġì', '²', 'ĺ', 'ë', '¦', '¬', 'Ġ', 'í', 'Ľ', 'Ħ', 'Ġë', 'ĭ', '¤', 'ì', 'ĭ', 'ľ', 'Ġì', 'Ĺ', '°', 'ë', 'Ŀ', '½', 'ë', 'ĵ', 'ľ', 'ë', '¦', '¬', 'ê', '²', 'ł', 'ì', 'Ĭ', 'µ', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'Ġ', 'ê', '°', 'Ĳ', 'ì', 'Ĥ¬', 'íķ', '©', 'ëĭ', 'Ī', 'ëĭ', '¤', '.']\n",
      "원래 문장:  저희 가축 번식 센터에서 2월 4일 밤 10시에 작물 오염로 인해 불편을 겪고 있어 연락드립니다. 감사합니다. 축산업 부서입니다. 가축 번식 센터에서 발생한 축산 폐기물 냄새 문제와 관련해 상담 드리겠습니다. 저희 가축 번식 센터에서 2월 4일 밤 10시에 발생한 작물 오염로 인해 눈 자극가 발생 중입니다. 현재 가축 번식 센터에서 발생한 작물 오염 문제에 대해 조치 중입니다.    2월 4일 밤 10시에 완료될 예정입니다. 그럼 문제를 빨리 해결해 주세요. 확인 후 처리하도록 하겠습니다. 좋은 하루 되세요. 네, 처리 후 다시 연락드리겠습니다. 감사합니다.\n",
      "smell키워드 추출: 부패된 고기 냄새\n",
      "\n",
      "원래 문장: 저희 가축 번식 센터에서 2월 4일 밤 10시에 작물 오염로 인해 불편을 겪고 있어 연락드립니다. 감사합니다. 축산업 부서입니다. 가축 번식 센터에서 발생한 축산 폐기물 냄새 문제와 관련해 상담 드리겠습니다. 저희 가축 번식 센터에서 2월 4일 밤 10시에 발생한 작물 오염로 인해 눈 자극가 발생 중입니다. 현재 가축 번식 센터에서 발생한 작물 오염 문제에 대해 조치 중입니다.    2월 4일 밤 10시에 완료될 예정입니다. 그럼 문제를 빨리 해결해 주세요. 확인 후 처리하도록 하겠습니다. 좋은 하루 되세요. 네, 처리 후 다시 연락드리겠습니다. 감사합니다.\n",
      "time키워드 추출: ['2월 4일', '10시', '2월 4일', '10시', '2월 4일', '10시']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "def test_model(text, saved_model_path, type):\n",
    "\n",
    "    # 모델과 토크나이저 불러오기\n",
    "    tokenizer = AutoTokenizer.from_pretrained(saved_model_path, local_files_only=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(saved_model_path, local_files_only=True).to('cpu')\n",
    "    \n",
    "    tokenized_text = tokenizer.tokenize(f\"키워드 추출: {text}\")\n",
    "    print(\"토큰화된 텍스트:\", tokenized_text)\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        f\"키워드 추출: {text}\",\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=128,\n",
    "        truncation=True\n",
    "    ).to(\"cpu\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=128,\n",
    "        num_beams=5,\n",
    "        length_penalty=0.7,\n",
    "        repetition_penalty=1.2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"원래 문장: \", text)\n",
    "    print(f\"{type}키워드 추출: {result}\")\n",
    "    \n",
    "text = '저희 가축 번식 센터에서 2월 4일 밤 10시에 작물 오염로 인해 불편을 겪고 있어 연락드립니다. 감사합니다. 축산업 부서입니다. 가축 번식 센터에서 발생한 축산 폐기물 냄새 문제와 관련해 상담 드리겠습니다. 저희 가축 번식 센터에서 2월 4일 밤 10시에 발생한 작물 오염로 인해 눈 자극가 발생 중입니다. 현재 가축 번식 센터에서 발생한 작물 오염 문제에 대해 조치 중입니다.\\\n",
    "    2월 4일 밤 10시에 완료될 예정입니다. 그럼 문제를 빨리 해결해 주세요. 확인 후 처리하도록 하겠습니다. 좋은 하루 되세요. 네, 처리 후 다시 연락드리겠습니다. 감사합니다.'\n",
    "        \n",
    "test_model(text, '/home/yjtech/Desktop/LLM/Pre_processing/smell_keyword/location_models/best_model_epoch_7', 'location')\n",
    "\n",
    "print()\n",
    "\n",
    "test_model(text, '/home/yjtech/Desktop/LLM/Pre_processing/smell_keyword/smell_models/best_model_epoch_14', 'smell')\n",
    "\n",
    "print()\n",
    "\n",
    "print(f\"원래 문장: {text}\")\n",
    "print(f\"time키워드 추출: {extract_dates_and_times(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text       여보세요? 산업용 배터리 공장에서 9월 18일 오전 11시부터 12시에 가스 폭발 ...\n",
       "keyword                                             기체 혼합 냄새\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_smell_df.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text       여보세요? 사무실 건물 후문에서 11월 2일에 악취 배출 통제 문제로 인해 문의 드...\n",
       "keyword                                            사무실 건물 후문\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_location_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text       안녕하세요. 지하철 환승 구간에서 1월 23일 저녁 7시에 공기 질 저하로 인해 피...\n",
       "keyword                                            지하철 환승 구간\n",
       "Name: 999, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_location_df.iloc[999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text       저희 골목길 식당가에서 1월 21일 오전 10시부터 11시에 심각한 생활 환경 문제...\n",
       "keyword                                              자극적인 냄새\n",
       "Name: 999, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_smell_df.iloc[999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화된 텍스트: ['í', 'Ĥ', '¤', 'ì', 'Ľ', 'Į', 'ë', 'ĵ', 'ľ', 'Ġì', '¶', 'Ķ', 'ì', '¶', 'ľ', ':', 'Ġì', 'ķ', 'Ī', 'ë', 'ħ', 'ķ', 'íķ', 'ĺ', 'ì', 'Ħ', '¸', 'ì', 'ļ', 'Ķ', '.', 'Ġë', 'ĭ', '¤', 'ë', '¦', '¬', 'Ġ', 'ê', '±', '´', 'ì', 'Ħ', '¤', 'Ġ', 'í', 'ĺ', 'Ħ', 'ì', 'ŀ', '¥', 'ì', 'Ĺ', 'Ĳ', 'ì', 'Ħ', 'ľ', 'Ġ2', 'ì', 'Ľ', 'Ķ', 'Ġ14', 'ìĿ', '¼', 'Ġì', 'ĥ', 'Ī', 'ë', '²', '½', 'Ġ4', 'ì', 'ĭ', 'ľ', 'ì', 'Ĺ', 'Ĳ', 'Ġ', 'ê', '³', 'µ', 'ì', 'Ĥ¬', 'Ġì', '§', 'Ħ', 'í', 'ĸ', 'ī', 'Ġì', '¤', 'ĳ', 'Ġ', 'í', 'Ļ', 'ĺ', 'ê', '²', '½', 'Ġì', 'ĺ', '¤', 'ì', 'Ĺ', '¼', 'ë', '¡', 'ľ', 'Ġì', 'Ŀ', '¸', 'íķ', '´', 'Ġ', 'í', 'Ķ', '¼', 'íķ', '´', 'ë', '¥', '¼', 'Ġì', 'ŀ', 'ħ', 'ê', '³', 'ł', 'Ġì', 'ŀ', 'Ī', 'ì', 'ĸ', '´', 'Ġì', 'Ĺ', '°', 'ë', 'Ŀ', '½', 'ë', 'ĵ', 'ľ', 'ë', '¦', '½', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'Ġì', 'ķ', 'Ī', 'ë', 'ħ', 'ķ', 'íķ', 'ĺ', 'ì', 'Ħ', '¸', 'ì', 'ļ', 'Ķ', '.', 'Ġë', 'ĭ', '¤', 'ë', '¦', '¬', 'Ġ', 'ê', '±', '´', 'ì', 'Ħ', '¤', 'Ġ', 'í', 'ĺ', 'Ħ', 'ì', 'ŀ', '¥', 'ìĿ', 'ĺ', 'Ġ2', 'ì', 'Ľ', 'Ķ', 'Ġ14', 'ìĿ', '¼', 'Ġì', 'ĥ', 'Ī', 'ë', '²', '½', 'Ġ4', 'ì', 'ĭ', 'ľ', 'Ġì', 'ĥ', 'ģ', 'í', 'Ļ', '©', 'ìĿ', 'Ħ', 'Ġ', 'í', 'Ļ', 'ķ', 'ìĿ', '¸', 'íķ', 'ĺ', 'ê', '²', 'ł', 'ì', 'Ĭ', 'µ', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'Ġë', 'ĭ', '¤', 'ë', '¦', '¬', 'Ġ', 'ê', '±', '´', 'ì', 'Ħ', '¤', 'Ġ', 'í', 'ĺ', 'Ħ', 'ì', 'ŀ', '¥', 'ì', 'Ĺ', 'Ĳ', 'ì', 'Ħ', 'ľ', 'Ġ2', 'ì', 'Ľ', 'Ķ', 'Ġ14', 'ìĿ', '¼', 'Ġì', 'ĥ', 'Ī', 'ë', '²', '½', 'Ġ4', 'ì', 'ĭ', 'ľ', 'ì', 'Ĺ', 'Ĳ', 'Ġ', 'í', 'Ķ', 'Į', 'ë', 'Ŀ', '¼', 'ì', 'Ĭ', '¤', 'í', 'ĭ', '±', 'Ġì', 'ļ', '©', 'ì', 'ľ', 'µ', 'Ġë', 'ĥ', 'Ħ', 'ì', 'ĥ', 'Ī', 'ìĿ', '´', 'Ġì', 'Ħ', 'ľ', 'ì', 'Ħ', 'ľ', 'í', 'ŀ', 'Ī', 'Ġì', 'Ĭ', '¤', 'ë', '©', '°', 'ë', 'ĵ', 'ľ', 'ë', 'Ĭ', 'Ķ', 'Ġë', 'Ĥ', 'ĺ', 'ê', '³', 'ł', 'Ġì', 'ŀ', 'Ī', 'ì', 'ľ', '¼', 'ë', '©', '°', ',', 'Ġ', 'í', 'Ķ', '¼', 'ë', '¡', 'ľ', 'ê', '°', 'Ĳ', 'ë', '¡', 'ľ', 'Ġì', 'ĥ', 'Ŀ', 'í', 'Ļ', 'ľ', 'ì', 'Ĺ', 'Ĳ', 'Ġë', '¶', 'Ī', 'í', 'İ', '¸', 'ìĿ', '´', 'Ġë', '§', 'İ', 'ì', 'Ĭ', 'µ', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'Ġë', 'ĭ', '¤', 'ë', '¦', '¬', 'Ġ', 'ê', '±', '´', 'ì', 'Ħ', '¤', 'Ġ', 'í', 'ĺ', 'Ħ', 'ì', 'ŀ', '¥', 'ì', 'Ĺ', 'Ĳ', 'ì', 'Ħ', 'ľ', 'Ġë', '°', 'ľ', 'ì', 'ĥ', 'Ŀ', 'íķ', 'ľ', 'Ġ', 'ê', '³', 'µ', 'ì', 'Ĥ¬', 'Ġì', '§', 'Ħ', 'í', 'ĸ', 'ī', 'Ġì', '¤', 'ĳ', 'Ġ', 'í', 'Ļ', 'ĺ', 'ê', '²', '½', 'Ġì', 'ĺ', '¤', 'ì', 'Ĺ', '¼', 'Ġë', '¬', '¸', 'ì', 'ł', 'ľ', 'ì', 'Ĺ', 'Ĳ', 'Ġë', 'Į', 'Ģ', 'íķ', '´', 'Ġ2', 'ì', 'Ľ', 'Ķ', 'Ġ14', 'ìĿ', '¼', 'Ġì', 'ĥ', 'Ī', 'ë', '²', '½', 'Ġ4', 'ì', 'ĭ', 'ľ', 'ì', 'Ĺ', 'Ĳ', 'Ġì', '¦', 'ī', 'ì', 'ĭ', 'ľ', 'Ġì', 'ł', 'Ĳ', 'ê', '²', 'Ģ', 'ìĿ', 'Ħ', 'Ġì', 'ĭ', 'ľ', 'ì', 'ŀ', 'ĳ', 'íķ', 'ĺ', 'ê', '²', 'ł', 'ì', 'Ĭ', 'µ', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'Ġì', 'ķ', 'Į', 'ê', '²', 'ł', 'ì', 'Ĭ', 'µ', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'Ġë', '¹', 'ł', 'ë', '¥', '¸', 'Ġì', '²', 'ĺ', 'ë', '¦', '¬', 'Ġë', '¶', 'Ģ', 'í', 'ĥ', 'ģ', 'ë', 'ĵ', 'ľ', 'ë', '¦', '½', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'Ġì', 'ķ', 'Į', 'ê', '²', 'ł', 'ì', 'Ĭ', 'µ', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'Ġì', 'µ', 'ľ', 'ë', 'Į', 'Ģ', 'íķ', 'ľ', 'Ġë', '¹', 'ł', 'ë', '¥', '´', 'ê', '²', 'Į', 'Ġì', '²', 'ĺ', 'ë', '¦', '¬', 'íķ', 'ĺ', 'ê', '²', 'ł', 'ì', 'Ĭ', 'µ', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'Ġì', 'Ī', 'ĺ', 'ê', '³', 'ł', 'Ġë', '§', 'İ', 'ì', 'ľ', '¼', 'ì', 'ĭ', 'Ń', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'Ġ', 'ê', '°', 'Ĳ', 'ì', 'Ĥ¬', 'íķ', '©', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'Ġë', '¹', 'ł', 'ë', '¥', '´', 'ê', '²', 'Į', 'Ġì', '²', 'ĺ', 'ë', '¦', '¬', 'íķ', 'ĺ', 'ê', '²', 'ł', 'ì', 'Ĭ', 'µ', 'ëĭ', 'Ī', 'ëĭ', '¤', '.']\n",
      "원래 문장:  안녕하세요. 다리 건설 현장에서 2월 14일 새벽 4시에 공사 진행 중 환경 오염로 인해 피해를 입고 있어 연락드립니다. 안녕하세요. 다리 건설 현장의 2월 14일 새벽 4시 상황을 확인하겠습니다. 다리 건설 현장에서 2월 14일 새벽 4시에 플라스틱 용융 냄새이 서서히 스며드는 나고 있으며, 피로감로 생활에 불편이 많습니다. 다리 건설 현장에서 발생한 공사 진행 중 환경 오염 문제에 대해 2월 14일 새벽 4시에 즉시 점검을 시작하겠습니다. 알겠습니다. 빠른 처리 부탁드립니다. 알겠습니다. 최대한 빠르게 처리하겠습니다. 수고 많으십니다. 감사합니다. 빠르게 처리하겠습니다.\n",
      "smell키워드 추출: 부패된 고기 냄새\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = train_smell_df.iloc[0][0]\n",
    "# text = \"오늘 오전 9시 버스 정류장 가는 길에 음식물 쓰레기 냄새가 발생하였습니다. 빨리 처리해주세요.\"\n",
    "# text = '안녕하세요. 포항시 오천읍에 거주하는 이강일입니다.  저는 지난 11월 13일 이후로 집에서 나오는 수돗물에서 고약한 냄새  가 나기 시작했습니다. 처음에는 아파트 물탱크의 문제가  있는 줄 알고 대수롭지 않게 여 겠는데 냄새가 계속 나서 걱정이  돼서 신고합니다. 현재 수돗물에서 흙냄새와 곰팡이  냄새가 나고 그래서 설거지나 세수 는 물론 물을 마실 수가 없습니다.  생수로 대체해서 사용하고 있는데 이 문제로 매우 불편을 겪고 있습니다.  포항시 수돗물 원수의 40%를 공급 하는 경주 안개 때문에서 녹조  현상이 발생했다고 들었고 남조류 에서 발생한 지오스민이 냄새를  유발한다고 합니다. 이 문제에 대한 빠른 대응을 부탁  드립니다. 감사합니다.'\n",
    "\n",
    "      \n",
    "test_model(text, '/home/yjtech/Desktop/LLM/Pre_processing/smell_keyword/smell_models/best_model_epoch_14', 'smell')\n",
    "print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv38-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
