{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 복합 키워드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "mecab = Mecab()\n",
    "\n",
    "\n",
    "def extract_dynamic_keywords(text, category):\n",
    "    \"\"\"\n",
    "    문장에서 복합 키워드를 순차적으로 결합하며 추출.\n",
    "    \"\"\"\n",
    "    # 형태소 분석\n",
    "    pos_tags = mecab.pos(text)\n",
    "    extracted_keywords = [word for word, tag in pos_tags if tag in [\"NNG\", \"NNP\"]]\n",
    "\n",
    "    # 카테고리 키워드 집합 생성\n",
    "    category_keywords = set(\n",
    "        categories[category][\"locations\"] +\n",
    "        categories[category][\"smell_types\"] +\n",
    "        categories[category][\"intensity_adjectives\"]\n",
    "    )\n",
    "\n",
    "    # 복합 키워드 처리\n",
    "    final_keywords = []\n",
    "    temp_combination = []\n",
    "\n",
    "    for i, word in enumerate(extracted_keywords):\n",
    "        temp_combination.append(word)  # 단어를 임시 결합 리스트에 추가\n",
    "        combined = \" \".join(temp_combination)  # 결합된 문자열 생성\n",
    "\n",
    "        if combined in category_keywords:  # 결합 결과가 카테고리 키워드에 있으면\n",
    "            final_keywords.append(combined)  # 키워드로 추가\n",
    "            temp_combination = []  # 임시 결합 초기화\n",
    "        elif i == len(extracted_keywords) - 1:  # 마지막 단어 처리\n",
    "            # 마지막 단어까지 결합이 없으면 개별 단어 추가\n",
    "            final_keywords.extend(temp_combination)\n",
    "\n",
    "    # 문장 길이에 따라 동적 키워드 수 결정\n",
    "    total_words = len(pos_tags)\n",
    "    num_keywords = max(1, int(total_words * 0.5))\n",
    "\n",
    "    return final_keywords[:num_keywords]  # 키워드 제한 적용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['방음벽 공사 현장', '화학', '물', '냄새', '발생']\n"
     ]
    }
   ],
   "source": [
    "categories = {\n",
    "    \"odor\": {\n",
    "        \"locations\": [\"방음벽 공사 현장\", \"공장\"],\n",
    "        \"smell_types\": [\"화학물 냄새\", \"음식물 쓰레기 냄새\"],\n",
    "        \"intensity_adjectives\": [\"심각하게\", \"조용히\"],\n",
    "        \"effects\": [\"건강에 악영향을 미칩니다.\", \"불쾌감을 유발합니다.\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "text = \"방음벽 공사 현장에서 화학물 냄새가 조용히 발생하고 있습니다.\"\n",
    "\n",
    "# 테스트 실행\n",
    "keywords = extract_dynamic_keywords(text, \"odor\")\n",
    "print(keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 띄어쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['방음벽 공사 현장', '화학물 냄새', '조용히']\n"
     ]
    }
   ],
   "source": [
    "def extract_keywords_by_space(text, category):\n",
    "    \"\"\"\n",
    "    띄어쓰기를 기준으로 복합 키워드를 추출하며 조사를 제거.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    # 문장을 띄어쓰기 기준으로 분리\n",
    "    words = text.split()\n",
    "\n",
    "    # 조사 목록 (필요에 따라 추가)\n",
    "    particles = [\"에서\", \"으로\", \"에\", \"의\", \"을\", \"를\", \"이\", \"가\", \"과\", \"와\", \"도\", \"는\", \"다\", \"만\"]\n",
    "\n",
    "    # 단어에서 조사를 제거\n",
    "    def remove_particle(word):\n",
    "        for particle in particles:\n",
    "            if word.endswith(particle):\n",
    "                return re.sub(f\"{particle}$\", \"\", word)  # 조사 제거\n",
    "        return word\n",
    "\n",
    "    # 카테고리 키워드 목록 (집합으로 변환)\n",
    "    category_keywords = set(\n",
    "        categories[category][\"locations\"] +\n",
    "        categories[category][\"smell_types\"] +\n",
    "        categories[category][\"intensity_adjectives\"]\n",
    "    )\n",
    "\n",
    "    # 복합 키워드 처리\n",
    "    final_keywords = []\n",
    "    temp_combination = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        clean_word = remove_particle(words[i])  # 단어에서 조사 제거\n",
    "        temp_combination.append(clean_word)  # 단어 추가\n",
    "        combined = \" \".join(temp_combination)  # 결합된 문자열 생성\n",
    "\n",
    "        if combined in category_keywords:  # 결합 결과가 정확히 일치하면\n",
    "            final_keywords.append(combined)  # 키워드로 저장\n",
    "            temp_combination = []  # 임시 결합 초기화\n",
    "        elif i == len(words) - 1:  # 마지막 단어 처리\n",
    "            # temp_combination이 키워드와 매칭되지 않으면 버림\n",
    "            temp_combination = []  # 초기화\n",
    "        i += 1  # 다음 단어로 이동\n",
    "\n",
    "    return final_keywords\n",
    "\n",
    "\n",
    "categories = {\n",
    "    \"odor\": {\n",
    "        \"locations\": [\"방음벽 공사 현장\", \"공장\"],\n",
    "        \"smell_types\": [\"화학물 냄새\", \"음식물 쓰레기 냄새\"],\n",
    "        \"intensity_adjectives\": [\"심각하게\", \"조용히\"],\n",
    "        \"effects\": [\"건강에 악영향을 미칩니다.\", \"불쾌감을 유발합니다.\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "text = \"방음벽 공사 현장에서 화학물 냄새가 조용히 발생하고 있습니다.\"\n",
    "\n",
    "# 테스트 실행\n",
    "keywords = extract_keywords_by_space(text, \"odor\")\n",
    "print(keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('쓰레기', 'NNG'), ('매립장', 'NNG'), ('에서', 'JKB'), ('악취', 'NNG'), ('가', 'JKS'), ('쾌쾌하', 'VA'), ('게', 'EC'), ('발생', 'NNG'), ('하', 'XSV'), ('고', 'EC'), ('있', 'VX'), ('습니다', 'EF'), ('.', 'SF'), ('이', 'NP'), ('로', 'JKB'), ('인해', 'VV+EC'), ('주민', 'NNG'), ('들', 'XSN'), ('의', 'JKG'), ('일상', 'NNG'), ('생활', 'NNG'), ('에', 'JKB'), ('심각', 'XR'), ('한', 'XSA+ETM'), ('지장', 'NNG'), ('을', 'JKO'), ('초래', 'NNG'), ('하', 'XSV'), ('고', 'EC'), ('있', 'VX'), ('으며', 'EC'), (',', 'SC'), ('특히', 'MAG'), ('아이', 'NNG'), ('들', 'XSN'), ('과', 'JC'), ('노약자', 'NNG'), ('들', 'XSN'), ('의', 'JKG'), ('건강', 'NNG'), ('이', 'JKS'), ('우려', 'NNG'), ('됩니다', 'XSA+EC')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "mecab = Mecab()\n",
    "\n",
    "print(mecab.pos('쓰레기 매립장에서 악취가 쾌쾌하게 발생하고 있습니다. 이로 인해 주민들의 일상생활에 심각한 지장을 초래하고 있으며, 특히 아이들과 노약자들의 건강이 우려됩니다'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['쾌쾌하게', '비린내', '아파트 공사장']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "mecab = Mecab()\n",
    "\n",
    "def extract_keywords_with_ngram(text, category):\n",
    "    \"\"\"\n",
    "    문장에서 N-gram 방식으로 키워드를 추출.\n",
    "    \"\"\"\n",
    "    # 형태소 분석\n",
    "    pos_tags = mecab.pos(text)\n",
    "\n",
    "    # 필요한 품사 추출 및 결합\n",
    "    words = []\n",
    "    temp_word = \"\"\n",
    "    skip_next = False\n",
    "\n",
    "    for i, (word, tag) in enumerate(pos_tags):\n",
    "        if skip_next:  # 이미 결합 처리한 경우\n",
    "            skip_next = False\n",
    "            continue\n",
    "\n",
    "        # 'XR' + 'XSA' 패턴 결합 처리\n",
    "        if tag == \"XR\" and i + 1 < len(pos_tags) and pos_tags[i + 1][1] == \"XSA\":\n",
    "            temp_word = word + pos_tags[i + 1][0]  # 어근 + 접사 결합\n",
    "            if i + 2 < len(pos_tags) and pos_tags[i + 2][1] == \"EC\":  # 연결 어미까지 포함\n",
    "                temp_word += pos_tags[i + 2][0]\n",
    "                skip_next = True  # 이미 처리한 어미 건너뛰기\n",
    "            words.append(temp_word)\n",
    "            skip_next = True  # 접사 건너뛰기\n",
    "            temp_word = \"\"\n",
    "        elif tag in [\"NNG\", \"NNP\", \"MAG\", \"VA\"]:  # 명사, 부사, 형용사 추가\n",
    "            words.append(word)\n",
    "\n",
    "    # 카테고리 키워드 집합\n",
    "    category_keywords = set(\n",
    "        categories[category][\"locations\"] +\n",
    "        categories[category][\"smell_types\"] +\n",
    "        categories[category][\"intensity_adjectives\"]\n",
    "    )\n",
    "\n",
    "    # N-gram으로 키워드 매칭\n",
    "    final_keywords = []\n",
    "    for n in range(1, len(words) + 1):  # 1-gram부터 전체 길이까지\n",
    "        for i in range(len(words) - n + 1):\n",
    "            ngram = \" \".join(words[i:i + n])  # N-gram 생성\n",
    "            if ngram in category_keywords:  # 키워드 매칭\n",
    "                final_keywords.append(ngram)\n",
    "\n",
    "    # 중복 제거 및 순서 유지\n",
    "    final_keywords = list(dict.fromkeys(final_keywords))\n",
    "\n",
    "    return final_keywords\n",
    "\n",
    "# 테스트 데이터\n",
    "categories = {\n",
    "    \"odor\": {\n",
    "        \"locations\": [\"아파트 공사장\", \"하수구\"],\n",
    "        \"smell_types\": [\"비린내\", \"하수구 냄새\"],\n",
    "        \"intensity_adjectives\": [\"쾌쾌하게\", \"축축하게\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "text = \"아파트 공사장에서 쾌쾌하게 비린내가 발생하고 있습니다.\"\n",
    "\n",
    "# 실행\n",
    "keywords = extract_keywords_with_ngram(text, \"odor\")\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### komoran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('쓰레기', 'NNP'), ('매립장', 'NNG'), ('에서', 'JKB'), ('쾌', 'NNG'), ('쾌', 'NNG'), ('하', 'XSV'), ('게', 'EC'), ('악취', 'NNP'), ('가', 'JKS'), ('발생', 'NNG'), ('하', 'XSV'), ('고', 'EC'), ('있', 'VX'), ('습니다', 'EF'), ('.', 'SF')]\n",
      "\n",
      "[('쓰레기', 'NNP'), ('매립장', 'NNG'), ('에서', 'JKB'), ('악취', 'NNP'), ('가', 'JKS'), ('쾌', 'NNG'), ('쾌', 'NNG'), ('하', 'XSV'), ('게', 'EC'), ('발생', 'NNG'), ('하', 'XSV'), ('고', 'EC'), ('있', 'VX'), ('습니다', 'EF'), ('.', 'SF'), ('이', 'NP'), ('로', 'JKB'), ('인해', 'NNP'), ('주민', 'NNG'), ('들', 'XSN'), ('의', 'JKG'), ('일상생활', 'NNG'), ('에', 'JKB'), ('심각', 'XR'), ('하', 'XSA'), ('ㄴ', 'ETM'), ('지장', 'NNG'), ('을', 'JKO'), ('초래', 'NNG'), ('하', 'XSV'), ('고', 'EC'), ('있', 'VV'), ('으며', 'EC'), (',', 'SP'), ('특히', 'MAG'), ('아이들', 'NNP'), ('과', 'JC'), ('노약자', 'NNG'), ('들', 'XSN'), ('의', 'JKG'), ('건강', 'NNG'), ('이', 'JKS'), ('우려', 'NNG'), ('되', 'XSV'), ('ㅂ니다', 'EC')]\n",
      "\n",
      "[('쓰레기', 'NNP'), ('매립장', 'NNG'), ('에서', 'JKB'), ('지독', 'XR'), ('하', 'XSA'), ('게', 'EC'), ('악취', 'NNP'), ('가', 'JKS'), ('발생', 'NNG'), ('하', 'XSV'), ('고', 'EC'), ('있', 'VX'), ('습니다', 'EF'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()\n",
    "\n",
    "print(komoran.pos('쓰레기 매립장에서 쾌쾌하게 악취가 발생하고 있습니다.'))\n",
    "print()\n",
    "print(komoran.pos('쓰레기 매립장에서 악취가 쾌쾌하게 발생하고 있습니다. 이로 인해 주민들의 일상생활에 심각한 지장을 초래하고 있으며, 특히 아이들과 노약자들의 건강이 우려됩니다'))\n",
    "print()\n",
    "print(komoran.pos('쓰레기 매립장에서 지독하게 악취가 발생하고 있습니다.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('쓰레기', 'NNP'), ('매립장', 'NNG'), ('에서', 'JKB'), ('고약', 'XR'), ('하', 'XSA'), ('ㄴ', 'ETM'), ('냄새', 'NNG'), ('가', 'JKS'), ('자욱', 'XR'), ('하', 'XSA'), ('게', 'EC'), ('발생', 'NNG'), ('하', 'XSV'), ('고', 'EC'), ('있', 'VX'), ('습니다', 'EF'), ('.', 'SF'), ('이', 'MM'), ('문제', 'NNG'), ('로', 'JKB'), ('인해', 'NNP'), ('주민', 'NNG'), ('들', 'XSN'), ('이', 'JKS'), ('집', 'NNG'), ('을', 'JKO'), ('떠나', 'VV'), ('아', 'EC'), ('이사', 'NNG'), ('를', 'JKO'), ('고민', 'NNG'), ('하', 'XSV'), ('고', 'EC'), ('있', 'VV'), ('습니다', 'EC')] \n",
      "\n",
      "[('쓰레기', 'NNP'), ('매립장', 'NNG'), ('에서', 'JKB'), ('고약', 'XR'), ('하', 'XSA'), ('ㄴ', 'ETM'), ('냄새', 'NNG'), ('가', 'JKS'), ('자욱', 'XR'), ('하', 'XSA'), ('게', 'EC'), ('발생', 'NNG'), ('하', 'XSV'), ('고', 'EC'), ('있', 'VX'), ('습니다', 'EF'), ('.', 'SF'), ('악취', 'NNP'), ('로', 'JKB'), ('인해', 'NNP'), ('외부', 'NNG'), ('에서', 'JKB'), ('오', 'VV'), ('ㄴ', 'ETM'), ('방문객', 'NNG'), ('들', 'XSN'), ('이', 'JKS'), ('심각', 'XR'), ('하', 'XSA'), ('ㄴ', 'ETM'), ('불쾌감', 'NNG'), ('을', 'JKO'), ('느끼', 'VV'), ('고', 'EC'), ('있', 'VV'), ('습니다', 'EC')] \n",
      "\n",
      "[('쓰레기', 'NNP'), ('매립장', 'NNG'), ('에서', 'JKB'), ('고약', 'XR'), ('하', 'XSA'), ('ㄴ', 'ETM'), ('냄새', 'NNG'), ('가', 'JKS'), ('자욱', 'XR'), ('하', 'XSA'), ('게', 'EC'), ('발생', 'NNG'), ('하', 'XSV'), ('고', 'EC'), ('있', 'VX'), ('습니다', 'EF'), ('.', 'SF'), ('지역', 'NNG'), ('행사', 'NNG'), ('와', 'JC'), ('축제', 'NNG'), ('에', 'JKB'), ('도', 'JX'), ('부정', 'NNG'), ('적', 'XSN'), ('이', 'VCP'), ('ㄴ', 'ETM'), ('영향', 'NNG'), ('을', 'JKO'), ('미치', 'VV'), ('고', 'EC'), ('있', 'VV'), ('습니다', 'EC')] \n",
      "\n",
      "[('쓰레기', 'NNP'), ('매립장', 'NNG'), ('에서', 'JKB'), ('고약', 'XR'), ('하', 'XSA'), ('ㄴ', 'ETM'), ('냄새', 'NNG'), ('가', 'JKS'), ('자욱', 'XR'), ('하', 'XSA'), ('게', 'EC'), ('발생', 'NNG'), ('하', 'XSV'), ('고', 'EC'), ('있', 'VX'), ('습니다', 'EF'), ('.', 'SF'), ('이', 'MM'), ('문제', 'NNG'), ('는', 'JX'), ('더', 'MAG'), ('이상', 'NNG'), ('방치', 'NNG'), ('하', 'XSV'), ('ㄹ', 'ETM'), ('수', 'NNB'), ('없', 'VA'), ('는', 'ETM'), ('심각', 'XR'), ('하', 'XSA'), ('ㄴ', 'ETM'), ('상태', 'NNG'), ('에', 'JKB'), ('이르', 'VV'), ('었', 'EP'), ('습니다', 'EC')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_lst = ['쓰레기 매립장에서 고약한 냄새가 자욱하게 발생하고 있습니다. 이 문제로 인해 주민들이 집을 떠나 이사를 고민하고 있습니다',\n",
    "            '쓰레기 매립장에서 고약한 냄새가 자욱하게 발생하고 있습니다. 악취로 인해 외부에서 온 방문객들이 심각한 불쾌감을 느끼고 있습니다',\n",
    "            '쓰레기 매립장에서 고약한 냄새가 자욱하게 발생하고 있습니다. 지역 행사와 축제에도 부정적인 영향을 미치고 있습니다',\n",
    "            '쓰레기 매립장에서 고약한 냄새가 자욱하게 발생하고 있습니다. 이 문제는 더 이상 방치할 수 없는 심각한 상태에 이르렀습니다']\n",
    "\n",
    "for text in text_lst:\n",
    "    print(komoran.pos(text), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장: 쓰레기 매립장에서 지독하게 악취가 발생하고 있습니다.\n",
      "키워드: ['악취', '지독', '매립장', '쓰레기 매립장']\n",
      "\n",
      "문장: 고약한 냄새로 인해 주민들이 집을 떠나 이사를 고민하고 있습니다.\n",
      "키워드: ['고약', '떠나', '고민', '이사']\n",
      "\n",
      "문장: 매립장의 자욱한 비린내가 인근 지역에 심각한 문제를 일으키고 있습니다.\n",
      "키워드: ['문제', '비린내', '자욱', '심각', '매립장']\n",
      "\n",
      "문장: 방음벽 공사 현장에서 연소된 화학물 냄새가 조용히 스며들게 발생하고 있습니다. 이 문제는 지역 전체의 건강과 안전을 위협하고 있습니다\n",
      "키워드: ['문제', '화학물 냄새', '조용', '방음벽 공사 현장']\n",
      "\n",
      "문장: 쓰레기 매립장에서 매캐한 냄새가 강압적으로 발생하고 있습니다. 이 문제로 인해 주변 부동산 가격이 하락하고 있습니다\n",
      "키워드: ['강압적으로', '문제', '매캐한', '쓰레기 매립장', '매립장']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Komoran\n",
    "import re\n",
    "\n",
    "class AdvancedKoreanKeywordExtractor:\n",
    "    def __init__(self, categories):\n",
    "        self.komoran = Komoran()\n",
    "        self.categories = categories\n",
    "        \n",
    "        # 기본 키워드 확장 패턴\n",
    "        self.keyword_expansion_patterns = {\n",
    "            # 강도/감정 표현 확장\n",
    "            'intensity_adjectives': [\n",
    "                r'^(고약|자욱|엄청|매우|너무|정말|몹시|극도로|특히).*',\n",
    "                r'.*(?:하다|스럽다|롭다|덥다)$',\n",
    "                r'.*(?:게|히)$'\n",
    "            ],\n",
    "            # 문제 상황 키워드 확장\n",
    "            'problem_keywords': [\n",
    "                r'.*(?:문제|이사|고민|어려움|불편).*',\n",
    "                r'(?:떠나|포기|힘들|불가능).*'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "    def _normalize_word(self, word, tag):\n",
    "        \"\"\"\n",
    "        단어 정규화 및 확장 로직\n",
    "        \"\"\"\n",
    "        # 형용사, 부사 관련 정규화\n",
    "        if tag in ['VA', 'MAG', 'XSV', 'XSA', 'XR']:\n",
    "            word = re.sub(r'(하|스럽|롭|덥)다?$', '', word)\n",
    "            word = re.sub(r'(게|히)$', '', word)\n",
    "        \n",
    "        return word\n",
    "\n",
    "    def _match_expanded_patterns(self, word, pattern_type):\n",
    "        \"\"\"\n",
    "        확장 패턴 매칭\n",
    "        \"\"\"\n",
    "        for pattern in self.keyword_expansion_patterns.get(pattern_type, []):\n",
    "            if re.match(pattern, word):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def extract_keywords(self, text, category):\n",
    "        \"\"\"\n",
    "        고급 키워드 추출 메서드\n",
    "        \"\"\"\n",
    "        # 형태소 분석\n",
    "        pos_tags = self.komoran.pos(text)\n",
    "        \n",
    "        # 카테고리 키워드 집합\n",
    "        category_keywords = {\n",
    "            key: set(values) \n",
    "            for key, values in self.categories[category].items()\n",
    "        }\n",
    "        \n",
    "        # 추출 대상 품사 확장\n",
    "        target_tags = [\"NNG\", \"NNP\", \"VA\", \"MAG\", \"XSV\", \"XSA\", \"XR\", \"MM\"]\n",
    "        \n",
    "        # 키워드 추출 결과 저장\n",
    "        final_keywords = set()\n",
    "        \n",
    "        # 1. 직접 키워드 매칭\n",
    "        for word, tag in pos_tags:\n",
    "            if tag in target_tags:\n",
    "                normalized_word = self._normalize_word(word, tag)\n",
    "                \n",
    "                # 각 카테고리별 키워드 매칭\n",
    "                for category_type, keywords in category_keywords.items():\n",
    "                    if normalized_word in keywords:\n",
    "                        final_keywords.add(normalized_word)\n",
    "                    \n",
    "                    # 확장 패턴 매칭\n",
    "                    if category_type == 'intensity_adjectives':\n",
    "                        if self._match_expanded_patterns(word, 'intensity_adjectives'):\n",
    "                            final_keywords.add(normalized_word)\n",
    "        \n",
    "        # 2. N-gram 키워드 추출\n",
    "        words = [word for word, tag in pos_tags if tag in target_tags]\n",
    "        for n in range(2, min(4, len(words) + 1)):\n",
    "            for i in range(len(words) - n + 1):\n",
    "                ngram = \" \".join(words[i:i + n])\n",
    "                for keywords in category_keywords.values():\n",
    "                    if ngram in keywords:\n",
    "                        final_keywords.add(ngram)\n",
    "        \n",
    "        # 3. 문제 상황 키워드 추출\n",
    "        problem_keywords = []\n",
    "        for word, tag in pos_tags:\n",
    "            if self._match_expanded_patterns(word, 'problem_keywords'):\n",
    "                problem_keywords.append(word)\n",
    "        \n",
    "        if problem_keywords:\n",
    "            final_keywords.update(problem_keywords)\n",
    "        \n",
    "        # 4. 부분 일치 및 컨텍스트 기반 키워드\n",
    "        for keyword_list in category_keywords.values():\n",
    "            for keyword in keyword_list:\n",
    "                if keyword in text:\n",
    "                    final_keywords.add(keyword)\n",
    "        \n",
    "        return list(final_keywords)\n",
    "\n",
    "# 카테고리 확장\n",
    "categories = {\n",
    "    \"odor\": {\n",
    "        \"locations\": [\"쓰레기 매립장\", \"매립장\", \"쓰레기장\", \"방음벽 공사 현장\"],\n",
    "        \"smell_types\": [\n",
    "            \"악취\", \"비린내\", \"썩은내\", \"고약한내\", \n",
    "            \"지독한냄새\", \"역겨운냄새\", '화학물 냄새', '매캐한'\n",
    "        ],\n",
    "        \"intensity_adjectives\": [\n",
    "            \"쾌쾌\", \"쾌쾌하다\", \"심각\", \"지독\", \n",
    "            \"지독하다\", \"심각하다\", \"역겹다\", \n",
    "            \"강렬하다\", \"고약하다\", \"자욱하다\", \"조용하게\", '강압적으로'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# 사용 예시\n",
    "extractor = AdvancedKoreanKeywordExtractor(categories)\n",
    "\n",
    "# 테스트 문장들\n",
    "test_sentences = [\n",
    "    \"쓰레기 매립장에서 지독하게 악취가 발생하고 있습니다.\",\n",
    "    \"고약한 냄새로 인해 주민들이 집을 떠나 이사를 고민하고 있습니다.\",\n",
    "    \"매립장의 자욱한 비린내가 인근 지역에 심각한 문제를 일으키고 있습니다.\",\n",
    "    '방음벽 공사 현장에서 연소된 화학물 냄새가 조용히 스며들게 발생하고 있습니다. 이 문제는 지역 전체의 건강과 안전을 위협하고 있습니다',\n",
    "    '쓰레기 매립장에서 매캐한 냄새가 강압적으로 발생하고 있습니다. 이 문제로 인해 주변 부동산 가격이 하락하고 있습니다'\n",
    "]\n",
    "\n",
    "# 각 문장에 대해 키워드 추출\n",
    "for sentence in test_sentences:\n",
    "    print(f\"문장: {sentence}\")\n",
    "    keywords = extractor.extract_keywords(sentence, \"odor\")\n",
    "    print(f\"키워드: {keywords}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 머신러닝 & 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec 모델 로드 성공!\n",
      "문장: 쓰레기 매립장에서 지독하게 악취가 발생하고 있습니다.\n",
      "형태소 분석 결과: [('쓰레기', 'NNP'), ('매립장', 'NNG'), ('에서', 'JKB'), ('지독', 'XR'), ('하', 'XSA'), ('게', 'EC'), ('악취', 'NNP'), ('가', 'JKS'), ('발생', 'NNG'), ('하', 'XSV'), ('고', 'EC'), ('있', 'VX'), ('습니다', 'EF'), ('.', 'SF')]\n",
      "키워드: ['쓰레기 매립장', '매립장', '쓰레기장', '악취', '지독하게', '심각하다', '강렬하다', '고약하다']\n",
      "\n",
      "문장: 고약한 냄새로 인해 주민들이 집을 떠나 이사를 고민하고 있습니다.\n",
      "형태소 분석 결과: [('고약', 'XR'), ('하', 'XSA'), ('ㄴ', 'ETM'), ('냄새', 'NNG'), ('로', 'JKB'), ('인해', 'NNP'), ('주민', 'NNG'), ('들', 'XSN'), ('이', 'JKS'), ('집', 'NNG'), ('을', 'JKO'), ('떠나', 'VV'), ('아', 'EC'), ('이사', 'NNG'), ('를', 'JKO'), ('고민', 'NNG'), ('하', 'XSV'), ('고', 'EC'), ('있', 'VX'), ('습니다', 'EF'), ('.', 'SF')]\n",
      "키워드: ['지독한냄새', '역겨운냄새', '지독하게', '심각하다', '강렬하다', '고약하다']\n",
      "\n",
      "문장: 매립장의 자욱한 비린내가 인근 지역에 심각한 문제를 일으키고 있습니다.\n",
      "형태소 분석 결과: [('매립장', 'NNG'), ('의', 'JKG'), ('자욱', 'XR'), ('하', 'XSA'), ('ㄴ', 'ETM'), ('비린내', 'NNG'), ('가', 'JKS'), ('인근', 'NNG'), ('지역', 'NNG'), ('에', 'JKB'), ('심각', 'XR'), ('하', 'XSA'), ('ㄴ', 'ETM'), ('문제', 'NNG'), ('를', 'JKO'), ('일으키', 'VV'), ('고', 'EC'), ('있', 'VX'), ('습니다', 'EF'), ('.', 'SF')]\n",
      "키워드: ['쓰레기 매립장', '매립장', '비린내', '지독하게', '심각하다', '강렬하다', '고약하다']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate, GlobalMaxPooling1D\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from konlpy.tag import Komoran\n",
    "import re\n",
    "import json\n",
    "import faiss\n",
    "\n",
    "class SemanticKeywordExtractor:\n",
    "    def __init__(self, categories, embedding_dim=100, max_words=1000):\n",
    "        self.komoran = Komoran()\n",
    "        self.categories = categories\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_words = max_words\n",
    "            \n",
    "        # 한국어 Word2Vec 사전 학습 모델 (예시 경로, 실제 경로로 변경 필요)\n",
    "        self.word2vec_model = self._load_word2vec_model()\n",
    "        \n",
    "        # 벡터 크기 확인 (디버깅용)\n",
    "        # if self.word2vec_model:\n",
    "        #     for word in self.word2vec_model.index_to_key:\n",
    "        #         print(f\"Word: {word}, Vector Shape: {self.word2vec_model[word].shape}\")\n",
    "        \n",
    "        # 카테고리 벡터 생성\n",
    "        self.category_vectors = self._create_category_vectors()\n",
    "        \n",
    "        # Faiss 인덱스 생성 (고속 유사도 검색)\n",
    "        self.faiss_index = self._build_faiss_index()\n",
    "        \n",
    "        # 딥러닝 키워드 분류기 모델\n",
    "        self.keyword_classifier = self._build_keyword_classifier()\n",
    "\n",
    "    def _load_word2vec_model(self):\n",
    "        \"\"\"\n",
    "        Word2Vec 또는 FastText 모델 로드 함수\n",
    "        \"\"\"\n",
    "        try:\n",
    "            path = '/home/yjtech2/Desktop/yurim/LLM/Pre_processing/smell_keyword/kor_w2v'\n",
    "            model = KeyedVectors.load_word2vec_format(path, binary=False)\n",
    "            print(\"Word2Vec 모델 로드 성공!\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"모델 로드 실패: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _create_category_vectors(self):\n",
    "        category_vectors = {}\n",
    "        for category, keywords in self.categories.items():\n",
    "            category_vector = []\n",
    "            for keyword_list in keywords.values():\n",
    "                for keyword in keyword_list:\n",
    "                    # 단어 벡터를 가져오거나 없는 경우 0 벡터 사용\n",
    "                    keyword_vectors = [\n",
    "                        self.word2vec_model[word] if word in self.word2vec_model else np.zeros(self.embedding_dim)\n",
    "                        for word in keyword.split()\n",
    "                    ]\n",
    "                    \n",
    "                    # keyword_vectors가 비어있지 않고, 벡터 크기가 모두 동일한 경우 처리\n",
    "                    if keyword_vectors and all(vec.shape == (self.embedding_dim,) for vec in keyword_vectors):\n",
    "                        category_vector.append(np.mean(keyword_vectors, axis=0))\n",
    "\n",
    "            # category_vector가 비어있지 않으면 평균을 저장, 그렇지 않으면 0 벡터 저장\n",
    "            if category_vector:\n",
    "                category_vectors[category] = np.mean(category_vector, axis=0)\n",
    "            else:\n",
    "                category_vectors[category] = np.zeros(self.embedding_dim)\n",
    "        return category_vectors\n",
    "\n",
    "\n",
    "\n",
    "    def _build_faiss_index(self):\n",
    "        \"\"\"\n",
    "        Faiss 인덱스 생성 (고속 유사도 검색)\n",
    "        \"\"\"\n",
    "        # 카테고리 벡터로 Faiss 인덱스 생성\n",
    "        category_matrix = np.array(list(self.category_vectors.values()))\n",
    "        dimension = category_matrix.shape[1]\n",
    "        \n",
    "        # L2 거리 기반 인덱스 생성\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(category_matrix)\n",
    "        \n",
    "        return index\n",
    "\n",
    "    def _build_keyword_classifier(self):\n",
    "        \"\"\"\n",
    "        딥러닝 키워드 분류기 모델 구축\n",
    "        \"\"\"\n",
    "        # 입력 레이어\n",
    "        input_layer = Input(shape=(None,))\n",
    "        \n",
    "        # 임베딩 레이어\n",
    "        embedding_layer = Embedding(\n",
    "            input_dim=self.max_words, \n",
    "            output_dim=self.embedding_dim, \n",
    "            mask_zero=True\n",
    "        )(input_layer)\n",
    "        \n",
    "        # LSTM 레이어\n",
    "        lstm_layer = LSTM(64, return_sequences=True)(embedding_layer)\n",
    "        \n",
    "        # 글로벌 맥스 풀링\n",
    "        pooling_layer = GlobalMaxPooling1D()(lstm_layer)\n",
    "        \n",
    "        # 다중 레이블 분류 출력 레이어\n",
    "        output_layer = Dense(\n",
    "            len(self.categories), \n",
    "            activation='sigmoid'\n",
    "        )(pooling_layer)\n",
    "        \n",
    "        model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        model.compile(\n",
    "            optimizer='adam', \n",
    "            loss='binary_crossentropy', \n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        텍스트 전처리\n",
    "        \"\"\"\n",
    "        # Komoran 형태소 분석\n",
    "        pos_tags = self.komoran.pos(text)\n",
    "        print(f\"형태소 분석 결과: {pos_tags}\")  # 디버깅용 출력\n",
    "\n",
    "        # 명사, 형용사, 부사 추출\n",
    "        keywords = [\n",
    "            word for word, tag in pos_tags \n",
    "            if tag in ['NNG', 'NNP', 'VA', 'MAG', 'XSA']\n",
    "        ]\n",
    "        return keywords\n",
    "\n",
    "\n",
    "    def semantic_similarity(self, text_keywords):\n",
    "        \"\"\"\n",
    "        의미론적 유사성 분석\n",
    "        \"\"\"\n",
    "        # 키워드 벡터 변환\n",
    "        keyword_vectors = [\n",
    "            self.word2vec_model[keyword] if keyword in self.word2vec_model else np.zeros(self.embedding_dim)\n",
    "            for keyword in text_keywords\n",
    "        ]\n",
    "        \n",
    "        # 카테고리와의 유사도 계산\n",
    "        similarities = {}\n",
    "        for category, category_vector in self.category_vectors.items():\n",
    "            # 키워드 벡터와 카테고리 벡터 간 코사인 유사도 계산\n",
    "            category_similarities = [\n",
    "                cosine_similarity([vec], [category_vector])[0][0]\n",
    "                for vec in keyword_vectors\n",
    "            ]\n",
    "            similarities[category] = np.mean(category_similarities)\n",
    "        \n",
    "        return similarities\n",
    "\n",
    "    def extract_keywords(self, text, top_k=10):\n",
    "        \"\"\"\n",
    "        키워드 추출 메인 메서드 (형태 유사성 기반)\n",
    "        \"\"\"\n",
    "        # 텍스트 전처리: 문장에서 명사, 형용사, 부사를 추출\n",
    "        keywords = self.preprocess_text(text)\n",
    "        \n",
    "        # 카테고리별 키워드 필터링 (형태 유사성 포함)\n",
    "        matched_keywords = []\n",
    "        for category, keyword_types in self.categories.items():\n",
    "            for keyword_list in keyword_types.values():\n",
    "                for kw in keyword_list:\n",
    "                    # 문장 키워드 중에서 부분 문자열 포함 또는 접두/접미가 유사한 경우 추출\n",
    "                    for word in keywords:\n",
    "                        if kw in word or word in kw:\n",
    "                            matched_keywords.append(kw)\n",
    "        \n",
    "        # 중복 제거 및 상위 K개 키워드 반환\n",
    "        matched_keywords = list(dict.fromkeys(matched_keywords))  # 중복 제거\n",
    "        return matched_keywords[:top_k]  # 상위 K개 키워드 반환\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 카테고리 정의\n",
    "categories = {\n",
    "    \"odor\": {\n",
    "        \"locations\": [\"쓰레기 매립장\", \"매립장\", \"쓰레기장\"],\n",
    "        \"smell_types\": [\n",
    "            \"악취\", \"비린내\", \"썩은내\", \"고약한내\", \n",
    "            \"지독한냄새\", \"역겨운냄새\"\n",
    "        ],\n",
    "        \"intensity_adjectives\": [\n",
    "            \"쾌쾌\", \"심각\", \"지독하게\", \n",
    "            \"심각하다\", \"역겹다\", \n",
    "            \"강렬하다\", \"고약하다\", '자욱한'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# 키워드 추출기 초기화\n",
    "extractor = SemanticKeywordExtractor(categories, embedding_dim=150)\n",
    "\n",
    "# 테스트 문장들\n",
    "test_sentences = [\n",
    "    \"쓰레기 매립장에서 지독하게 악취가 발생하고 있습니다.\",\n",
    "    \"고약한 냄새로 인해 주민들이 집을 떠나 이사를 고민하고 있습니다.\",\n",
    "    \"매립장의 자욱한 비린내가 인근 지역에 심각한 문제를 일으키고 있습니다.\"\n",
    "]\n",
    "\n",
    "# 각 문장에 대해 키워드 추출\n",
    "for sentence in test_sentences:\n",
    "    print(f\"문장: {sentence}\")\n",
    "    keywords = extractor.extract_keywords(sentence)\n",
    "    print(f\"키워드: {keywords}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('매립장', 'NNG'), ('의', 'JKG'), ('자욱', 'XR'), ('한', 'XSA+ETM'), ('비린내', 'NNG'), ('가', 'JKS'), ('인근', 'NNG'), ('지역', 'NNG'), ('에', 'JKB'), ('심각', 'XR'), ('한', 'XSA+ETM'), ('문제', 'NNG'), ('를', 'JKO'), ('일으키', 'VV'), ('고', 'EC'), ('있', 'VX'), ('습니다', 'EF'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "mecab = Mecab()\n",
    "print(mecab.pos('매립장의 자욱한 비린내가 인근 지역에 심각한 문제를 일으키고 있습니다.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
