{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "    \"쓰레기 관련\": {\n",
    "        \"locations\": [\n",
    "            \"쓰레기 매립장\", \"쓰레기 처리장\", \"재활용 센터\", \"음식물 쓰레기장\", \"불법 폐기물 처리장\",\n",
    "            \"폐기물 소각장\", \"주변 골목길\", \"공동 주택 쓰레기장\", \"아파트 단지 쓰레기장\", \"학교 근처 쓰레기장\",\n",
    "            \"병원 쓰레기장\", \"공원 주변 쓰레기장\", \"도로변 쓰레기장\", \"지하철 역 주변\", \"대형마트 쓰레기장\",\n",
    "            \"버스 정류장 근처\", \"도시 외곽 쓰레기장\", \"항만 주변\", \"농촌 지역 쓰레기장\", \"도시 중심가 쓰레기장\",\n",
    "            \"포항시 북구\", \"포항시 남구\", \"포스코\", \"포항항\", \"포항대학교\",\n",
    "            \"포항 철강 산업단지\", \"포항 주변 해안\", \"포항시내\", \"포항 공원\", \"포항 해상\",\n",
    "            \"포항역\", \"포항 근처 산\", \"포항 해수욕장\", \"포항시 외곽\", \"포항 근처 도로\",\n",
    "            \"포항 남부 해안도로\", \"포항 북부 해안도로\", \"포항 근교 농촌\", \"포항 일대 농업지역\", \"포항 주요 광장\"\n",
    "        ],\n",
    "\n",
    "        \"smell_types\": [\n",
    "            \"악취\", \"화학 냄새\", \"부패한 냄새\", \"쓰레기 냄새\", \"찌든 냄새\", \"타는 냄새\", \"고약한 냄새\", \n",
    "            \"불쾌한 냄새\", \"탄 냄새\", \"침투성 냄새\", \"메스꺼운 냄새\", \"가스 냄새\", \"산성 냄새\", \"날카로운 냄새\", \n",
    "            \"오염된 냄새\", \"지속적으로 나는 냄새\", \"폭발적 악취\", \"썩은 물질 냄새\", \"퇴비 냄새\", \"더러운 냄새\"\n",
    "        ],\n",
    "\n",
    "        \"intensity_adjectives\": [\n",
    "            \"쾌쾌하게\", \"심하게\", \"지속적으로\", \"불쾌하게\", \"찌들게\", \"강하게\", \"독하게\", \"자극적으로\", \n",
    "            \"비린내처럼\", \"지독하게\", \"매캐하게\", \"묵직하게\", \"축축하게\", \"역하게\", \"찌르는 듯이\", \"강렬하게\", \n",
    "            \"폭발적으로\", \"극도로\", \"심각하게\", \"거슬리게\"\n",
    "        ],\n",
    "\n",
    "        \"issues\": [\n",
    "            \"쓰레기 처리 미비\", \"불법 투기된 폐기물\", \"재활용 분리수거 미흡\", \"음식물 쓰레기 악취\", \"폐기물 소각으로 인한 매연\", \n",
    "            \"쓰레기 적체\", \"쓰레기차 통행 불편\", \"폐기물 배출 지연\", \"유해 가스 발생\", \"불법 폐기물 매립\", \n",
    "            \"쓰레기통 부족\", \"도로에 쓰레기 방치\", \"재활용 품목 혼합\", \"쓰레기장 인근 불쾌한 냄새\", \"쓰레기 처리 불완전\", \n",
    "            \"도로에 쓰레기 던져짐\", \"소각장 연기 발생\", \"고장난 재활용 기계\", \"쓰레기통 불법 방치\", \"분리배출 혼잡\"\n",
    "        ],\n",
    "\n",
    "        \"departments\": [\n",
    "            \"환경 관리팀\", \"청소 관리팀\", \"재활용 관리팀\", \"폐기물 처리팀\", \"소각장 운영팀\", \"대기 관리팀\", \n",
    "            \"도시 재활용팀\", \"환경 안전팀\", \"교통 관리팀\", \"환경 연구팀\", \"자원 회수팀\", \"공공 서비스팀\", \n",
    "            \"청소 공공팀\", \"환경 감시팀\", \"기후 변화 대응팀\", \"시설 운영팀\", \"도시 환경 관리팀\", \"폐기물 분석팀\", \n",
    "            \"오염 물질 처리팀\", \"재활용 기술팀\"\n",
    "        ],\n",
    "\n",
    "        \"effects\": [\n",
    "            \"생활 불편\", \"호흡기 문제\", \"냄새로 인한 불쾌감\", \"알레르기 증상\", \"건강에 해로움\", \"환경 오염\", \n",
    "            \"시민 불만\", \"민원 발생\", \"주변 환경 개선 필요\", \"지역 이미지 훼손\", \"공공 안전 문제\", \"도시 미관 저하\", \n",
    "            \"공기 질 악화\", \"교통 혼잡\", \"관광지 방문 감소\", \"쾌적한 환경 저하\", \"수면 방해\", \"민원 처리 지연\", \n",
    "            \"공공서비스 감소\", \"경제적 손실\"\n",
    "        ],\n",
    "        \"time_of_days\": [\n",
    "            \"아침\", \"오전\", \"낮\", \"오후\", \"저녁\", \"밤\", \"새벽\",\n",
    "            \"오전 6시부터 7시\", \"오전 9시부터 10시\", \"오후 1시부터 2시\", \n",
    "            \"오후 3시부터 4시\", \"저녁 6시부터 7시\", \"밤 10시부터 11시\", \n",
    "            \"새벽 2시부터 3시\"\n",
    "        ]\n",
    "     },\n",
    "\n",
    "\n",
    "\n",
    "    \"날씨 관련\": {\n",
    "        \"locations\": [\n",
    "            \"포항시 북구\", \"포항시 남구\", \"포스코\", \"포항항\", \"포항대학교\",\n",
    "            \"포항 철강 산업단지\", \"포항 주변 해안\", \"포항시내\", \"포항 공원\", \"포항 해상\",\n",
    "            \"포항역\", \"포항 근처 산\", \"포항 해수욕장\", \"포항시 외곽\", \"포항 근처 도로\",\n",
    "            \"포항 남부 해안도로\", \"포항 북부 해안도로\", \"포항 근교 농촌\", \"포항 일대 농업지역\", \"포항 주요 광장\"\n",
    "        ],\n",
    "\n",
    "        \"smell_types\": [\n",
    "            \"산성 냄새\", \"습한 냄새\", \"상쾌한 냄새\", \"짙은 미세먼지\", \"소나기 후 냄새\",\n",
    "            \"염분이 섞인 냄새\", \"포근한 바람 냄새\", \"비 오는 날 냄새\", \"진한 해풍 냄새\", \"우박 냄새\",\n",
    "            \"장마철 냄새\", \"선선한 냄새\", \"습기 섞인 냄새\", \"더운 날씨의 냄새\", \"뿌연 공기\",\n",
    "            \"자연의 냄새\", \"먼지 냄새\", \"미세먼지 냄새\", \"구름 냄새\", \"비 온 후 냄새\"\n",
    "        ],\n",
    "\n",
    "        \"intensity_adjectives\": [\n",
    "            \"강하게\", \"약하게\", \"지속적으로\", \"급격하게\", \"급히\", \"덥게\", \"차갑게\", \"어두운\", \n",
    "            \"차츰\", \"가끔\", \"심하게\", \"낮게\", \"매우\", \"불규칙하게\", \"짧게\", \"긴 시간 동안\", \"폭발적으로\", \n",
    "            \"차분하게\", \"점차적으로\", \"습기 찬\"\n",
    "        ],\n",
    "\n",
    "        \"issues\": [\n",
    "            \"갑작스러운 비\", \"폭염\", \"대설\", \"장마\", \"강풍\", \"미세먼지\", \"안개\", \"강한 비바람\", \n",
    "            \"황사\", \"온도 급변\", \"건조한 날씨\", \"태풍\", \"기온 차이\", \"눈보라\", \"뇌우\", \"열대야\", \n",
    "            \"저기압\", \"고기압\", \"풍속 증가\", \"불안정한 날씨\"\n",
    "        ],\n",
    "\n",
    "\n",
    "        \"departments\": [\n",
    "            \"기상 예보팀\", \"기후 연구팀\", \"미세먼지 대책팀\", \"기상 안전팀\", \"기후 변화 대응팀\", \n",
    "            \"대기 관리팀\", \"기상 기술팀\", \"폭염 대책팀\", \"기상 정보 제공팀\", \"강수량 관리팀\", \n",
    "            \"기후 모델링 팀\", \"태풍 연구팀\", \"날씨 분석팀\", \"기상 측정팀\", \"온도 측정팀\", \"기상 해석팀\", \n",
    "            \"대기 오염 관리팀\", \"기상 데이터 분석팀\", \"산악 기상팀\", \"기후 변화 예측팀\"\n",
    "        ],\n",
    "\n",
    "        \"effects\": [\n",
    "            \"교통 혼잡\", \"건강 문제\", \"미세먼지 영향\", \"농작물 피해\", \"산불 위험\", \"대기 오염 증가\", \n",
    "            \"집중력 저하\", \"눈길 사고\", \"체온 조절 어려움\", \"습기 문제\", \"기상 난민 발생\", \"수확량 감소\", \n",
    "            \"기후 변화로 인한 생활 불편\", \"폭염으로 인한 탈진\", \"강풍으로 인한 시설 피해\", \"눈사태 발생\", \n",
    "            \"시민 불편\", \"야외 활동 제한\", \"소방 활동 어려움\", \"기상 예보 오류\"\n",
    "        ],\n",
    "        \"time_of_days\": [\n",
    "            \"아침\", \"오전\", \"낮\", \"오후\", \"저녁\", \"밤\", \"새벽\",\n",
    "            \"오전 6시부터 7시\", \"오전 9시부터 10시\", \"오후 1시부터 2시\", \n",
    "            \"오후 3시부터 4시\", \"저녁 6시부터 7시\", \"밤 10시부터 11시\", \n",
    "            \"새벽 2시부터 3시\"\n",
    "        ]\n",
    "     },\n",
    "\n",
    "\n",
    "    \"공장 관련\": {\n",
    "        \"locations\": [\n",
    "            \"화학 공장\", \"플라스틱 제조 공장\", \"폐수 처리 공장\", \"섬유 공장\", \"금속 가공 공장\",\n",
    "            \"자동차 부품 제조 공장\", \"전자 부품 공장\", \"석유 정제 공장\", \"시멘트 공장\", \"비료 공장\",\n",
    "            \"고무 공장\", \"제지 공장\", \"가구 제조 공장\", \"유리 공장\", \"주조 공장\",\n",
    "            \"플라스틱 가공 공장\", \"코팅 공장\", \"제약 공장\", \"화장품 제조 공장\", \"고체 연료 공장\",\n",
    "            \"전자 폐기물 처리 공장\", \"아스팔트 공장\", \"타이어 제조 공장\", \"도료 제조 공장\", \"농약 공장\",\n",
    "            \"음료 제조 공장\", \"고속 인쇄 공장\", \"전력 생산 공장\", \"스틸 가공 공장\", \"세제 제조 공장\",\n",
    "            \"복합 소재 제조 공장\", \"케이블 제조 공장\", \"냉각 기기 제조 공장\", \"반도체 공장\", \"방직 공장\",\n",
    "            \"폐기물 소각 공장\", \"세라믹 제조 공장\", \"폴리머 공장\", \"석탄 연료 공장\", \"합성수지 공장\"\n",
    "        ],\n",
    "        \"smell_types\": [\n",
    "            \"화학약품 냄새\", \"타는 냄새\", \"매운 냄새\", \"오염된 물 냄새\", \"기름 냄새\",\n",
    "            \"산성 냄새\", \"날카로운 냄새\", \"지속적인 냄새\", \"부식 냄새\", \"찌든 냄새\",\n",
    "            \"유기물 냄새\", \"불쾌한 냄새\", \"탄 냄새\", \"매캐한 냄새\", \"고약한 냄새\",\n",
    "            \"찌르는 냄새\", \"폭발적인 악취\", \"가스 냄새\", \"비린내\", \"혼합된 화학 냄새\",\n",
    "            \"플라스틱 녹는 냄새\", \"고무 타는 냄새\", \"독성 화학물 냄새\", \"미세 먼지 냄새\", \"금속성 냄새\",\n",
    "            \"염료 냄새\", \"공기 중의 찌든 냄새\", \"방부제 냄새\", \"부패 냄새\", \"미묘한 가스 냄새\",\n",
    "            \"강한 접착제 냄새\", \"분말 화학 냄새\", \"오존 냄새\", \"소각된 폐기물 냄새\", \"연소 냄새\"\n",
    "        ],\n",
    "        \"intensity_adjectives\": [\n",
    "            \"습하게\", \"축축하게\", \"뜨겁게\", \"서늘하게\", \"쾌쾌하게\",\n",
    "            \"지독하게\", \"자극적으로\", \"비릿하게\", \"무겁게\", \"매캐하게\",\n",
    "            \"강하게\", \"불쾌하게\", \"날카롭게\", \"침투성 있게\", \"섞인 듯이\",\n",
    "            \"지속적으로\", \"심하게\", \"뿌옇게\", \"잔뜩\", \"독하게\",\n",
    "            \"질식할 듯이\", \"스산하게\", \"뚜렷하게\", \"가득히\", \"찌르는 듯이\",\n",
    "            \"폭발적으로\", \"스며들 듯이\", \"거슬리게\", \"끈질기게\", \"오랫동안 남아있게\",\n",
    "            \"혼탁하게\", \"무겁고 답답하게\", \"강렬하게\", \"끈적하게\", \"날카롭게 퍼지게\"\n",
    "        ],\n",
    "        \"effects\": [\n",
    "            \"호흡이 어려움\", \"불쾌감\", \"피로감\", \"두통\", \"심한 자극\", \n",
    "            \"산소 부족\", \"기운이 떨어짐\", \"눈이 따끔거림\", \"집중력이 떨어짐\", \"숨쉬기 힘듦\",\n",
    "            \"어지럼증\", \"메스꺼움\", \"고통스러움\", \"불쾌한 기분\", \"화학물질로 인한 부작용\",\n",
    "            \"기계 소리로 인한 스트레스\", \"피부 자극\", \"호흡기 질환 유발\", \"불안감\", \"우울한 기분\"\n",
    "        ],\n",
    "        \"issues\": [\n",
    "            \"화학물질 유출\", \"공기 오염\", \"소음 문제\", \"산업 폐기물 처리 문제\", \"고온 방출 문제\", \n",
    "            \"화학 가스 누출\", \"폐수 방류\", \"타는 냄새 발생\", \"연기 발생\", \"불법 배출\",\n",
    "            \"불완전 연소\", \"가스 폭발 위험\", \"기계 고장\", \"부주의로 인한 사고\", \"공기 질 악화\",\n",
    "            \"소음 공해\", \"환경 파괴\", \"미세먼지 발생\", \"기계 오작동\", \"산업 재해\"\n",
    "        ],\n",
    "        \"departments\": [\n",
    "            \"환경 관리부\", \"안전 관리부\", \"시설 관리부\", \"기계 설비 부서\", \"전기 관리부\", \n",
    "            \"화학 부서\", \"폐기물 처리 부서\", \"사고 예방 부서\", \"노동 안전 부서\", \"품질 관리 부서\",\n",
    "            \"기술 부서\", \"연구개발 부서\", \"운영 부서\", \"제조 부서\", \"정비 부서\",\n",
    "            \"에너지 관리 부서\", \"운송 부서\", \"공정 관리 부서\", \"위험물 관리 부서\", \"프로젝트 관리 부서\"\n",
    "        ],\n",
    "        \"time_of_days\": [\n",
    "            \"아침\", \"오전\", \"낮\", \"오후\", \"저녁\", \"밤\", \"새벽\",\n",
    "            \"오전 6시부터 7시\", \"오전 9시부터 10시\", \"오후 1시부터 2시\", \n",
    "            \"오후 3시부터 4시\", \"저녁 6시부터 7시\", \"밤 10시부터 11시\", \n",
    "            \"새벽 2시부터 3시\"\n",
    "        ]\n",
    "     },\n",
    "    \n",
    "    \"축산 관련\": {\n",
    "        \"locations\": [\n",
    "            \"축사\", \"가축 사육장\", \"도살장\", \"젖소 농장\", \"돼지 농장\",\n",
    "            \"닭 농장\", \"가금류 농장\", \"양 농장\", \"산란계 농장\", \"육계 농장\",\n",
    "            \"소 사육장\", \"염소 농장\", \"오리 농장\", \"타조 농장\", \"사료 공장\",\n",
    "            \"가축 폐기물 처리장\", \"동물 보호소\", \"동물 실험 시설\", \"농업 단지\", \"대규모 축산 시설\",\n",
    "            \"우시장\", \"작은 동물 농장\", \"방목된 가축 사육지\", \"계란 생산 농장\", \"유제품 가공 공장\",\n",
    "            \"축산 폐수 처리장\", \"말 농장\", \"사슴 농장\", \"낙타 농장\", \"산림지 축산 단지\",\n",
    "            \"농촌 지역 축사\", \"가축 퇴비 저장소\", \"야생동물 보호 구역\", \"동물 사육 체험장\", \"생축 경매장\",\n",
    "            \"가축 운송 시설\", \"가축 질병 격리 시설\", \"고지대 목장\", \"호수 인근 축사\", \"냄새 민감 지역 축사\"\n",
    "        ],\n",
    "        \"smell_types\": [\n",
    "            \"악취\", \"분뇨 냄새\", \"동물 배설물 냄새\", \"소 똥 냄새\", \"돼지 똥 냄새\",\n",
    "            \"새우 배설물 냄새\", \"시큼한 냄새\", \"퇴비 냄새\", \"동물 사체 냄새\", \"음식물 냄새\",\n",
    "            \"동물의 땀 냄새\", \"단백질 냄새\", \"육류 냄새\", \"비린 냄새\", \"불쾌한 냄새\",\n",
    "            \"새끼 동물 냄새\", \"소화불량 냄새\", \"기름 냄새\", \"가축 사료 냄새\", \"발효된 냄새\",\n",
    "            \"침전된 물질 냄새\", \"동물 사료 냄새\", \"자극적인 냄새\", \"축산물 가공 냄새\", \"석회 냄새\"\n",
    "        ],\n",
    "        \"intensity_adjectives\": [\n",
    "            \"강하게\", \"지속적으로\", \"심하게\", \"불쾌하게\", \"역하게\", \n",
    "            \"찌들게\", \"매캐하게\", \"매우\", \"강렬하게\", \"매우 강하게\", \n",
    "            \"거세게\", \"자욱하게\", \"잔뜩\", \"지독하게\", \"짙게\", \n",
    "            \"심각하게\", \"스며들 듯이\", \"찬물에 푹 담근 듯이\", \"집요하게\", \"끈질기게\"\n",
    "        ],\n",
    "        \"effects\": [\n",
    "            \"호흡 곤란\", \"메스꺼움\", \"불쾌감\", \"피로감\", \"두통\", \n",
    "            \"기운이 빠짐\", \"눈 자극\", \"어지러움\", \"냄새로 인한 스트레스\", \"심한 자극\", \n",
    "            \"구역질\", \"기분 나쁨\", \"화학적 반응\", \"심한 불쾌감\", \"호흡기 질환\", \n",
    "            \"피부 자극\", \"산소 부족\", \"집중력 저하\", \"호흡기 알레르기\", \"불안감\"\n",
    "        ],\n",
    "        \"issues\": [\n",
    "            \"동물 배설물 처리 문제\", \"악취 발생\", \"환경 오염\", \"위생 문제\", \"사료 관리 문제\",\n",
    "            \"동물 건강 문제\", \"동물 도살 문제\", \"가축 질병 발생\", \"미세먼지 발생\", \"불법 축산물 유통\",\n",
    "            \"동물 학대 문제\", \"동물 복지 문제\", \"축산 폐기물 처리 문제\", \"소음 문제\", \"분뇨 유출\",\n",
    "            \"해충 문제\", \"하수도 문제\", \"악취와 건강 문제\", \"작물 오염\", \"가축 재배 문제\"\n",
    "        ],\n",
    "        \"departments\": [\n",
    "            \"환경 관리부\", \"농업 부서\", \"축산업 부서\", \"식품 위생 부서\", \"동물 관리 부서\", \n",
    "            \"사료 관리 부서\", \"농업 안전 부서\", \"위생 관리 부서\", \"농업 시설 관리 부서\", \"질병 관리 부서\", \n",
    "            \"동물 복지 부서\", \"농업 교육 부서\", \"환경 보호 부서\", \"농업 연구 부서\", \"농장 안전 부서\", \n",
    "            \"소음 및 악취 관리 부서\", \"농업 생산 부서\", \"폐기물 처리 부서\", \"축산업 기술 부서\", \"수의학 부서\"\n",
    "        ],\n",
    "        \"time_of_days\": [\n",
    "            \"아침\", \"오전\", \"낮\", \"오후\", \"저녁\", \"밤\", \"새벽\",\n",
    "            \"오전 6시부터 7시\", \"오전 9시부터 10시\", \"오후 1시부터 2시\", \n",
    "            \"오후 3시부터 4시\", \"저녁 6시부터 7시\", \"밤 10시부터 11시\", \n",
    "            \"새벽 2시부터 3시\"\n",
    "        ]\n",
    "     },\n",
    " \n",
    "    \"생활 악취 관련\": {\n",
    "        \"locations\": [\n",
    "            \"주택가\", \"아파트 단지\", \"공동 주택\", \"주차장\", \"상가 근처\", \n",
    "            \"학교 주변\", \"병원 주변\", \"도시 근교\", \"공원\", \"놀이터\", \n",
    "            \"버스 정류장\", \"지하철 역\", \"길거리\", \"대형마트\", \"택시 승강장\", \n",
    "            \"레스토랑 주변\", \"카페 근처\", \"도심지\", \"주택가 골목길\", \"공공 화장실\"\n",
    "        ],\n",
    "        \"smell_types\": [\n",
    "            \"악취\", \"음식물 냄새\", \"쓰레기 냄새\", \"하수구 냄새\", \"화학 냄새\", \n",
    "            \"배수구 냄새\", \"쓰레기통 냄새\", \"담배 냄새\", \"곰팡이 냄새\", \"사람 땀 냄새\", \n",
    "            \"동물 냄새\", \"오래된 냄새\", \"불쾌한 냄새\", \"자극적인 냄새\", \"타는 냄새\", \n",
    "            \"강한 냄새\", \"화장실 냄새\", \"음악 센터 냄새\", \"비린내\", \"가스 냄새\"\n",
    "        ],\n",
    "        \"intensity_adjectives\": [\n",
    "            \"쾌쾌하게\", \"심하게\", \"지속적으로\", \"불쾌하게\", \"찌들게\", \n",
    "            \"강하게\", \"독하게\", \"자극적으로\", \"비린내처럼\", \"지독하게\", \n",
    "            \"매캐하게\", \"묵직하게\", \"축축하게\", \"역하게\", \"찌르는 듯이\", \n",
    "            \"강렬하게\", \"폭발적으로\", \"극도로\", \"심각하게\", \"거슬리게\", \n",
    "            \"짙게\", \"스며들 듯이\", \"자욱하게\", \"묵은 듯이\", \"날카롭게\"\n",
    "        ],\n",
    "        \n",
    "        \"effects\": [\n",
    "            \"호흡 곤란\", \"메스꺼움\", \"불쾌감\", \"피로감\", \"두통\", \n",
    "            \"기운이 빠짐\", \"눈 자극\", \"어지러움\", \"냄새로 인한 스트레스\", \n",
    "            \"심한 자극\", \"구역질\", \"기분 나쁨\", \"화학적 반응\", \"심한 불쾌감\", \n",
    "            \"호흡기 질환\", \"피부 자극\", \"산소 부족\", \"집중력 저하\", \"호흡기 알레르기\", \n",
    "            \"불안감\"\n",
    "    \t],\n",
    "\n",
    "        \"issues\": [\n",
    "            \"냄새로 인한 생활 불편\", \"주민 건강에 미치는 영향\", \"주민 불만 증가\", \n",
    "            \"공기 질 저하\", \"불쾌한 환경\", \"심각한 생활 환경 문제\", \"청결 부족\", \n",
    "            \"위생 상태 문제\", \"불쾌한 공기\", \"악취 배출 통제 문제\", \"환경 오염\", \n",
    "            \"냄새로 인한 스트레스\", \"심각한 공공 문제\", \"일상 생활 불편\", \n",
    "            \"집안 환경의 악취\", \"사회적 불편\", \"소음과 냄새의 복합적 문제\", \"냄새 민감도 증가\"\n",
    "        ],\n",
    "        \"departments\": [\n",
    "            \"환경부\", \"보건복지부\", \"지자체 환경 부서\", \"시청 환경 관리 부서\", \n",
    "            \"주택 관리 부서\", \"상수도 관리 부서\", \"도시 관리 부서\", \n",
    "            \"보건소\", \"지역 환경 협의체\", \"위생 관리 부서\", \"공공건물 관리 부서\", \n",
    "            \"지자체 보건 부서\", \"하수도 관리 부서\", \"교통부\", \"민원센터\", \n",
    "            \"주차장 관리 부서\", \"상가 관리 부서\", \"도시 재개발 부서\", \n",
    "            \"환경정화 부서\", \"공공서비스 부서\"\n",
    "        ],\n",
    "        \"time_of_days\": [\n",
    "            \"아침\", \"오전\", \"낮\", \"오후\", \"저녁\", \"밤\", \"새벽\",\n",
    "            \"오전 6시부터 7시\", \"오전 9시부터 10시\", \"오후 1시부터 2시\", \n",
    "            \"오후 3시부터 4시\", \"저녁 6시부터 7시\", \"밤 10시부터 11시\", \n",
    "            \"새벽 2시부터 3시\"\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    \"건설현장 관련\": {\n",
    "        \"locations\": [\n",
    "            \"주택 공사 현장\", \"상업용 건물 공사 현장\", \"도로 건설 현장\", \"다리 건설 현장\", \n",
    "            \"철도 건설 현장\", \"지하철 공사 현장\", \"도시 재개발 현장\", \"고속도로 공사 현장\", \n",
    "            \"기반 시설 공사 현장\", \"상하수도 공사 현장\", \"아파트 단지 건설 현장\", \n",
    "            \"상가 및 빌딩 건설 현장\", \"학교 건설 현장\", \"병원 건설 현장\", \"공장 건설 현장\", \n",
    "            \"터널 건설 현장\", \"산업단지 건설 현장\", \"고층 건물 공사 현장\", \"주차장 건설 현장\", \n",
    "            \"공항 건설 현장\"\n",
    "        ],\n",
    "        \"smell_types\": [\n",
    "            \"시멘트 냄새\", \"타는 냄새\", \"화학 물질 냄새\", \"아스팔트 냄새\", \"석유 냄새\", \n",
    "            \"배관 설치 냄새\", \"청소 화학 냄새\", \"페인트 냄새\", \"솔벤트 냄새\", \"금속 냄새\", \n",
    "            \"붕괴된 건축물 냄새\", \"조합 재료 냄새\", \"비산 먼지 냄새\", \"기계 오일 냄새\", \n",
    "            \"유해 화학물질 냄새\", \"기계 연료 냄새\", \"시멘트 분말 냄새\", \"섬유 화학 냄새\", \n",
    "            \"건설 자재 냄새\", \"유리 및 플라스틱 재료 냄새\"\n",
    "        ],\n",
    "        \"intensity_adjectives\": [\n",
    "            \"강하게\", \"지속적으로\", \"심하게\", \"자욱하게\", \"불쾌하게\", \n",
    "            \"심각하게\", \"매캐하게\", \"화학적으로\", \"묵직하게\", \"자극적으로\", \n",
    "            \"독하게\", \"지독하게\", \"폭발적으로\", \"강렬하게\", \"비산성으로\", \n",
    "            \"눈에 띄게\", \"빈번하게\", \"거슬리게\", \"질식할 정도로\", \"짙게\"\n",
    "        ],\n",
    "        \"effects\": [\n",
    "            \"호흡 곤란\", \"눈 자극\", \"피로감\", \"어지러움\", \"불쾌감\", \n",
    "            \"기침\", \"두통\", \"기운이 빠짐\", \"화학적 반응\", \"호흡기 질환\", \n",
    "            \"피부 자극\", \"집중력 저하\", \"스트레스\", \"심리적 불편\", \"소음으로 인한 스트레스\"\n",
    "   \t\t],\n",
    "        \"issues\": [\n",
    "            \"주변 주민 불편\", \"건설 근로자의 건강 문제\", \"대기 오염\", \"화학물질 유출\", \n",
    "            \"공사로 인한 소음과 악취\", \"공공 건강 위협\", \"도로 교통 문제\", \"공기 질 저하\", \n",
    "            \"근로자 안전 문제\", \"주거지 근처 건설 현장\", \"주차 공간 부족\", \"지속적인 냄새 문제\", \n",
    "            \"구조물 붕괴 위험\", \"지속적인 공사 소음\", \"공사 진행 중 환경 오염\", \"화학 물질 누출 위험\", \n",
    "            \"시멘트 먼지 문제\", \"불완전한 작업 환경\", \"비효율적인 건설 작업\", \"작업 환경으로 인한 스트레스\"\n",
    "        ],\n",
    "        \"departments\": [\n",
    "            \"건설부\", \"환경부\", \"시청 건설 관리 부서\", \"안전 관리 부서\", \"환경 오염 관리 부서\", \n",
    "            \"공공 안전 부서\", \"건설 현장 관리 부서\", \"위험 관리 부서\", \"주택 관리 부서\", \n",
    "            \"도로 공사 관리 부서\", \"교통 관리 부서\", \"지자체 환경 관리 부서\", \"위생 관리 부서\", \n",
    "            \"재활용 및 폐기물 관리 부서\", \"시설 공사 관리 부서\", \"시설 안전 부서\", \n",
    "            \"노동부\", \"보건소\", \"건설 근로자 안전 부서\", \"공공 서비스 부서\"\n",
    "        ],\n",
    "        \"time_of_days\": [\n",
    "            \"아침\", \"오전\", \"낮\", \"오후\", \"저녁\", \"밤\", \"새벽\",\n",
    "            \"오전 6시부터 7시\", \"오전 9시부터 10시\", \"오후 1시부터 2시\", \n",
    "            \"오후 3시부터 4시\", \"저녁 6시부터 7시\", \"밤 10시부터 11시\", \n",
    "            \"새벽 2시부터 3시\"\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\n",
    "    \"O\",  # Outside of a named entity\n",
    "    \"B-LOCATION\", \"I-LOCATION\",\n",
    "    \"B-ISSUE\", \"I-ISSUE\",\n",
    "    \"B-COMPANY\", \"I-COMPANY\",\n",
    "    \"B-DEPARTMENT\", \"I-DEPARTMENT\",  # 추가됨\n",
    "    \"B-SMELL_TYPE\", \"I-SMELL_TYPE\",\n",
    "    \"B-INTENSITY\", \"I-INTENSITY\",\n",
    "    \"B-TIME\", \"I-TIME\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "citizen_greeting_templates = [\n",
    "\t\"여보세요?\",\n",
    "\t\"안녕하세요?\",\n",
    "\t\"저희 {location}에서 {issue} 관련하여 연락드렸습니다.\",\n",
    "\t\"안녕하세요. {location}에서 전화드렸습니다.\",\n",
    "\t\"여보세요? {location}의 {issue}에 대해 문의 드립니다.\",\n",
    "\t\"저희 {location}에서 {smell_type} 문제로 연락드렸습니다.\",\n",
    "\t\"안녕하세요. {location}에서 발생한 {issue} 문제에 대해 문의 드립니다.\"\n",
    "]\n",
    "\n",
    "agent_greeting_templates = [\n",
    "\t\"네, 민원센터입니다.\",\n",
    "\t\"네, {location}의 {department}입니다. 어떤 문제로 연락주셨나요?\",\n",
    "\t\"네, {location}에서 발생한 문제에 대해 상담 드리겠습니다.\",\n",
    "\t\"안녕하세요, {department}입니다. 무엇을 도와드릴까요?\",\n",
    "\t\"감사합니다, {department}입니다. 어떻게 도와드릴까요?\",\n",
    "\t\"안녕하세요. 무엇을 도와드릴 수 있을까요?\",\n",
    "\t\"네, {location}의 문제에 대해 확인하겠습니다.\"\n",
    "]\n",
    "\n",
    "citizen_complaint_templates = [\n",
    "\t\"저희 {location}에서 {smell_type}이 {intensity_adjective} 나고 있습니다. {effect}\",\n",
    "\t\"최근 {location}에서 {issue}로 불편을 겪고 있습니다. {effect}\",\n",
    "\t\"{location}에서 {smell_type}으로 인해 {effect} 문제가 발생하고 있습니다.\",\n",
    "\t\"저희 {location}에서 {smell_type}이 {intensity_adjective} 나고 있어 {effect}로 불편을 겪고 있습니다.\",\n",
    "\t\"저희 {location}에서 {issue}로 인해 심각한 불편을 겪고 있습니다. {effect}\",\n",
    "\t\"{location}에서 {smell_type} 때문에 {effect}가 발생하고 있습니다.\",\n",
    "\t\"최근 {location}에서 {smell_type}이 {intensity_adjective} 나고 있어 {effect}로 큰 불편을 겪고 있습니다.\",\n",
    "\t\"{location}에서 {smell_type}이 {intensity_adjective} 나고 있으며, {effect}로 생활에 불편이 많습니다.\",\n",
    "\t\"{location}에서 발생한 {issue}로 인해 {effect}로 불편함을 겪고 있습니다.\",\n",
    "\t\"{location}에서 {smell_type} 때문에 {effect} 문제가 발생하고 있습니다.\",\n",
    "\t\n",
    "\t\"저희 {location}에서 {time_of_day}에 {smell_type}이 {intensity_adjective} 나고 있어 {effect} 발생 중입니다.\",\n",
    "\t\"최근 {location}에서 {time_of_day}에 {issue}로 심각한 불편을 겪고 있습니다. {effect}\",\n",
    "\t\"{location}에서 {time_of_day}에 발생한 {smell_type} 때문에 {effect} 문제가 발생하고 있습니다.\",\n",
    "\t\"저희 {location}에서 {time_of_day}에 {issue}로 큰 불편을 겪고 있습니다. {effect}\",\n",
    "\t\"{location}에서 {time_of_day}에 {smell_type}이 {intensity_adjective} 나고 있어 {effect}로 매우 불편합니다.\"\n",
    "]\n",
    "\n",
    "agent_response_templates = [\n",
    "\t\"불편을 드려 대단히 죄송합니다. {effect} 해결을 위해 {location}에 대한 현장 점검을 실시하겠습니다. 잠시만 기다려 주시겠습니까?\",\n",
    "\t\"해당 문제를 해결하기 위해 최선을 다하겠습니다. 조치를 취할 수 있도록 {location}을 빠르게 점검하겠습니다.\",\n",
    "\t\"죄송합니다. {location}에서 발생한 문제에 대해 조치를 취하고 있습니다. 조금만 기다려 주세요.\",\n",
    "\t\"불편을 드려 정말 죄송합니다. {effect} 해결을 위해 신속히 대응하겠습니다.\",\n",
    "\t\"현재 {location}에 대한 점검을 시작하여 문제를 해결하겠습니다. 잠시만 기다려 주세요.\",\n",
    "\t\"문제를 해결하기 위해 최선을 다하겠습니다. 현장 점검 후 즉시 조치하겠습니다.\",\n",
    "\t\"불편을 드려 대단히 죄송합니다. 문제를 해결하기 위해 {location}을 점검하겠습니다.\",\n",
    "\t\"문제를 즉시 해결할 수 있도록 {location}에 대한 점검을 진행하겠습니다. 조금만 기다려 주세요.\",\n",
    "\t\"불편을 드려 대단히 죄송합니다. 해당 문제에 대해 최선의 조치를 취할 것입니다.\",\n",
    "\t\"죄송합니다. {location}에서 발생한 문제를 해결하기 위해 현장 점검을 진행하겠습니다.\",\n",
    "\t\n",
    "\t\"불편을 드려 대단히 죄송합니다. {time_of_day}에 발생한 {issue} 문제를 해결하기 위해 {location}에 대한 점검을 실시하겠습니다.\",\n",
    "\t\"현재 {location}에서 발생한 {issue} 문제에 대해 조치 중입니다. {time_of_day}에 완료될 예정입니다.\",\n",
    "\t\"죄송합니다. {time_of_day}에 {location}에서 발생한 문제를 해결하기 위해 신속히 대응하고 있습니다.\",\n",
    "\t\"불편을 드려 정말 죄송합니다. {time_of_day}에 발생한 {effect} 문제를 해결하기 위해 현장 점검을 진행 중입니다.\",\n",
    "\t\"{location}에서 발생한 {issue} 문제에 대해 {time_of_day}에 즉시 점검을 시작하겠습니다.\"\n",
    "]\n",
    "\n",
    "citizen_response_templates = [\n",
    "\t\"네, 그러니 빨리 좀 처리해주세요.\",\n",
    "\t\"그럼 신속히 해결해주세요.\",\n",
    "\t\"알겠습니다. 빠른 처리 부탁드립니다.\",\n",
    "\t\"네, 확인해 주세요. 빠른 해결을 바랍니다.\",\n",
    "\t\"알겠습니다. 최대한 빨리 처리해 주세요.\",\n",
    "\t\"그럼 문제를 빨리 해결해 주세요.\",\n",
    "\t\"네, 기다리겠습니다. 빨리 처리해 주세요.\",\n",
    "\t\"그럼 빠르게 해결해 주세요.\",\n",
    "\t\"알겠습니다. 처리 좀 해주세요.\",\n",
    "\t\"네, 처리가 되도록 부탁드립니다.\"\n",
    "]\n",
    "\n",
    "agent_acknowledge_templates = [\n",
    "\t\"네, 알겠습니다.\",\n",
    "\t\"확인 후 조치를 취하겠습니다.\",\n",
    "\t\"즉시 처리하겠습니다. 조금만 기다려 주세요.\",\n",
    "\t\"빠르게 조치를 취하겠습니다. 감사합니다.\",\n",
    "\t\"네, 확인 후 바로 처리하겠습니다.\",\n",
    "\t\"확인 후 처리하도록 하겠습니다.\",\n",
    "\t\"네, 바로 조치를 취하겠습니다.\",\n",
    "\t\"확인해 보고 빠르게 처리하겠습니다.\",\n",
    "\t\"알겠습니다. 최대한 빠르게 처리하겠습니다.\",\n",
    "\t\"네, 신속하게 처리하겠습니다.\"\n",
    "]\n",
    "\n",
    "citizen_farewell_templates = [\n",
    "\t\"수고하세요.\",\n",
    "\t\"감사합니다.\",\n",
    "\t\"좋은 하루 되세요.\",\n",
    "\t\"빠른 처리 감사합니다.\",\n",
    "\t\"수고 많으십니다.\",\n",
    "\t\"잘 부탁드립니다.\",\n",
    "\t\"고맙습니다.\",\n",
    "\t\"감사합니다. 처리 잘 부탁드립니다.\",\n",
    "\t\"수고하셨습니다.\",\n",
    "\t\"감사합니다. 좋은 하루 되세요.\"\n",
    "]\n",
    "\n",
    "agent_farewell_templates = [\n",
    "\t\"네, 감사합니다.\",\n",
    "\t\"저희가 도와드리겠습니다. 감사합니다.\",\n",
    "\t\"언제든지 연락 주세요. 감사합니다.\",\n",
    "\t\"감사합니다. 빠르게 처리하겠습니다.\",\n",
    "\t\"네, 처리 후 다시 연락드리겠습니다. 감사합니다.\",\n",
    "\t\"네, 감사합니다. 좋은 하루 되세요.\",\n",
    "\t\"저희가 도와드릴 수 있어 기쁩니다. 감사합니다.\",\n",
    "\t\"언제든지 연락주세요. 감사합니다.\",\n",
    "\t\"네, 처리 후 다시 연락드리겠습니다. 감사합니다.\",\n",
    "\t\"감사합니다. 좋은 하루 되세요.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_ner_data_from_categories(main_category, subcategories, label_list, num_sentences=1000):\n",
    "#     \"\"\"\n",
    "#     main_category: 상위 카테고리 키 (예: \"쓰레기 관련\", \"공장 관련\")\n",
    "#     subcategories: 하위 카테고리 딕셔너리 (예: \"locations\", \"issues\" 등)\n",
    "#     \"\"\"\n",
    "#     label_map = {label: idx for idx, label in enumerate(label_list)}\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "#     data = {\"tokens\": [], \"ner_tags\": []}\n",
    "\n",
    "#     # 모든 조합 생성\n",
    "#     combinations = list(product(\n",
    "#         subcategories[\"locations\"],\n",
    "#         subcategories[\"issues\"],\n",
    "#         subcategories[\"departments\"],\n",
    "#         subcategories[\"smell_types\"],\n",
    "#         subcategories[\"intensity_adjectives\"],\n",
    "#         subcategories[\"effects\"],\n",
    "#         subcategories[\"time_of_days\"]\n",
    "#     ))\n",
    "\n",
    "#     print(f\"[{main_category}] 총 조합 수: {len(combinations)}\")\n",
    "\n",
    "#     # 조합 샘플링\n",
    "#     sampled_combinations = list(islice(combinations, num_sentences))\n",
    "#     print(f\"[{main_category}] 샘플링된 조합 수: {len(sampled_combinations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_ner_data_from_categories(main_category, subcategories, label_list, num_sentences=10000):\n",
    "#     \"\"\"\n",
    "#     main_category: 상위 카테고리 키 (예: \"쓰레기 관련\", \"공장 관련\")\n",
    "#     subcategories: 하위 카테고리 딕셔너리 (예: \"locations\", \"issues\" 등)\n",
    "#     \"\"\"\n",
    "#     label_map = {label: idx for idx, label in enumerate(label_list)}\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "#     data = {\"tokens\": [], \"ner_tags\": []}\n",
    "\n",
    "#    def balanced_sample_combinations(subcategories, num_samples):\n",
    "#         \"\"\"균형 잡힌 조합을 생성.\"\"\"\n",
    "#         balanced_subcategories = {\n",
    "#             key: random.sample(values, min(3, len(values)))  # 각 카테고리에서 최대 3개만 선택\n",
    "#             for key, values in subcategories.items()\n",
    "#         }\n",
    "#         combinations = list(product(\n",
    "#             balanced_subcategories[\"locations\"],\n",
    "#             balanced_subcategories[\"issues\"],\n",
    "#             balanced_subcategories[\"departments\"],\n",
    "#             balanced_subcategories[\"smell_types\"],\n",
    "#            balanced_subcategories[\"intensity_adjectives\"],\n",
    "#             balanced_subcategories[\"effects\"],\n",
    "#             balanced_subcategories[\"time_of_days\"]\n",
    "#         ))\n",
    "#         return random.sample(combinations, min(num_samples, len(combinations)))\n",
    "\n",
    "    # 조합 샘플링\n",
    "#     sampled_combinations = balanced_sample_combinations(subcategories, num_sentences)\n",
    "#    print(f\"[{main_category}] 샘플링된 조합 수: {len(sampled_combinations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from itertools import product, islice\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "def generate_phone_conversation(location, issue, department, smell_type, intensity_adjective, effect, time_of_day):\n",
    "    citizen_greeting = random.choice(citizen_greeting_templates).format(location=location, issue=issue, smell_type=smell_type)\n",
    "    agent_greeting = random.choice(agent_greeting_templates).format(location=location, department=department)\n",
    "    citizen_complaint = random.choice(citizen_complaint_templates).format(location=location, smell_type=smell_type, intensity_adjective=intensity_adjective, effect=effect, issue=issue, time_of_day=time_of_day)\n",
    "    agent_response = random.choice(agent_response_templates).format(location=location, effect=effect, time_of_day=time_of_day, issue=issue)\n",
    "    citizen_response = random.choice(citizen_response_templates)\n",
    "    agent_acknowledge = random.choice(agent_acknowledge_templates)\n",
    "    citizen_farewell = random.choice(citizen_farewell_templates)\n",
    "    agent_farewell = random.choice(agent_farewell_templates)\n",
    "\n",
    "    conversation = f\"{citizen_greeting} {agent_greeting} {citizen_complaint} {agent_response} {citizen_response} {agent_acknowledge} {citizen_farewell} {agent_farewell}\"\n",
    "    return conversation.strip()\n",
    "\n",
    "def generate_ner_data_from_categories(main_category, subcategories, label_list, num_sentences=10000):\n",
    "    \"\"\"\n",
    "    main_category: 상위 카테고리 키 (예: \"쓰레기 관련\", \"공장 관련\")\n",
    "    subcategories: 하위 카테고리 딕셔너리 (예: \"locations\", \"issues\" 등)\n",
    "    \"\"\"\n",
    "    label_map = {label: idx for idx, label in enumerate(label_list)}\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "    data = {\"tokens\": [], \"ner_tags\": []}\n",
    "\n",
    "    def balanced_sample_combinations(subcategories, num_samples):\n",
    "        \"\"\"균형 잡힌 조합을 생성.\"\"\"\n",
    "        balanced_subcategories = {\n",
    "            key: random.sample(values, min(10, len(values)))  # 각 카테고리에서 최대 3개만 선택\n",
    "            for key, values in subcategories.items()\n",
    "        }\n",
    "        combinations = list(product(\n",
    "            balanced_subcategories[\"locations\"],\n",
    "            balanced_subcategories[\"issues\"],\n",
    "            balanced_subcategories[\"departments\"],\n",
    "            balanced_subcategories[\"smell_types\"],\n",
    "            balanced_subcategories[\"intensity_adjectives\"],\n",
    "            balanced_subcategories[\"effects\"],\n",
    "            balanced_subcategories[\"time_of_days\"]\n",
    "        ))\n",
    "        return random.sample(combinations, min(num_samples, len(combinations)))\n",
    "\n",
    "    # 조합 샘플링\n",
    "    sampled_combinations = balanced_sample_combinations(subcategories, num_sentences)\n",
    "    print(f\"[{main_category}] 샘플링된 조합 수: {len(sampled_combinations)}\")\n",
    "\n",
    "    for idx, (location, issue, department, smell_type, intensity_adjective, effect, time_of_day) in enumerate(sampled_combinations):\n",
    "        # 문장 생성\n",
    "        text = generate_phone_conversation(\n",
    "            location=location,\n",
    "            issue=issue,\n",
    "            department=department,\n",
    "            smell_type=smell_type,\n",
    "            intensity_adjective=intensity_adjective,\n",
    "            effect=effect,\n",
    "            time_of_day=time_of_day\n",
    "        )\n",
    "\n",
    "        # 토큰화\n",
    "        tokenized = tokenizer(text, return_offsets_mapping=True, truncation=True)\n",
    "        tokens = tokenizer.convert_ids_to_tokens(tokenized[\"input_ids\"])\n",
    "\n",
    "        # 복원된 토큰 생성\n",
    "        restored_tokens = []\n",
    "        current_word = \"\"\n",
    "        for token in tokens:\n",
    "            if token.startswith(\"##\"):\n",
    "                current_word += token[2:]\n",
    "            else:\n",
    "                if current_word:\n",
    "                    restored_tokens.append(current_word)\n",
    "                current_word = token\n",
    "\n",
    "        if current_word:\n",
    "            restored_tokens.append(current_word)\n",
    "\n",
    "        # 태그 생성\n",
    "        tags = [\"O\"] * len(restored_tokens)\n",
    "\n",
    "        for entity, label in zip(\n",
    "            [location, issue, department, smell_type, intensity_adjective, time_of_day],\n",
    "            [\"LOCATION\", \"ISSUE\", \"DEPARTMENT\", \"SMELL_TYPE\", \"INTENSITY\", \"TIME\"]\n",
    "        ):\n",
    "            for match in re.finditer(re.escape(entity), text):\n",
    "                start_idx, end_idx = match.start(), match.end()\n",
    "                for i, token in enumerate(restored_tokens):\n",
    "                    if token in text[start_idx:end_idx]:\n",
    "                        tags[i] = f\"B-{label}\" if text[start_idx:end_idx].startswith(token) else f\"I-{label}\"\n",
    "\n",
    "        # 태그를 라벨 ID로 변환\n",
    "        tag_ids = [label_map[tag] for tag in tags]\n",
    "        data[\"tokens\"].append(restored_tokens)\n",
    "        data[\"ner_tags\"].append(tag_ids)\n",
    "\n",
    "    print(f\"[{main_category}] 생성된 NER 엔트리: {len(data['tokens'])}\")\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import time\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification, \n",
    "    AdamW, \n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "class NERModelTrainer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name: str = \"klue/bert-base\", \n",
    "        learning_rate: float = 5e-5,\n",
    "        num_epochs: int = 15,\n",
    "        batch_size: int = 16,\n",
    "        max_length: int = 128\n",
    "    ):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        if self.device == \"cuda\":\n",
    "            print(f\"GPU Model: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.label_list = [\n",
    "            'O', \n",
    "            'B-LOCATION', 'I-LOCATION', \n",
    "            'B-ISSUE', 'I-ISSUE', \n",
    "            'B-DEPARTMENT', 'I-DEPARTMENT', \n",
    "            'B-SMELL_TYPE', 'I-SMELL_TYPE', \n",
    "            'B-INTENSITY', 'I-INTENSITY', \n",
    "            'B-TIME', 'I-TIME'\n",
    "        ]\n",
    "        \n",
    "        self.label_map = {label: idx for idx, label in enumerate(self.label_list)}\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(\n",
    "            model_name, \n",
    "            num_labels=len(self.label_list),\n",
    "            id2label={i: label for label, i in self.label_map.items()},\n",
    "            label2id=self.label_map\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        \n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'train_metrics': [],\n",
    "            'val_metrics': [],\n",
    "            'best_epoch': 0,\n",
    "            'epoch_times': []\n",
    "        }\n",
    "        \n",
    "        self.save_dir = \"./best_ner_model\"\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "    def tokenize_and_align_labels(self, texts: List[List[str]], labels: List[List[int]]):\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            is_split_into_words=True,\n",
    "            padding=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        # 토큰화된 텍스트에 라벨 정렬\n",
    "        aligned_labels = []\n",
    "        for i, label in enumerate(labels):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "            aligned_label = []\n",
    "            previous_word_idx = None\n",
    "            for word_idx in word_ids:\n",
    "                if word_idx is None:\n",
    "                    aligned_label.append(-100)  # 손실 계산에서 무시\n",
    "                elif word_idx != previous_word_idx:\n",
    "                    aligned_label.append(label[word_idx])\n",
    "                else:\n",
    "                    aligned_label.append(-100)\n",
    "                previous_word_idx = word_idx\n",
    "            aligned_labels.append(aligned_label)\n",
    "        \n",
    "        # -100으로 패딩 토큰을 마스킹\n",
    "        labels = aligned_labels\n",
    "        for i in range(len(labels)):\n",
    "            for j in range(len(labels[i])):\n",
    "                if labels[i][j] == self.tokenizer.pad_token_id:\n",
    "                    labels[i][j] = -100\n",
    "\n",
    "        # 검증 조건 추가\n",
    "        for label_ids in labels:\n",
    "            assert all(\n",
    "                label == -100 or 0 <= label < len(self.label_list)\n",
    "                for label in label_ids\n",
    "            ), \"Label out of range!\"\n",
    "\n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "        return tokenized_inputs\n",
    "\n",
    "\n",
    "    def prepare_dataset(self, tokens, labels):\n",
    "        encodings = self.tokenize_and_align_labels(tokens, labels)\n",
    "        \n",
    "        # **NaN 또는 Inf 값 검증**\n",
    "        assert not np.isnan(np.array(encodings['input_ids'])).any(), \"Input contains NaN!\"\n",
    "        assert not np.isinf(np.array(encodings['input_ids'])).any(), \"Input contains Inf!\"\n",
    "        \n",
    "        dataset = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(encodings['input_ids']),\n",
    "            torch.tensor(encodings['attention_mask']),\n",
    "            torch.tensor(encodings['labels'])\n",
    "        )\n",
    "        return dataset\n",
    "\n",
    "    def train(self, all_tokens, all_labels, validation_split=0.2):\n",
    "        # 데이터셋 분할\n",
    "        train_tokens, val_tokens, train_labels, val_labels = train_test_split(\n",
    "            all_tokens, all_labels, test_size=validation_split, random_state=42\n",
    "        )\n",
    "        \n",
    "        # **데이터 길이 검증**\n",
    "        for tokens, labels in zip(train_tokens, train_labels):\n",
    "            assert len(tokens) == len(labels), \"Tokens and labels length mismatch!\"\n",
    "\n",
    "        # 데이터셋 준비\n",
    "        train_dataset = self.prepare_dataset(train_tokens, train_labels)\n",
    "        val_dataset = self.prepare_dataset(val_tokens, val_labels)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=True\n",
    "        )\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        # 옵티마이저 및 스케줄러 설정\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)\n",
    "        total_steps = len(train_loader) * self.num_epochs\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer, \n",
    "            num_warmup_steps=500, \n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "        # 학습 시작\n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        print(f\"\\n{'=' * 50}\\n학습 시작: {datetime.now()}\")\n",
    "        print(f\"총 학습 데이터: {len(train_dataset)}\")\n",
    "        print(f\"총 검증 데이터: {len(val_dataset)}\")\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            # 훈련 모드\n",
    "            self.model.train()\n",
    "            train_losses = []\n",
    "            \n",
    "            progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{self.num_epochs}\")\n",
    "            for batch in progress_bar:\n",
    "                input_ids, attention_mask, labels = batch\n",
    "                \n",
    "                # **장치 검증 및 이동**\n",
    "                input_ids = input_ids.to(self.device)\n",
    "                attention_mask = attention_mask.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids, \n",
    "                    attention_mask=attention_mask, \n",
    "                    labels=labels\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                \n",
    "                train_losses.append(loss.item())\n",
    "                progress_bar.set_postfix({'loss': loss.item()})\n",
    "            \n",
    "            avg_train_loss = np.mean(train_losses)\n",
    "            self.history['train_loss'].append(avg_train_loss)\n",
    "            \n",
    "            # 검증 모드\n",
    "            self.model.eval()\n",
    "            val_losses = []\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(val_loader, desc=\"검증 중\"):\n",
    "                    input_ids, attention_mask, labels = batch\n",
    "                    \n",
    "                    input_ids = input_ids.to(self.device)\n",
    "                    attention_mask = attention_mask.to(self.device)\n",
    "                    labels = labels.to(self.device)\n",
    "                    \n",
    "                    outputs = self.model(\n",
    "                        input_ids=input_ids, \n",
    "                        attention_mask=attention_mask, \n",
    "                        labels=labels\n",
    "                    )\n",
    "                    loss = outputs.loss\n",
    "                    \n",
    "                    val_losses.append(loss.item())\n",
    "                    \n",
    "                    logits = outputs.logits\n",
    "                    preds = torch.argmax(logits, dim=2)\n",
    "                    \n",
    "                    all_preds.append(preds.cpu().numpy())\n",
    "                    all_labels.append(labels.cpu().numpy())\n",
    "            \n",
    "            # 성능 평가\n",
    "            avg_val_loss = np.mean(val_losses)\n",
    "            self.history['val_loss'].append(avg_val_loss)\n",
    "            \n",
    "            # 최고 모델 저장\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                self.save_model()\n",
    "                self.history['best_epoch'] = epoch + 1\n",
    "            \n",
    "            print(f\"\\n에폭 {epoch+1} 요약:\")\n",
    "            print(f\"훈련 손실: {avg_train_loss:.4f}\")\n",
    "            print(f\"검증 손실: {avg_val_loss:.4f}\")\n",
    "            \n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "            self.history['epoch_times'].append(epoch_time)\n",
    "            print(f\"소요 시간: {epoch_time:.2f}초\\n\")\n",
    "\n",
    "    def save_model(self):\n",
    "        save_path = os.path.join(self.save_dir, \"./ner_best_model\")\n",
    "        self.model.save_pretrained(save_path)\n",
    "        self.tokenizer.save_pretrained(save_path)\n",
    "        torch.save(self.history, os.path.join(save_path, 'training_history.pt'))\n",
    "        print(f\"Best model saved at {save_path}\")\n",
    "\n",
    "    def predict_entities(self, text):\n",
    "        # 모델 로드 및 예측 메서드\n",
    "        model = AutoModelForTokenClassification.from_pretrained(\n",
    "            os.path.join(self.save_dir, \"./ner_best_model\")\n",
    "        ).to(self.device)\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            text, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True, \n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        predictions = torch.argmax(outputs.logits, dim=2)\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "        \n",
    "        entities = {}\n",
    "        current_entity = \"\"\n",
    "        current_type = \"\"\n",
    "\n",
    "        for token, label in zip(tokens, predictions[0].tolist()):\n",
    "            label_name = self.label_list[label]\n",
    "            \n",
    "            if label_name.startswith(\"B-\"):\n",
    "                if current_entity:\n",
    "                    entities[current_type] = current_entity\n",
    "                current_type = label_name[2:]\n",
    "                current_entity = token.replace('##', '')\n",
    "            elif label_name.startswith(\"I-\") and label_name[2:] == current_type:\n",
    "                current_entity += token.replace('##', '')\n",
    "        \n",
    "        if current_entity:\n",
    "            entities[current_type] = current_entity\n",
    "        \n",
    "        return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[쓰레기 관련] 샘플링된 조합 수: 10000\n",
      "[쓰레기 관련] 생성된 NER 엔트리: 10000\n",
      "[날씨 관련] 샘플링된 조합 수: 10000\n",
      "[날씨 관련] 생성된 NER 엔트리: 10000\n",
      "[공장 관련] 샘플링된 조합 수: 10000\n",
      "[공장 관련] 생성된 NER 엔트리: 10000\n",
      "[축산 관련] 샘플링된 조합 수: 10000\n",
      "[축산 관련] 생성된 NER 엔트리: 10000\n",
      "[생활 악취 관련] 샘플링된 조합 수: 10000\n",
      "[생활 악취 관련] 생성된 NER 엔트리: 10000\n",
      "[건설현장 관련] 샘플링된 조합 수: 10000\n",
      "[건설현장 관련] 생성된 NER 엔트리: 10000\n",
      "Using device: cuda\n",
      "GPU Model: NVIDIA GeForce RTX 4080\n",
      "Available GPU memory: 15.59 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 15:12:42.501527: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-02 15:12:42.526831: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-02 15:12:42.945722: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Label out of range!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m모델 학습 완료 및 저장 완료!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 23\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTOKENIZERS_PARALLELISM\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# 멀티프로세싱 경고 방지\u001b[39;00m\n\u001b[1;32m     18\u001b[0m trainer \u001b[38;5;241m=\u001b[39m NERModelTrainer(num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m모델 학습 완료 및 저장 완료!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 143\u001b[0m, in \u001b[0;36mNERModelTrainer.train\u001b[0;34m(self, all_tokens, all_labels, validation_split)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokens and labels length mismatch!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# 데이터셋 준비\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_dataset(val_tokens, val_labels)\n\u001b[1;32m    146\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m    147\u001b[0m     train_dataset, \n\u001b[1;32m    148\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, \n\u001b[1;32m    149\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    150\u001b[0m )\n",
      "Cell \u001b[0;32mIn[7], line 119\u001b[0m, in \u001b[0;36mNERModelTrainer.prepare_dataset\u001b[0;34m(self, tokens, labels)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_dataset\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens, labels):\n\u001b[0;32m--> 119\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize_and_align_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# **NaN 또는 Inf 값 검증**\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(np\u001b[38;5;241m.\u001b[39marray(encodings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]))\u001b[38;5;241m.\u001b[39many(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput contains NaN!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[7], line 109\u001b[0m, in \u001b[0;36mNERModelTrainer.tokenize_and_align_labels\u001b[0;34m(self, texts, labels)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# 검증 조건 추가\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label_ids \u001b[38;5;129;01min\u001b[39;00m labels:\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    110\u001b[0m         label \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m label \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_list)\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m label_ids\n\u001b[1;32m    112\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel out of range!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m tokenized_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m labels\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_inputs\n",
      "\u001b[0;31mAssertionError\u001b[0m: Label out of range!"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 전체 NER 데이터 생성\n",
    "    all_ner_data = {\"tokens\": [], \"ner_tags\": []}\n",
    "    for main_category, subcategories in categories.items():\n",
    "        ner_data = generate_ner_data_from_categories(main_category, subcategories, label_list, num_sentences=10000)\n",
    "        all_ner_data[\"tokens\"].extend(ner_data[\"tokens\"])\n",
    "        all_ner_data[\"ner_tags\"].extend(ner_data[\"ner_tags\"])\n",
    "\n",
    "    train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "        all_ner_data[\"tokens\"], \n",
    "        all_ner_data[\"ner_tags\"], \n",
    "        test_size=0.2, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # 멀티프로세싱 경고 방지\n",
    "\n",
    "    trainer = NERModelTrainer(num_epochs=5)\n",
    "    trainer.train(train_texts, train_labels)\n",
    "    print(\"모델 학습 완료 및 저장 완료!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "테스트 문장 예측 결과:\n",
      "\n",
      "문장: 여보세요. 네 안녕하세요 포항시청입니다. 오후에 음식물 쓰레기 냄새가 진동합니다. 빨리 좀 해결해주세요. 아 음식물 쓰레기 냄새요? 장소는 어디입니까? 포항시청 앞 버스정류장입니다. 네 확인했습니다. 빨리 조취를 취해겠스니다. 감사합니다\n",
      "추출된 엔터티: {'ISSUE': '음식물쓰레기'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "def main():\n",
    "    # 저장된 모델 경로\n",
    "    saved_model_path = '/home/yjtech2/Desktop/yurim/LLM/Pre_processing/smell_keyword/best_ner_model/ner_best_model'\n",
    "    \n",
    "    # 모델과 토크나이저 불러오기\n",
    "    tokenizer = AutoTokenizer.from_pretrained(saved_model_path, local_files_only=True)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(saved_model_path, local_files_only=True).to('cpu')\n",
    "    \n",
    "    # 라벨 리스트 정의\n",
    "    label_list = [\n",
    "        'O', \n",
    "        'B-LOCATION', 'I-LOCATION', \n",
    "        'B-ISSUE', 'I-ISSUE', \n",
    "        'B-DEPARTMENT', 'I-DEPARTMENT', \n",
    "        'B-SMELL_TYPE', 'I-SMELL_TYPE', \n",
    "        'B-INTENSITY', 'I-INTENSITY', \n",
    "        'B-TIME', 'I-TIME'\n",
    "    ]\n",
    "\n",
    "    # 테스트 문장으로 예측\n",
    "    test_sentences = [\n",
    "        \"여보세요. 네 안녕하세요 포항시청입니다. 오후에 음식물 쓰레기 냄새가 진동합니다. 빨리 좀 해결해주세요. 아 음식물 쓰레기 냄새요? 장소는 어디입니까? 포항시청 앞 버스정류장입니다. 네 확인했습니다. 빨리 조취를 취해겠스니다. 감사합니다\"\n",
    "    ]\n",
    "\n",
    "    print(\"\\n테스트 문장 예측 결과:\")\n",
    "    for sentence in test_sentences:\n",
    "        inputs = tokenizer(\n",
    "            sentence, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True, \n",
    "            padding=True\n",
    "        ).to('cpu')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        predictions = torch.argmax(outputs.logits, dim=2)\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "        \n",
    "        entities = {}\n",
    "        current_entity = \"\"\n",
    "        current_type = \"\"\n",
    "\n",
    "        for token, label in zip(tokens, predictions[0].tolist()):\n",
    "            label_name = label_list[label]\n",
    "            \n",
    "            if label_name.startswith(\"B-\"):\n",
    "                if current_entity:\n",
    "                    entities[current_type] = current_entity\n",
    "                current_type = label_name[2:]\n",
    "                current_entity = token.replace('##', '')\n",
    "            elif label_name.startswith(\"I-\") and label_name[2:] == current_type:\n",
    "                current_entity += token.replace('##', '')\n",
    "        \n",
    "        if current_entity:\n",
    "            entities[current_type] = current_entity\n",
    "        \n",
    "        print(f\"\\n문장: {sentence}\")\n",
    "        print(\"추출된 엔터티:\", entities)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
