{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   text     30000 non-null  object\n",
      " 1   keyword  30000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 468.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    JSON 데이터를 불러와 Pandas DataFrame으로 변환\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        json_data = json.load(f)\n",
    "        \n",
    "    data = []\n",
    "    for item in json_data['data']:\n",
    "        if 'text' in item and 'keyword' in item:\n",
    "            data.append({\n",
    "                'text': item['text'],\n",
    "                'keyword': item['keyword']\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(df.info())\n",
    "    # None 값을 빈 문자열로 대체\n",
    "    df = df.fillna('')\n",
    "    return df\n",
    "\n",
    "\n",
    "# JSON 파일 경로\n",
    "file_path_smell = \"/home/yjtech2/Desktop/yurim/LLM/Data/smell_keyword/smell_type_data.json\"\n",
    "smell_df = load_data(file_path_smell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smell\n",
      "Train Data:\n",
      "                                                    text          keyword\n",
      "21753  여보세요? 택배 물류 창고 근처에서 12개월 전 새벽 5시에 화장실 냄새 청결 부족...           화장실 냄새\n",
      "251    안녕하세요. 지하철 역 주변에서 11월 14일 오전에 공공장소 쓰레기 냄새 관련하여...      공공장소 쓰레기 냄새\n",
      "22941  안녕하세요. 아파트 커뮤니티 센터에서 12월 8일 저녁 7시에 오래된 냄새 관련하여...           오래된 냄새\n",
      "618    여보세요? 음식물 쓰레기장에서 1월 17일 자정에 발생한 썩은 물질 냄새 불법 폐기...         썩은 물질 냄새\n",
      "17090  여보세요? 돼지 농장에서 5월 4일 저녁 7시에 발생한 동물 건강 관리용 약물 냄새...  동물 건강 관리용 약물 냄새\n",
      "\n",
      "Validation Data:\n",
      "                                                    text        keyword\n",
      "2308   여보세요? 농업 생산 기지에서 1년 전 저녁에 발생한 강한 페인트 잔여물 냄새 쓰레...  강한 페인트 잔여물 냄새\n",
      "22404  안녕하세요. 최근 하수 처리장 근처에서 9월 12일 오전 11시부터 12시에 발생한...       썩은 음식 냄새\n",
      "23397  안녕하세요. 거리 공연장 근처에서 11월 28일 오후 8시부터 9시에 발생한 비린 ...       비린 고기 냄새\n",
      "25058  여보세요? 대형 교량 공사 현장에서 11월 10일 저녁 6시에 발생한 구리 배선 냄...       구리 배선 냄새\n",
      "2664   안녕하세요. 최근 고속도로 톨게이트 주변에서 7개월 전 자정에 발생한 부패된 유기물...     부패된 유기물 냄새\n",
      "Train size: 24000, Validation size: 6000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터를 70:30 비율로 나누기\n",
    "def split_data(df, train_ratio = 0.8):\n",
    "    \"\"\"\n",
    "    DataFrame을 train과 val로 나눔\n",
    "    \"\"\"\n",
    "    train_df, val_df = train_test_split(df, train_size = train_ratio, random_state = 42, shuffle = True)\n",
    "    return train_df, val_df\n",
    "\n",
    "# train, val로 나누기\n",
    "train_smell_df, val_smell_df = split_data(smell_df)\n",
    "\n",
    "train_smell_df.to_csv('/home/yjtech2/Desktop/yurim/LLM/Data/smell_keyword/train_smell_df.csv', index = False)\n",
    "val_smell_df.to_csv('/home/yjtech2/Desktop/yurim/LLM/Data/smell_keyword/val_smell_df.csv', index = False)\n",
    "\n",
    "\n",
    "print('Smell')\n",
    "print(\"Train Data:\")\n",
    "print(train_smell_df.head())\n",
    "print(\"\\nValidation Data:\")\n",
    "print(val_smell_df.head())\n",
    "\n",
    "# 데이터 크기 확인\n",
    "print(f\"Train size: {len(train_smell_df)}, Validation size: {len(val_smell_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "train_smell_df = pd.read_csv('/home/yjtech2/Desktop/yurim/LLM/Data/smell_keyword/train_smell_df.csv')\n",
    "val_smell_df = pd.read_csv('/home/yjtech2/Desktop/yurim/LLM/Data/smell_keyword/val_smell_df.csv')\n",
    "\n",
    "\n",
    "\n",
    "train_smell_data_dict = train_smell_df.to_dict(orient='list')\n",
    "smell_train_dataset = Dataset.from_dict(train_smell_data_dict)\n",
    "smell_train_dataset\n",
    "print(len(smell_train_dataset))\n",
    "\n",
    "val_smell_data_dict = val_smell_df.to_dict(orient='list')\n",
    "smell_val_dataset = Dataset.from_dict(val_smell_data_dict)\n",
    "print(len(smell_val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 16:22:41.052332: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-04 16:22:41.076499: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-04 16:22:41.410090: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from typing import Dict\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AdamW,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from mecab import MeCab\n",
    "\n",
    "class CustomKeyBERTTrainer:\n",
    "    def __init__(self, model_name: str, **kwargs):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        if self.device == \"cuda\":\n",
    "            print(f\"GPU Model: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=kwargs.get(\"learning_rate\", 1e-4))\n",
    "        self.max_length = kwargs.get(\"max_length\", 512)\n",
    "        self.training_args = kwargs\n",
    "        self.save_dir = kwargs.get(\"save_dir\", \"./smell_models\")\n",
    "        self.mecab = MeCab()  # MeCab 초기화\n",
    "        self.best_model_path = os.path.join(self.save_dir, \"pytorch_model.bin\")\n",
    "        self.tokenizer_path = self.save_dir\n",
    "        \n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'epoch_times': [],\n",
    "            'best_epoch': 0\n",
    "        }\n",
    "\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        \n",
    "    def _mecab_tokenize(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        MeCab을 사용하여 입력 텍스트에서 명사, 형용사, 부사를 추출\n",
    "        \"\"\"\n",
    "        pos_tags = self.mecab.pos(text)  # 품사 태그 추출\n",
    "        # 원하는 품사 필터링: 명사(NNG, NNP), 형용사(VA), 부사(MAG, MAJ)\n",
    "        keywords = [\n",
    "            word for word, pos in pos_tags if pos in (\"NNG\", \"NNP\", \"VA\", \"MAG\", \"MAJ\")\n",
    "        ]\n",
    "        return \" \".join(keywords)  # 추출한 단어를 공백으로 연결하여 반환\n",
    "    \n",
    "    def preprocess_data(self, examples: Dict) -> Dict:\n",
    "        # MeCab 형태소 분석과 품사 필터링 적용\n",
    "        examples[\"text\"] = [self._mecab_tokenize(text) for text in examples[\"text\"]]\n",
    "        \n",
    "        # 기존 코드 유지\n",
    "        inputs = [f\"키워드 추출: {text}\" for text in examples[\"text\"]]\n",
    "        model_inputs = self.tokenizer(\n",
    "            inputs,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=None  # 텐서 변환을 DataCollator에 맡김\n",
    "        )\n",
    "\n",
    "        # 레이블(키워드) 처리\n",
    "        # 키워드가 리스트가 아니라 단일 문자열일 경우에도 처리 가능하도록 수정\n",
    "        labels = []\n",
    "        for keyword in examples[\"keyword\"]:\n",
    "            if isinstance(keyword, list):  # 리스트일 경우\n",
    "                labels.append(\", \".join(keyword) if keyword else \"\")\n",
    "            else:  # 단일 문자열일 경우\n",
    "                labels.append(keyword if keyword else \"\")\n",
    "        \n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            tokenized_labels = self.tokenizer(\n",
    "                labels,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=None  # 텐서 변환을 DataCollator에 맡김\n",
    "            )\n",
    "\n",
    "        # -100으로 패딩 토큰을 마스킹\n",
    "        labels = tokenized_labels[\"input_ids\"]\n",
    "        for i in range(len(labels)):\n",
    "            for j in range(len(labels[i])):\n",
    "                if labels[i][j] == self.tokenizer.pad_token_id:\n",
    "                    labels[i][j] = -100\n",
    "\n",
    "        model_inputs[\"labels\"] = labels\n",
    "        return model_inputs\n",
    "        \n",
    "    def save_model_and_tokenizer(self, epoch=None, is_best=False):\n",
    "        \"\"\"\n",
    "        최고 성능 모델만 저장하고 이전 모델을 삭제\n",
    "        \"\"\"\n",
    "        if is_best:\n",
    "            # 이전 최고 모델 디렉토리 삭제\n",
    "            if os.path.exists(self.best_model_path):\n",
    "                print(f\"Deleting previous best model at {self.best_model_path}\")\n",
    "                os.system(f\"rm -rf {self.best_model_path}\")\n",
    "            \n",
    "            # 새로운 최고 모델 저장\n",
    "            save_path = os.path.join(self.save_dir, f\"best_model_epoch_{epoch}\")\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            self.model.save_pretrained(save_path)\n",
    "            self.tokenizer.save_pretrained(save_path)\n",
    "            torch.save(self.history, os.path.join(save_path, 'training_history.pt'))\n",
    "            print(f\"New best model saved at {save_path}\")\n",
    "\n",
    "            # 최고 모델 경로 업데이트\n",
    "            self.best_model_path = save_path\n",
    "\n",
    "    def calculate_metrics(self, predictions, labels):\n",
    "        predictions = torch.argmax(predictions, dim=-1)\n",
    "        correct = (predictions == labels).masked_fill(labels == -100, 0)\n",
    "        accuracy = correct.sum().item() / (labels != -100).sum().item()\n",
    "        return accuracy\n",
    "\n",
    "    def train(self, train_dataset, valid_dataset=None):\n",
    "        start_time = time.time()\n",
    "        print(f\"\\nStarting training at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"Training parameters:\")\n",
    "        print(f\"- Batch size: {self.training_args['batch_size']}\")\n",
    "        print(f\"- Learning rate: {self.training_args.get('learning_rate', '1e-4')}\")\n",
    "        print(f\"- Max length: {self.max_length}\")\n",
    "        print(f\"- Number of epochs: {self.training_args['num_epochs']}\")\n",
    "        print(f\"- Training samples: {len(train_dataset)}\")\n",
    "        if valid_dataset:\n",
    "            print(f\"- Validation samples: {len(valid_dataset)}\")\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "        # 데이터셋 전처리\n",
    "        print(\"Preprocessing training data...\")\n",
    "        train_dataset = train_dataset.map(\n",
    "            self.preprocess_data,\n",
    "            batched=True,\n",
    "            remove_columns=train_dataset.column_names,\n",
    "            desc=\"Processing training data\"\n",
    "        )\n",
    "\n",
    "        if valid_dataset is not None:\n",
    "            print(\"Preprocessing validation data...\")\n",
    "            valid_dataset = valid_dataset.map(\n",
    "                self.preprocess_data,\n",
    "                batched=True,\n",
    "                remove_columns=valid_dataset.column_names,\n",
    "                desc=\"Processing validation data\"\n",
    "            )\n",
    "\n",
    "        # DataCollator 설정\n",
    "        data_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer=self.tokenizer,\n",
    "            model=self.model,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # DataLoader 설정 (num_workers=0으로 변경하여 멀티프로세싱 관련 오류 방지)\n",
    "        train_dataloader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.training_args[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            collate_fn=data_collator,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        if valid_dataset is not None:\n",
    "            valid_dataloader = torch.utils.data.DataLoader(\n",
    "                valid_dataset,\n",
    "                batch_size=self.training_args[\"batch_size\"],\n",
    "                shuffle=False,\n",
    "                collate_fn=data_collator,\n",
    "                num_workers=0,\n",
    "                pin_memory=True\n",
    "            )\n",
    "        best_val_loss = float('inf')\n",
    "        early_stopping_counter = 0\n",
    "        early_stopping_patience = self.training_args.get('patience', 3)\n",
    "\n",
    "        for epoch in range(self.training_args[\"num_epochs\"]):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            self.model.train()\n",
    "            epoch_loss = 0\n",
    "            epoch_accuracy = 0\n",
    "            train_steps = 0\n",
    "            \n",
    "            progress_bar = tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}\")\n",
    "            batch_losses = []\n",
    "            batch_accuracies = []\n",
    "            \n",
    "            for batch_idx, batch in enumerate(progress_bar):\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels,\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss # 손실 계산\n",
    "                accuracy = self.calculate_metrics(outputs.logits, labels)\n",
    "                \n",
    "                loss.backward()# 손실 역전파 \n",
    "                self.optimizer.step() # 가중치 업데이트\n",
    "                self.optimizer.zero_grad() # 그래디언트 초기화\n",
    "\n",
    "                batch_losses.append(loss.item())\n",
    "                batch_accuracies.append(accuracy)\n",
    "                \n",
    "                current_loss = np.mean(batch_losses[-100:])\n",
    "                current_accuracy = np.mean(batch_accuracies[-100:])\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f'{current_loss:.4f}',\n",
    "                    'accuracy': f'{current_accuracy:.4f}',\n",
    "                    'batch': f'{batch_idx + 1}/{len(train_dataloader)}'\n",
    "                })\n",
    "\n",
    "            avg_train_loss = np.mean(batch_losses)\n",
    "            avg_train_accuracy = np.mean(batch_accuracies)\n",
    "\n",
    "            if valid_dataset is not None:\n",
    "                self.model.eval()\n",
    "                val_losses = []\n",
    "                val_accuracies = []\n",
    "\n",
    "                print(\"\\nRunning validation...\")\n",
    "                with torch.no_grad():\n",
    "                    for batch in tqdm(valid_dataloader, desc=\"Validating\"):\n",
    "                        input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                        attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                        labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "                        outputs = self.model(\n",
    "                            input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            labels=labels,\n",
    "                        )\n",
    "                        \n",
    "                        loss = outputs.loss\n",
    "                        accuracy = self.calculate_metrics(outputs.logits, labels)\n",
    "                        \n",
    "                        val_losses.append(loss.item())\n",
    "                        val_accuracies.append(accuracy)\n",
    "\n",
    "                avg_val_loss = np.mean(val_losses)\n",
    "                avg_val_accuracy = np.mean(val_accuracies)\n",
    "\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    early_stopping_counter = 0\n",
    "                    self.history['best_epoch'] = epoch + 1\n",
    "                    print(f\"\\nNew best validation loss: {best_val_loss:.4f}\")\n",
    "                    self.save_model_and_tokenizer(epoch + 1, is_best=True)  # 최고 모델만 저장\n",
    "                else:\n",
    "                    early_stopping_counter += 1\n",
    "\n",
    "\n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "            self.history['epoch_times'].append(epoch_time)\n",
    "            self.history['train_loss'].append(avg_train_loss)\n",
    "            if valid_dataset is not None:\n",
    "                self.history['val_loss'].append(avg_val_loss)\n",
    "\n",
    "            # Print epoch summary\n",
    "            print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "            print(f\"Time taken: {epoch_time:.2f} seconds\")\n",
    "            print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "            print(f\"Training accuracy: {avg_train_accuracy:.4f}\")\n",
    "            if valid_dataset is not None:\n",
    "                print(f\"Validation loss: {avg_val_loss:.4f}\")\n",
    "                print(f\"Validation accuracy: {avg_val_accuracy:.4f}\")\n",
    "                print(f\"Best validation loss so far: {best_val_loss:.4f}\")\n",
    "                print(f\"Early stopping counter: {early_stopping_counter}/{early_stopping_patience}\")\n",
    "\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print(\"\\nEarly stopping triggered.\")\n",
    "                break\n",
    "    def predict(self, text: str) -> str:\n",
    "            \"\"\"모델 추론\"\"\"\n",
    "            try:\n",
    "                max_length = self.max_length - 2  # Reserve space for special tokens\n",
    "                chunks = [text[i:i + max_length] for i in range(0, len(text), max_length)]\n",
    "                results = []\n",
    "                \n",
    "                for chunk in chunks:\n",
    "                    inputs = self.tokenizer(\n",
    "                        f\"키워드 추출: {self._normalize_text(chunk)}\",\n",
    "                        return_tensors=\"pt\",\n",
    "                        max_length=self.max_length,\n",
    "                        truncation=True,\n",
    "                    ).to(self.device)\n",
    "\n",
    "                    outputs = self.model.generate(\n",
    "                        inputs[\"input_ids\"], \n",
    "                        max_length=self.max_length, \n",
    "                        num_beams=5\n",
    "                    )\n",
    "                    \n",
    "                    result = self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "                    if result:\n",
    "                        results.append(result)\n",
    "                \n",
    "                return \", \".join(results).strip() or \"\"\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Keyword extraction error: {str(e)}\")\n",
    "                return \"\"\n",
    "    def _normalize_text(self, text: str) -> str:\n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Model: NVIDIA GeForce RTX 4080\n",
      "Available GPU memory: 15.59 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yjtech2/Desktop/yurim/anaconda3/envs/venv-llm/lib/python3.8/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Parameter 'function'=<bound method CustomKeyBERTTrainer.preprocess_data of <__main__.CustomKeyBERTTrainer object at 0x7f1c2c5a4400>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training at: 2024-12-04 16:22:44\n",
      "Training parameters:\n",
      "- Batch size: 8\n",
      "- Learning rate: 0.0001\n",
      "- Max length: 512\n",
      "- Number of epochs: 10\n",
      "- Training samples: 24000\n",
      "- Validation samples: 6000\n",
      "\n",
      "==================================================\n",
      "\n",
      "Preprocessing training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85c5ec0be9e49748c67a61803632ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing training data:   0%|          | 0/24000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yjtech2/Desktop/yurim/anaconda3/envs/venv-llm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:4114: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing validation data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1e542cfb8f4bb6b1bcfa551b231bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing validation data:   0%|          | 0/6000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 3000/3000 [09:43<00:00,  5.14it/s, loss=0.1582, accuracy=0.9580, batch=3000/3000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 750/750 [00:48<00:00, 15.37it/s]\n",
      "/home/yjtech2/Desktop/yurim/anaconda3/envs/venv-llm/lib/python3.8/site-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best validation loss: 0.0354\n",
      "New best model saved at ./smell_models/best_model_epoch_1\n",
      "\n",
      "Epoch 1 Summary:\n",
      "Time taken: 633.48 seconds\n",
      "Average training loss: 0.2084\n",
      "Training accuracy: 0.9425\n",
      "Validation loss: 0.0354\n",
      "Validation accuracy: 0.9882\n",
      "Best validation loss so far: 0.0354\n",
      "Early stopping counter: 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 3000/3000 [09:43<00:00,  5.14it/s, loss=0.0399, accuracy=0.9854, batch=3000/3000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 750/750 [00:49<00:00, 15.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best validation loss: 0.0220\n",
      "Deleting previous best model at ./smell_models/best_model_epoch_1\n",
      "New best model saved at ./smell_models/best_model_epoch_2\n",
      "\n",
      "Epoch 2 Summary:\n",
      "Time taken: 633.73 seconds\n",
      "Average training loss: 0.0661\n",
      "Training accuracy: 0.9809\n",
      "Validation loss: 0.0220\n",
      "Validation accuracy: 0.9914\n",
      "Best validation loss so far: 0.0220\n",
      "Early stopping counter: 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 3000/3000 [09:43<00:00,  5.14it/s, loss=0.0423, accuracy=0.9873, batch=3000/3000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 750/750 [00:49<00:00, 15.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best validation loss: 0.0207\n",
      "Deleting previous best model at ./smell_models/best_model_epoch_2\n",
      "New best model saved at ./smell_models/best_model_epoch_3\n",
      "\n",
      "Epoch 3 Summary:\n",
      "Time taken: 633.66 seconds\n",
      "Average training loss: 0.0355\n",
      "Training accuracy: 0.9881\n",
      "Validation loss: 0.0207\n",
      "Validation accuracy: 0.9922\n",
      "Best validation loss so far: 0.0207\n",
      "Early stopping counter: 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 3000/3000 [09:43<00:00,  5.14it/s, loss=0.0272, accuracy=0.9896, batch=3000/3000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 750/750 [00:48<00:00, 15.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      "Time taken: 632.50 seconds\n",
      "Average training loss: 0.0377\n",
      "Training accuracy: 0.9875\n",
      "Validation loss: 0.0241\n",
      "Validation accuracy: 0.9919\n",
      "Best validation loss so far: 0.0207\n",
      "Early stopping counter: 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 3000/3000 [09:43<00:00,  5.14it/s, loss=0.0482, accuracy=0.9857, batch=3000/3000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 750/750 [00:49<00:00, 15.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Summary:\n",
      "Time taken: 632.66 seconds\n",
      "Average training loss: 0.0304\n",
      "Training accuracy: 0.9895\n",
      "Validation loss: 0.0244\n",
      "Validation accuracy: 0.9917\n",
      "Best validation loss so far: 0.0207\n",
      "Early stopping counter: 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 3000/3000 [09:43<00:00,  5.14it/s, loss=0.0359, accuracy=0.9879, batch=3000/3000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 750/750 [00:49<00:00, 15.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Summary:\n",
      "Time taken: 632.85 seconds\n",
      "Average training loss: 0.0294\n",
      "Training accuracy: 0.9897\n",
      "Validation loss: 0.0262\n",
      "Validation accuracy: 0.9909\n",
      "Best validation loss so far: 0.0207\n",
      "Early stopping counter: 3/3\n",
      "\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # 멀티프로세싱 경고 방지\n",
    "\n",
    "trainer = CustomKeyBERTTrainer(\n",
    "    model_name=\"facebook/bart-base\", # t5-base     skt/kobart-base-v2\n",
    "    max_length=512,\n",
    "    learning_rate=1e-4,\n",
    "    batch_size=8,\n",
    "    num_epochs=10,\n",
    "    gradient_accumulation_steps=8,\n",
    "    patience=3 # 몇 에폭마다 체크포인트 저장할지\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()  # GPU 메모리 초기화\n",
    "    trainer.train(smell_train_dataset, smell_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원래 문장:  음식물 쓰레기 냄새가 납니다\n",
      "smell키워드 추출: 불쾌한 냄새\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "def test_model(text, saved_model_path, type):\n",
    "\n",
    "    # 모델과 토크나이저 불러오기\n",
    "    tokenizer = AutoTokenizer.from_pretrained(saved_model_path, local_files_only=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(saved_model_path, local_files_only=True).to('cpu')\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        f\"키워드 추출: {text}\",\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=128,\n",
    "        truncation=True\n",
    "    ).to(\"cpu\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=128,\n",
    "        num_beams=5,\n",
    "        length_penalty=0.7,\n",
    "        repetition_penalty=1.2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"원래 문장: \", text)\n",
    "    print(f\"{type}키워드 추출: {result}\")\n",
    "    \n",
    "\n",
    "text = '음식물 쓰레기 냄새가 납니다'\n",
    "\n",
    "test_model(text, '/home/yjtech2/Desktop/yurim/LLM/Pre_processing/smell_keyword/smell_models/best_model_epoch_3', 'smell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    여보세요? 유휴 토지 개발 현장에서 10개월 후 오후 10시부터 11시에 발생한 절...\n",
      "101    저희 말 농장에서 가축 사료 냄새 문제로 불편을 겪고 있습니다. 30일 전 저녁 6...\n",
      "102    여보세요? 청소 장비 제조 공장에서 4월 27일 오전 7시부터 8시에 발생한 제품 ...\n",
      "103    저희 알루미늄 압출 공장에서 1월 13일 정오에 포장지 처리 냄새 기계 오작동로 인...\n",
      "104    안녕하세요. 최근 포항 해수욕장에서 12일 전 밤에 발생한 부패된 고기 냄새 문제로...\n",
      "105    안녕하세요. 최근 가정용 전자기기 공장에서 2월 21일 오후 5시부터 6시에 발생한...\n",
      "106    여보세요? 레스토랑 및 카페 거리에서 2월 5일 오후 10시부터 11시에 젖은 쓰레...\n",
      "107    여보세요? 죽도초등학교 인근에서 3월 17일 오전에 얼음이 녹은 냄새 온도 급변로 ...\n",
      "108    저희 공원 산책로 인근에서 8일 후 오후 4시부터 5시에 플라스틱 태운 냄새 불법 ...\n",
      "109    여보세요? 도시 외곽 축사 단지에서 8월 1일 밤 11시에 발생한 동물 배설물 냄새...\n",
      "Name: text, dtype: object\n",
      "100     절단된 금속 냄새\n",
      "101      가축 사료 냄새\n",
      "102      제품 포장 냄새\n",
      "103     포장지 처리 냄새\n",
      "104     부패된 고기 냄새\n",
      "105    공업용 윤활유 냄새\n",
      "106     젖은 쓰레기 냄새\n",
      "107     얼음이 녹은 냄새\n",
      "108    플라스틱 태운 냄새\n",
      "109     동물 배설물 냄새\n",
      "Name: keyword, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(val_smell_df.iloc[100:110]['text'])\n",
    "print(val_smell_df.iloc[100:110]['keyword'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원래 문장:  여보세요? 유휴 토지 개발 현장에서 10개월 후 오후 10시부터 11시에 발생한 절단된 금속 냄새 건설 근로자의 건강 문제에 대해 상담 요청드립니다. 네, 유휴 토지 개발 현장에서 발생한 10개월 후 오후 10시부터 11시 절단된 금속 냄새 문제를 상담 드리겠습니다. 유휴 토지 개발 현장에서 10개월 후 오후 10시부터 11시에 절단된 금속 냄새으로 인해 호흡기 질환 문제가 발생하고 있습니다. 불편을 드려 정말 죄송합니다. 10개월 후 오후 10시부터 11시에 발생한 호흡기 질환 문제를 해결하기 위해 현장 점검을 진행 중입니다. 그럼 문제를 빨리 해결해 주세요. 확인 후 조치를 취하겠습니다. 수고 많으십니다. 저희가 도와드릴 수 있어 기쁩니다. 감사합니다.\n",
      "smell키워드 추출: 한 절단된 금속 냄새\n",
      "\n",
      "여보세요? 유휴 토지 개발 현장에서 10개월 후 오후 10시부터 11시에 발생한 절단된 금속 냄새 건설 근로자의 건강 문제에 대해 상담 요청드립니다. 네, 유휴 토지 개발 현장에서 발생한 10개월 후 오후 10시부터 11시 절단된 금속 냄새 문제를 상담 드리겠습니다. 유휴 토지 개발 현장에서 10개월 후 오후 10시부터 11시에 절단된 금속 냄새으로 인해 호흡기 질환 문제가 발생하고 있습니다. 불편을 드려 정말 죄송합니다. 10개월 후 오후 10시부터 11시에 발생한 호흡기 질환 문제를 해결하기 위해 현장 점검을 진행 중입니다. 그럼 문제를 빨리 해결해 주세요. 확인 후 조치를 취하겠습니다. 수고 많으십니다. 저희가 도와드릴 수 있어 기쁩니다. 감사합니다.\n",
      "None\n",
      "원래 문장:  저희 말 농장에서 가축 사료 냄새 문제로 불편을 겪고 있습니다. 30일 전 저녁 6시에 발생한 가축 사료 냄새과 관련해 문의 드립니다. 감사합니다. 농업 부서입니다. 30일 전 저녁 6시에 발생한 말 농장 문제를 도와드리겠습니다. 말 농장에서 30일 전 저녁 6시에 가축 사료 냄새으로 인해 호흡기 질환 문제가 발생하고 있습니다. 문제를 즉시 해결할 수 있도록 말 농장에 대한 점검을 진행하겠습니다. 조금만 기다려 주세요. 알겠습니다. 처리 좀 해주세요. 확인해 보고 빠르게 처리하겠습니다. 좋은 하루 되세요. 네, 처리 후 다시 연락드리겠습니다. 감사합니다.\n",
      "smell키워드 추출: 가축 사료 냄새\n",
      "\n",
      "저희 말 농장에서 가축 사료 냄새 문제로 불편을 겪고 있습니다. 30일 전 저녁 6시에 발생한 가축 사료 냄새과 관련해 문의 드립니다. 감사합니다. 농업 부서입니다. 30일 전 저녁 6시에 발생한 말 농장 문제를 도와드리겠습니다. 말 농장에서 30일 전 저녁 6시에 가축 사료 냄새으로 인해 호흡기 질환 문제가 발생하고 있습니다. 문제를 즉시 해결할 수 있도록 말 농장에 대한 점검을 진행하겠습니다. 조금만 기다려 주세요. 알겠습니다. 처리 좀 해주세요. 확인해 보고 빠르게 처리하겠습니다. 좋은 하루 되세요. 네, 처리 후 다시 연락드리겠습니다. 감사합니다.\n",
      "None\n",
      "원래 문장:  여보세요? 청소 장비 제조 공장에서 4월 27일 오전 7시부터 8시에 발생한 제품 포장 냄새 폐수 방류로 연락드렸습니다. 네, 민원센터입니다. 청소 장비 제조 공장의 폐수 방류에 대해 상담 드리겠습니다. 최근 청소 장비 제조 공장에서 4월 27일 오전 7시부터 8시에 제품 포장 냄새이 심하게 나고 있어 숨쉬기 힘듦로 큰 불편을 겪고 있습니다. 불편을 드려 대단히 죄송합니다. 4월 27일 오전 7시부터 8시에 발생한 폐수 방류 문제를 해결하기 위해 청소 장비 제조 공장에 대한 점검을 실시하겠습니다. 알겠습니다. 처리 좀 해주세요. 즉시 처리하겠습니다. 조금만 기다려 주세요. 빠른 처리 감사합니다. 감사합니다. 빠르게 처리하겠습니다.\n",
      "smell키워드 추출: 제조 공장 냄새\n",
      "\n",
      "여보세요? 청소 장비 제조 공장에서 4월 27일 오전 7시부터 8시에 발생한 제품 포장 냄새 폐수 방류로 연락드렸습니다. 네, 민원센터입니다. 청소 장비 제조 공장의 폐수 방류에 대해 상담 드리겠습니다. 최근 청소 장비 제조 공장에서 4월 27일 오전 7시부터 8시에 제품 포장 냄새이 심하게 나고 있어 숨쉬기 힘듦로 큰 불편을 겪고 있습니다. 불편을 드려 대단히 죄송합니다. 4월 27일 오전 7시부터 8시에 발생한 폐수 방류 문제를 해결하기 위해 청소 장비 제조 공장에 대한 점검을 실시하겠습니다. 알겠습니다. 처리 좀 해주세요. 즉시 처리하겠습니다. 조금만 기다려 주세요. 빠른 처리 감사합니다. 감사합니다. 빠르게 처리하겠습니다.\n",
      "None\n",
      "원래 문장:  저희 알루미늄 압출 공장에서 1월 13일 정오에 포장지 처리 냄새 기계 오작동로 인해 불편을 겪고 있어 연락드립니다. 감사합니다. 위험물 관리 부서입니다. 알루미늄 압출 공장에서 발생한 포장지 처리 냄새 문제와 관련해 상담 드리겠습니다. 알루미늄 압출 공장에서 1월 13일 정오에 포장지 처리 냄새으로 인해 기운이 떨어짐 문제가 발생하고 있습니다. 현재 알루미늄 압출 공장에서 발생한 기계 오작동 문제에 대해 조치 중입니다. 1월 13일 정오에 완료될 예정입니다. 그럼 신속히 해결해주세요. 확인 후 조치를 취하겠습니다. 고맙습니다. 감사합니다. 좋은 하루 되세요.\n",
      "smell키워드 추출: 포장지 처리 냄새\n",
      "\n",
      "저희 알루미늄 압출 공장에서 1월 13일 정오에 포장지 처리 냄새 기계 오작동로 인해 불편을 겪고 있어 연락드립니다. 감사합니다. 위험물 관리 부서입니다. 알루미늄 압출 공장에서 발생한 포장지 처리 냄새 문제와 관련해 상담 드리겠습니다. 알루미늄 압출 공장에서 1월 13일 정오에 포장지 처리 냄새으로 인해 기운이 떨어짐 문제가 발생하고 있습니다. 현재 알루미늄 압출 공장에서 발생한 기계 오작동 문제에 대해 조치 중입니다. 1월 13일 정오에 완료될 예정입니다. 그럼 신속히 해결해주세요. 확인 후 조치를 취하겠습니다. 고맙습니다. 감사합니다. 좋은 하루 되세요.\n",
      "None\n",
      "원래 문장:  안녕하세요. 최근 포항 해수욕장에서 12일 전 밤에 발생한 부패된 고기 냄새 문제로 연락드렸습니다. 네, 포항 해수욕장의 분리배출 혼잡에 대해 신속히 확인하겠습니다. 포항 해수욕장에서 12일 전 밤에 발생한 부패된 고기 냄새 분리배출 혼잡로 인해 냄새로 인한 불쾌감로 불편함을 겪고 있습니다. 포항 해수욕장에서 발생한 분리배출 혼잡 문제에 대해 12일 전 밤에 즉시 점검을 시작하겠습니다. 네, 그러니 빨리 좀 처리해주세요. 알겠습니다. 최대한 빠르게 처리하겠습니다. 감사합니다. 언제든지 연락 주세요. 감사합니다.\n",
      "smell키워드 추출: 부팬한 해수욕장연 냄새\n",
      "\n",
      "안녕하세요. 최근 포항 해수욕장에서 12일 전 밤에 발생한 부패된 고기 냄새 문제로 연락드렸습니다. 네, 포항 해수욕장의 분리배출 혼잡에 대해 신속히 확인하겠습니다. 포항 해수욕장에서 12일 전 밤에 발생한 부패된 고기 냄새 분리배출 혼잡로 인해 냄새로 인한 불쾌감로 불편함을 겪고 있습니다. 포항 해수욕장에서 발생한 분리배출 혼잡 문제에 대해 12일 전 밤에 즉시 점검을 시작하겠습니다. 네, 그러니 빨리 좀 처리해주세요. 알겠습니다. 최대한 빠르게 처리하겠습니다. 감사합니다. 언제든지 연락 주세요. 감사합니다.\n",
      "None\n",
      "원래 문장:  안녕하세요. 최근 가정용 전자기기 공장에서 2월 21일 오후 5시부터 6시에 발생한 공업용 윤활유 냄새 문제로 연락드렸습니다. 네, 가정용 전자기기 공장의 미세먼지 발생에 대해 신속히 확인하겠습니다. 저희 가정용 전자기기 공장에서 2월 21일 오후 5시부터 6시에 공업용 윤활유 냄새이 순간적으로 강해지는 나고 있습니다. 눈이 따끔거림 현재 가정용 전자기기 공장에 대한 점검을 시작하여 문제를 해결하겠습니다. 잠시만 기다려 주세요. 알겠습니다. 처리 좀 해주세요. 네, 바로 조치를 취하겠습니다. 빠른 처리 감사합니다. 언제든지 연락 주세요. 감사합니다.\n",
      "smell키워드 추출: 가정용 전새\n",
      "\n",
      "안녕하세요. 최근 가정용 전자기기 공장에서 2월 21일 오후 5시부터 6시에 발생한 공업용 윤활유 냄새 문제로 연락드렸습니다. 네, 가정용 전자기기 공장의 미세먼지 발생에 대해 신속히 확인하겠습니다. 저희 가정용 전자기기 공장에서 2월 21일 오후 5시부터 6시에 공업용 윤활유 냄새이 순간적으로 강해지는 나고 있습니다. 눈이 따끔거림 현재 가정용 전자기기 공장에 대한 점검을 시작하여 문제를 해결하겠습니다. 잠시만 기다려 주세요. 알겠습니다. 처리 좀 해주세요. 네, 바로 조치를 취하겠습니다. 빠른 처리 감사합니다. 언제든지 연락 주세요. 감사합니다.\n",
      "None\n",
      "원래 문장:  여보세요? 레스토랑 및 카페 거리에서 2월 5일 오후 10시부터 11시에 젖은 쓰레기 냄새 소각장 연기 발생로 인해 문의 드립니다. 안녕하세요. 레스토랑 및 카페 거리 관련 문의이신가요? 제가 도와드리겠습니다. 최근 레스토랑 및 카페 거리에서 2월 5일 오후 10시부터 11시에 젖은 쓰레기 냄새이 바람을 타고 확산되는 나고 있어 민원 처리 지연로 불편을 겪고 있습니다. 불편을 드려 정말 죄송합니다. 2월 5일 오후 10시부터 11시에 발생한 민원 처리 지연 문제를 해결하기 위해 현장 점검을 진행 중입니다. 네, 기다리겠습니다. 빨리 처리해 주세요. 확인 후 조치를 취하겠습니다. 수고 많으십니다. 언제든지 연락 주세요. 감사합니다.\n",
      "smell키워드 추출: 젖은 카린 냄새\n",
      "\n",
      "여보세요? 레스토랑 및 카페 거리에서 2월 5일 오후 10시부터 11시에 젖은 쓰레기 냄새 소각장 연기 발생로 인해 문의 드립니다. 안녕하세요. 레스토랑 및 카페 거리 관련 문의이신가요? 제가 도와드리겠습니다. 최근 레스토랑 및 카페 거리에서 2월 5일 오후 10시부터 11시에 젖은 쓰레기 냄새이 바람을 타고 확산되는 나고 있어 민원 처리 지연로 불편을 겪고 있습니다. 불편을 드려 정말 죄송합니다. 2월 5일 오후 10시부터 11시에 발생한 민원 처리 지연 문제를 해결하기 위해 현장 점검을 진행 중입니다. 네, 기다리겠습니다. 빨리 처리해 주세요. 확인 후 조치를 취하겠습니다. 수고 많으십니다. 언제든지 연락 주세요. 감사합니다.\n",
      "None\n",
      "원래 문장:  여보세요? 죽도초등학교 인근에서 3월 17일 오전에 얼음이 녹은 냄새 온도 급변로 인해 문의 드립니다. 안녕하세요. 죽도초등학교 인근 관련 문의이신가요? 제가 도와드리겠습니다. 죽도초등학교 인근에서 3월 17일 오전에 얼음이 녹은 냄새 때문에 폭염으로 인한 탈진가 발생하고 있습니다. 불편을 드려 정말 죄송합니다. 폭염으로 인한 탈진 해결을 위해 신속히 대응하겠습니다. 그럼 빠르게 해결해 주세요. 확인 후 조치를 취하겠습니다. 감사합니다. 처리 잘 부탁드립니다. 언제든지 연락 주세요. 감사합니다.\n",
      "smell키워드 추출: 에 섞인 얼이가 녹은 냄새\n",
      "\n",
      "여보세요? 죽도초등학교 인근에서 3월 17일 오전에 얼음이 녹은 냄새 온도 급변로 인해 문의 드립니다. 안녕하세요. 죽도초등학교 인근 관련 문의이신가요? 제가 도와드리겠습니다. 죽도초등학교 인근에서 3월 17일 오전에 얼음이 녹은 냄새 때문에 폭염으로 인한 탈진가 발생하고 있습니다. 불편을 드려 정말 죄송합니다. 폭염으로 인한 탈진 해결을 위해 신속히 대응하겠습니다. 그럼 빠르게 해결해 주세요. 확인 후 조치를 취하겠습니다. 감사합니다. 처리 잘 부탁드립니다. 언제든지 연락 주세요. 감사합니다.\n",
      "None\n",
      "원래 문장:  저희 공원 산책로 인근에서 8일 후 오후 4시부터 5시에 플라스틱 태운 냄새 불법 투기된 폐기물로 인해 불편을 겪고 있어 연락드립니다. 감사합니다. 환경 관리팀입니다. 공원 산책로 인근에서 발생한 플라스틱 태운 냄새 문제와 관련해 상담 드리겠습니다. 저희 공원 산책로 인근에서 8일 후 오후 4시부터 5시에 플라스틱 태운 냄새이 한꺼번에 강하게 퍼지는 나고 있습니다. 민원 처리 지연 현재 공원 산책로 인근에서 발생한 불법 투기된 폐기물 문제에 대해 조치 중입니다. 8일 후 오후 4시부터 5시에 완료될 예정입니다. 네, 처리가 되도록 부탁드립니다. 확인 후 조치를 취하겠습니다. 수고하셨습니다. 감사합니다. 좋은 하루 되세요.\n",
      "smell키워드 추출: 태운 냄새\n",
      "\n",
      "저희 공원 산책로 인근에서 8일 후 오후 4시부터 5시에 플라스틱 태운 냄새 불법 투기된 폐기물로 인해 불편을 겪고 있어 연락드립니다. 감사합니다. 환경 관리팀입니다. 공원 산책로 인근에서 발생한 플라스틱 태운 냄새 문제와 관련해 상담 드리겠습니다. 저희 공원 산책로 인근에서 8일 후 오후 4시부터 5시에 플라스틱 태운 냄새이 한꺼번에 강하게 퍼지는 나고 있습니다. 민원 처리 지연 현재 공원 산책로 인근에서 발생한 불법 투기된 폐기물 문제에 대해 조치 중입니다. 8일 후 오후 4시부터 5시에 완료될 예정입니다. 네, 처리가 되도록 부탁드립니다. 확인 후 조치를 취하겠습니다. 수고하셨습니다. 감사합니다. 좋은 하루 되세요.\n",
      "None\n",
      "원래 문장:  여보세요? 도시 외곽 축사 단지에서 8월 1일 밤 11시에 발생한 동물 배설물 냄새 위생 문제로 연락드렸습니다. 네, 민원센터입니다. 도시 외곽 축사 단지의 위생 문제에 대해 상담 드리겠습니다. 최근 도시 외곽 축사 단지에서 8월 1일 밤 11시에 동물 배설물 냄새이 열기와 함께 퍼지는 나고 있어 메스꺼움로 큰 불편을 겪고 있습니다. 불편을 드려 대단히 죄송합니다. 메스꺼움 해결을 위해 도시 외곽 축사 단지에 대한 현장 점검을 실시하겠습니다. 잠시만 기다려 주시겠습니까? 네, 처리가 되도록 부탁드립니다. 알겠습니다. 최대한 빠르게 처리하겠습니다. 수고 많으십니다. 저희가 도와드리겠습니다. 감사합니다.\n",
      "smell키워드 추출: 한 축사 배설물 냄새\n",
      "\n",
      "여보세요? 도시 외곽 축사 단지에서 8월 1일 밤 11시에 발생한 동물 배설물 냄새 위생 문제로 연락드렸습니다. 네, 민원센터입니다. 도시 외곽 축사 단지의 위생 문제에 대해 상담 드리겠습니다. 최근 도시 외곽 축사 단지에서 8월 1일 밤 11시에 동물 배설물 냄새이 열기와 함께 퍼지는 나고 있어 메스꺼움로 큰 불편을 겪고 있습니다. 불편을 드려 대단히 죄송합니다. 메스꺼움 해결을 위해 도시 외곽 축사 단지에 대한 현장 점검을 실시하겠습니다. 잠시만 기다려 주시겠습니까? 네, 처리가 되도록 부탁드립니다. 알겠습니다. 최대한 빠르게 처리하겠습니다. 수고 많으십니다. 저희가 도와드리겠습니다. 감사합니다.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for text in val_smell_df.iloc[100:110]['text']:\n",
    "    res = test_model(text, '/home/yjtech2/Desktop/yurim/LLM/Pre_processing/smell_keyword/smell_models/best_model_epoch_3', 'smell')\n",
    "    print()\n",
    "    print(text)\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>여보세요? 택배 물류 창고 근처에서 12개월 전 새벽 5시에 화장실 냄새 청결 부족...</td>\n",
       "      <td>화장실 냄새</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>안녕하세요. 지하철 역 주변에서 11월 14일 오전에 공공장소 쓰레기 냄새 관련하여...</td>\n",
       "      <td>공공장소 쓰레기 냄새</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>안녕하세요. 아파트 커뮤니티 센터에서 12월 8일 저녁 7시에 오래된 냄새 관련하여...</td>\n",
       "      <td>오래된 냄새</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>여보세요? 음식물 쓰레기장에서 1월 17일 자정에 발생한 썩은 물질 냄새 불법 폐기...</td>\n",
       "      <td>썩은 물질 냄새</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>여보세요? 돼지 농장에서 5월 4일 저녁 7시에 발생한 동물 건강 관리용 약물 냄새...</td>\n",
       "      <td>동물 건강 관리용 약물 냄새</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23995</th>\n",
       "      <td>저희 에너지 플랜트 공사 현장에서 기타 건축 폐기물 냄새 문제로 불편을 겪고 있습니...</td>\n",
       "      <td>기타 건축 폐기물 냄새</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23996</th>\n",
       "      <td>여보세요? 포항 근교 농촌에서 8월 2일 새벽에 발생한 여름 폭우 후 냄새 나는 호...</td>\n",
       "      <td>여름 폭우 후 냄새 나는 호수</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23997</th>\n",
       "      <td>여보세요? 포항대학교에서 10월 4일 저녁 6시에 발생한 공공장소 쓰레기 냄새 도로...</td>\n",
       "      <td>공공장소 쓰레기 냄새</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23998</th>\n",
       "      <td>축사에서 8월 1일 오후 8시부터 9시에 양식장에서 나는 냄새이 코끝을 찌르는 나고...</td>\n",
       "      <td>양식장에서 나는 냄새</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23999</th>\n",
       "      <td>안녕하세요. 최근 주차장에서 7개월 전 오전 7시부터 8시에 발생한 오래된 냄새 문...</td>\n",
       "      <td>오래된 냄새</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23952 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text           keyword\n",
       "0      여보세요? 택배 물류 창고 근처에서 12개월 전 새벽 5시에 화장실 냄새 청결 부족...            화장실 냄새\n",
       "1      안녕하세요. 지하철 역 주변에서 11월 14일 오전에 공공장소 쓰레기 냄새 관련하여...       공공장소 쓰레기 냄새\n",
       "2      안녕하세요. 아파트 커뮤니티 센터에서 12월 8일 저녁 7시에 오래된 냄새 관련하여...            오래된 냄새\n",
       "3      여보세요? 음식물 쓰레기장에서 1월 17일 자정에 발생한 썩은 물질 냄새 불법 폐기...          썩은 물질 냄새\n",
       "4      여보세요? 돼지 농장에서 5월 4일 저녁 7시에 발생한 동물 건강 관리용 약물 냄새...   동물 건강 관리용 약물 냄새\n",
       "...                                                  ...               ...\n",
       "23995  저희 에너지 플랜트 공사 현장에서 기타 건축 폐기물 냄새 문제로 불편을 겪고 있습니...      기타 건축 폐기물 냄새\n",
       "23996  여보세요? 포항 근교 농촌에서 8월 2일 새벽에 발생한 여름 폭우 후 냄새 나는 호...  여름 폭우 후 냄새 나는 호수\n",
       "23997  여보세요? 포항대학교에서 10월 4일 저녁 6시에 발생한 공공장소 쓰레기 냄새 도로...       공공장소 쓰레기 냄새\n",
       "23998  축사에서 8월 1일 오후 8시부터 9시에 양식장에서 나는 냄새이 코끝을 찌르는 나고...       양식장에서 나는 냄새\n",
       "23999  안녕하세요. 최근 주차장에서 7개월 전 오전 7시부터 8시에 발생한 오래된 냄새 문...            오래된 냄새\n",
       "\n",
       "[23952 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_smell_df[train_smell_df['keyword'] != \"고온 증기 냄새\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
